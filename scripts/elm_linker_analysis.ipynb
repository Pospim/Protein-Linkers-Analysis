{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42665fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Setup and Configuration\n",
    "\n",
    "Load required libraries and set working directory.\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Configuration\n",
    "workdir = \"~/Desktop/work/protein_linkers/input_2\"\n",
    "workdir = os.path.expanduser(workdir)\n",
    "elm_csv_path = os.path.join(workdir, 'elm', 'elm_results.csv')\n",
    "fasta_path = os.path.join(workdir, 'proteins.fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82de8ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. Load Linker + Domain Data\n",
    "\n",
    "Load the three clustered linker dictionaries (short, medium, long) that were\n",
    "generated by the clustering analysis, plus the filtered proteins dictionary\n",
    "with all domains and linkers.\n",
    "\"\"\"\n",
    "\n",
    "# Load short linkers\n",
    "short_linkers_path = os.path.join(workdir, 'short_linkers.json')\n",
    "with open(short_linkers_path, 'r') as f:\n",
    "    short_linkers = json.load(f)\n",
    "\n",
    "# Load medium linkers\n",
    "medium_linkers_path = os.path.join(workdir, 'medium_linkers.json')\n",
    "with open(medium_linkers_path, 'r') as f:\n",
    "    medium_linkers = json.load(f)\n",
    "\n",
    "# Load long linkers\n",
    "long_linkers_path = os.path.join(workdir, 'long_linkers.json')\n",
    "with open(long_linkers_path, 'r') as f:\n",
    "    long_linkers = json.load(f)\n",
    "\n",
    "# Load filtered proteins (contains all domains and linkers after outlier removal)\n",
    "filtered_proteins_path = os.path.join(workdir, 'filtered_proteins.json')\n",
    "with open(filtered_proteins_path, 'r') as f:\n",
    "    filtered_proteins = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be5aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check structure of the data\n",
    "example_protein = list(filtered_proteins.keys())[0]\n",
    "print(f\"Example protein: {example_protein}\\n\")\n",
    "print(\"\\nProtein data structure:\\n\")\n",
    "print(json.dumps(filtered_proteins[example_protein], indent=2)[:1000], \"\\n\")\n",
    "\n",
    "# Find a protein with linkers\n",
    "for pid, pdata in filtered_proteins.items():\n",
    "    if pdata.get('linkers'):\n",
    "        print(f\"\\nExample protein with linkers: {pid}\\n\")\n",
    "        print(json.dumps(pdata, indent=2)[:1500], \"\\n\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45b075fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. Load Protein Sequences\n",
    "\n",
    "Load the FASTA file to extract linker sequences for ELM analysis.\n",
    "\"\"\"\n",
    "\n",
    "def load_fasta(fasta_path):\n",
    "    \"\"\"\n",
    "    Load protein sequences from a FASTA file.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    fasta_path : str\n",
    "        Path to FASTA file\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with protein accessions as keys and sequences as values\n",
    "    \"\"\"\n",
    "    sequences_dict = {}\n",
    "\n",
    "    with open(fasta_path, 'r') as f:\n",
    "        current_acc = None\n",
    "        current_seq = []\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                # Save previous sequence if exists\n",
    "                if current_acc:\n",
    "                    sequences_dict[current_acc] = ''.join(current_seq)\n",
    "\n",
    "                # Start new sequence\n",
    "                current_acc = line[1:]\n",
    "                current_seq = []\n",
    "            else:\n",
    "                current_seq.append(line)\n",
    "\n",
    "        # Save last sequence\n",
    "        if current_acc:\n",
    "            sequences_dict[current_acc] = ''.join(current_seq)\n",
    "\n",
    "    return sequences_dict\n",
    "\n",
    "\n",
    "# Load sequences\n",
    "sequences_dict = load_fasta(fasta_path)\n",
    "\n",
    "print(f\"✓ Loaded {len(sequences_dict)} protein sequences from proteins.fa\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84826ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4. Load ELM Results (CSV) and Inspect Structure\n",
    "\"\"\"\n",
    "import csv\n",
    "from IPython.display import display\n",
    "\n",
    "# Ensure the CSV exists\n",
    "if not os.path.exists(elm_csv_path):\n",
    "    raise FileNotFoundError(f\"ELM results CSV not found at: {elm_csv_path}\")\n",
    "\n",
    "# Try to detect delimiter robustly\n",
    "with open(elm_csv_path, \"r\", encoding=\"utf-8\", errors=\"replace\") as fh:\n",
    "    sample = fh.read(4096)\n",
    "    try:\n",
    "        dialect = csv.Sniffer().sniff(sample, delimiters=[\",\", \"\\t\", \";\", \"|\"])\n",
    "        sep = dialect.delimiter\n",
    "    except csv.Error:\n",
    "        # Fallback: choose the most frequent candidate delimiter\n",
    "        candidates = [\",\", \"\\t\", \";\", \"|\"]\n",
    "        counts = {d: sample.count(d) for d in candidates}\n",
    "        sep = max(counts, key=counts.get)\n",
    "\n",
    "print(f\"Detected delimiter: {repr(sep)}\")\n",
    "\n",
    "# Load CSV with a tolerant parser for occasional bad lines\n",
    "elm_df = pd.read_csv(\n",
    "    elm_csv_path,\n",
    "    sep=sep,\n",
    "    engine=\"python\",\n",
    "    on_bad_lines=\"warn\"\n",
    ")\n",
    "elm_df[0:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a0adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5. Filter ELM Results by Protein Existence\n",
    "\n",
    "Remove entries where the protein ID (extracted from region_id)\n",
    "is not in filtered_proteins.keys()\n",
    "\"\"\"\n",
    "\n",
    "# Extract protein ID from region_id (format: <protein>_<domain|linker>_<idx>_<name>)\n",
    "elm_df['protein_id'] = elm_df['region_id'].str.split('_').str[0]\n",
    "\n",
    "# Before filtering\n",
    "print(f\"Before filtering: {len(elm_df):,} rows\")\n",
    "\n",
    "# Filter to keep only rows where protein exists in filtered_proteins\n",
    "elm_df = elm_df[elm_df['protein_id'].isin(filtered_proteins.keys())]\n",
    "\n",
    "# After filtering\n",
    "print(f\"After filtering: {len(elm_df):,} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9a232418",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6. Add ELM motifs to domains and linkers in filtered_proteins\n",
    "\n",
    "Refactored into smaller, focused functions for better readability and maintainability.\n",
    "\"\"\"\n",
    "\n",
    "def _convert_domains_to_dicts(filtered_proteins):\n",
    "    \"\"\"\n",
    "    Convert domain lists [name, start, end] to dictionaries with motifs field.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    filtered_proteins : dict\n",
    "        Protein data with domains as lists or dicts\n",
    "    \"\"\"\n",
    "    for pid, protein_entry in filtered_proteins.items():\n",
    "        if 'domains' in protein_entry:\n",
    "            new_domains = []\n",
    "            for domain in protein_entry['domains']:\n",
    "                if isinstance(domain, list):\n",
    "                    domain_dict = {\n",
    "                        'name': domain[0],\n",
    "                        'start': domain[1],\n",
    "                        'end': domain[2],\n",
    "                        'motifs': []\n",
    "                    }\n",
    "                    new_domains.append(domain_dict)\n",
    "                elif isinstance(domain, dict):\n",
    "                    if 'motifs' not in domain:\n",
    "                        domain['motifs'] = []\n",
    "                    new_domains.append(domain)\n",
    "            protein_entry['domains'] = new_domains\n",
    "\n",
    "\n",
    "def _convert_linkers_to_dicts(filtered_proteins):\n",
    "    \"\"\"\n",
    "    Convert linker lists [type, start, end] to dictionaries with motifs field.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    filtered_proteins : dict\n",
    "        Protein data with linkers as lists or dicts\n",
    "    \"\"\"\n",
    "    for pid, protein_entry in filtered_proteins.items():\n",
    "        if 'linkers' in protein_entry:\n",
    "            new_linkers = []\n",
    "            for linker in protein_entry['linkers']:\n",
    "                if isinstance(linker, list):\n",
    "                    linker_dict = {\n",
    "                        'type': linker[0],\n",
    "                        'start': linker[1],\n",
    "                        'end': linker[2],\n",
    "                        'motifs': []\n",
    "                    }\n",
    "                    new_linkers.append(linker_dict)\n",
    "                elif isinstance(linker, dict):\n",
    "                    if 'motifs' not in linker:\n",
    "                        linker['motifs'] = []\n",
    "                    new_linkers.append(linker)\n",
    "            protein_entry['linkers'] = new_linkers\n",
    "\n",
    "\n",
    "def _parse_region_id(region_id):\n",
    "    \"\"\"\n",
    "    Parse region_id into components.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    region_id : str\n",
    "        Format: <pid>_<domain|linker>_<idx>_<name>\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (protein_id, idx) or (None, None) if parsing fails\n",
    "    \"\"\"\n",
    "    parts = region_id.split('_', 3)\n",
    "    if len(parts) < 3:\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        idx = int(parts[2]) - 1  # Convert from 1-based to 0-based\n",
    "        return parts[0], idx\n",
    "    except ValueError:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def _assign_motifs_to_domains(elm_df, filtered_proteins):\n",
    "    \"\"\"\n",
    "    Assign ELM motifs to domains.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    elm_df : DataFrame\n",
    "        ELM results filtered for region_type='domains'\n",
    "    filtered_proteins : dict\n",
    "        Protein data with domains\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    int : Number of motifs added\n",
    "    \"\"\"\n",
    "    # Ensure domains are dicts\n",
    "    _convert_domains_to_dicts(filtered_proteins)\n",
    "\n",
    "    motif_count = 0\n",
    "    for _, row in elm_df.iterrows():\n",
    "        if row['region_type'] != 'domains':\n",
    "            continue\n",
    "\n",
    "        pid, idx = _parse_region_id(row['region_id'])\n",
    "        if pid is None:\n",
    "            continue\n",
    "\n",
    "        protein_entry = filtered_proteins.get(pid)\n",
    "        if not protein_entry or 'domains' not in protein_entry:\n",
    "            continue\n",
    "\n",
    "        if idx < 0 or idx >= len(protein_entry['domains']):\n",
    "            continue\n",
    "\n",
    "        # Add motif\n",
    "        motif_info = {\n",
    "            'motif': row['motif_id'],\n",
    "            'start': int(row['motif_start']),\n",
    "            'end': int(row['motif_end'])\n",
    "        }\n",
    "        protein_entry['domains'][idx]['motifs'].append(motif_info)\n",
    "        motif_count += 1\n",
    "\n",
    "    return motif_count\n",
    "\n",
    "\n",
    "def _assign_motifs_to_linkers(elm_df, filtered_proteins, linker_type):\n",
    "    \"\"\"\n",
    "    Assign ELM motifs to linkers of a specific type.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    elm_df : DataFrame\n",
    "        ELM results\n",
    "    filtered_proteins : dict\n",
    "        Protein data with linkers\n",
    "    linker_type : str\n",
    "        One of: 'inner', 'c-terminus', 'n-terminus'\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    int : Number of motifs added\n",
    "    \"\"\"\n",
    "    # Ensure linkers are dicts with motifs field\n",
    "    _convert_linkers_to_dicts(filtered_proteins)\n",
    "\n",
    "    motif_count = 0\n",
    "    for _, row in elm_df.iterrows():\n",
    "        if row['region_type'] != linker_type:\n",
    "            continue\n",
    "\n",
    "        pid, idx = _parse_region_id(row['region_id'])\n",
    "        if pid is None:\n",
    "            continue\n",
    "\n",
    "        protein_entry = filtered_proteins.get(pid)\n",
    "        if not protein_entry or 'linkers' not in protein_entry:\n",
    "            continue\n",
    "\n",
    "        if idx < 0 or idx >= len(protein_entry['linkers']):\n",
    "            continue\n",
    "\n",
    "        target_linker = protein_entry['linkers'][idx]\n",
    "\n",
    "        # Verify the linker type matches\n",
    "        if not isinstance(target_linker, dict) or target_linker.get('type') != linker_type:\n",
    "            continue\n",
    "\n",
    "        # Add motif\n",
    "        motif_info = {\n",
    "            'motif': row['motif_id'],\n",
    "            'start': int(row['motif_start']),\n",
    "            'end': int(row['motif_end'])\n",
    "        }\n",
    "        target_linker['motifs'].append(motif_info)\n",
    "        motif_count += 1\n",
    "\n",
    "    return motif_count\n",
    "\n",
    "\n",
    "def add_motifs_by_type(elm_df, filtered_proteins, region_type):\n",
    "    \"\"\"\n",
    "    Wrapper function that routes to the appropriate assignment function based on region type.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    elm_df : DataFrame\n",
    "        ELM results\n",
    "    filtered_proteins : dict\n",
    "        Protein data\n",
    "    region_type : str\n",
    "        Either 'domains' or one of the linker types: 'inner', 'c-terminus', 'n-terminus'\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    int : Number of motifs added\n",
    "    \"\"\"\n",
    "    if region_type == 'domains':\n",
    "        return _assign_motifs_to_domains(elm_df, filtered_proteins)\n",
    "    elif region_type in ['inner', 'c-terminus', 'n-terminus']:\n",
    "        return _assign_motifs_to_linkers(elm_df, filtered_proteins, region_type)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown region_type: {region_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c095f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing domains...\n",
      "✓ Added 19,094 motifs to domains\n",
      "Processing inner...\n",
      "✓ Added 19,094 motifs to domains\n",
      "Processing inner...\n",
      "✓ Added 1,095 motifs to inner\n",
      "Processing c-terminus...\n",
      "✓ Added 1,095 motifs to inner\n",
      "Processing c-terminus...\n",
      "✓ Added 80 motifs to c-terminus\n",
      "Processing n-terminus...\n",
      "✓ Added 80 motifs to c-terminus\n",
      "Processing n-terminus...\n",
      "✓ Added 81 motifs to n-terminus\n",
      "\n",
      "============================================================\n",
      "TOTAL: Added 20,350 motifs across all regions\n",
      "============================================================\n",
      "\n",
      "Sorting motifs by start position...\n",
      "✓ Sorted 179,382 motifs by start position\n",
      "✓ Added 81 motifs to n-terminus\n",
      "\n",
      "============================================================\n",
      "TOTAL: Added 20,350 motifs across all regions\n",
      "============================================================\n",
      "\n",
      "Sorting motifs by start position...\n",
      "✓ Sorted 179,382 motifs by start position\n"
     ]
    }
   ],
   "source": [
    "# Ensure protein_id column exists\n",
    "if 'protein_id' not in elm_df.columns:\n",
    "    elm_df['protein_id'] = elm_df['region_id'].str.split('_').str[0]\n",
    "\n",
    "# Process all region types\n",
    "region_types = ['domains', 'inner', 'c-terminus', 'n-terminus']\n",
    "total_motifs = 0\n",
    "\n",
    "for region_type in region_types:\n",
    "    print(f\"Processing {region_type}...\")\n",
    "    count = add_motifs_by_type(elm_df, filtered_proteins, region_type)\n",
    "    print(f\"✓ Added {count:,} motifs to {region_type}\")\n",
    "    total_motifs += count\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TOTAL: Added {total_motifs:,} motifs across all regions\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Sort motifs by start position in each domain and linker\n",
    "print(\"\\nSorting motifs by start position...\")\n",
    "sorted_count = 0\n",
    "\n",
    "for pid, protein_entry in filtered_proteins.items():\n",
    "    # Sort domain motifs\n",
    "    if 'domains' in protein_entry:\n",
    "        for domain in protein_entry['domains']:\n",
    "            if isinstance(domain, dict) and 'motifs' in domain and domain['motifs']:\n",
    "                domain['motifs'].sort(key=lambda m: m['start'])\n",
    "                sorted_count += len(domain['motifs'])\n",
    "\n",
    "    # Sort linker motifs\n",
    "    if 'linkers' in protein_entry:\n",
    "        for linker in protein_entry['linkers']:\n",
    "            if isinstance(linker, dict) and 'motifs' in linker and linker['motifs']:\n",
    "                linker['motifs'].sort(key=lambda m: m['start'])\n",
    "                sorted_count += len(linker['motifs'])\n",
    "\n",
    "print(f\"✓ Sorted {sorted_count:,} motifs by start position\")\n",
    "\n",
    "# Save to JSON\n",
    "output_path = os.path.join(workdir, 'proteins_elm_motifs.json')\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(filtered_proteins, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54b8d80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
