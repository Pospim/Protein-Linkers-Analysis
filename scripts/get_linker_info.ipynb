{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d6e0f7",
   "metadata": {},
   "source": [
    "# Protein Linker and Domain Analysis\n",
    "\n",
    "This notebook provides comprehensive analysis of protein linker regions and domains from InterPro database.\n",
    "\n",
    "## Overview\n",
    "\n",
    "**What are protein linkers?**  \n",
    "Linkers are unstructured regions between protein domains. They can be:\n",
    "- **N-terminus**: Region before the first domain\n",
    "- **C-terminus**: Region after the last domain  \n",
    "- **Inner linkers**: Regions between domains\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Fetches domain annotations from InterPro\n",
    "2. Calculates linker regions between domains\n",
    "3. Analyzes amino acid composition and length distributions\n",
    "4. Creates visualizations\n",
    "5. Clusters linkers by length for comparative analysis\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. **Data Collection**: Query InterPro API for domain annotations and merge overlaps\n",
    "2. **Linker Calculation**: Compute regions not covered by domains\n",
    "3. **Filtering**: Remove proteins with no domains\n",
    "4. **Statistical Analysis**: Calculate composition and length statistics\n",
    "5. **Visualization**: Generate plots and comparisons\n",
    "6. **Clustering**: Group linkers by length characteristics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2624ef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Core Functions: Domain Retrieval from InterPro\n",
    "\n",
    "These functions query the InterPro API to retrieve protein domain annotations.\n",
    "\n",
    "**Key functions:**\n",
    "- `_get_protein_seq()`: Fetches protein sequence from UniProt\n",
    "- `_extract_domains_from_source()`: Queries specific InterPro databases (Pfam, SMART, etc.)\n",
    "- `_get_interpro_domains()`: Aggregates domains from all databases\n",
    "- `summarize_protein_domains_dict()`: Returns complete domain information for a protein\n",
    "- `filter_proteins_without_domains()`: Removes proteins with no domain annotations\n",
    "\n",
    "**Features:**\n",
    "- Pagination support for large result sets\n",
    "- Representative-only filtering to reduce redundancy\n",
    "- Multi-database querying (Pfam, SMART, CDD, etc.)\n",
    "- Quality control filtering for meaningful analysis\n",
    "\"\"\"\n",
    "import requests, math, os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "INTERPRO_URL = \"https://www.ebi.ac.uk/interpro/api\"\n",
    "CATH_URL = f\"https://www.cathdb.info/version/v4_3_0/api/rest/uniprot_to_funfam\"\n",
    "\n",
    "\n",
    "KNOWN_DATABASES = [\"InterPro\", \"cathgene3d\", \"cdd\", \"HAMAP\", \"panther\", \"Pfam\", \"PIRSF\", \"PRINTS\", \"ssf\", \"antifam\",\n",
    "                        \"PROSITE\" \"Patterns\", \"PROSITE\", \"profile\", \"smart\", \"SFLD\", \"SUPERFAMILY\", \"ncbifam\"]\n",
    "# ================================\n",
    "# FINAL CONSOLIDATED FUNCTIONS\n",
    "# ================================\n",
    "\n",
    "def _get_protein_seq(uniprot_acc):\n",
    "    \"\"\"\n",
    "    Fetch protein sequence from UniProt API.\n",
    "    Returns tuple (sequence, length) or (None, -1) if not found.\n",
    "    \"\"\"\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/{uniprot_acc}.json\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        seq = data.get(\"sequence\", {}).get(\"value\")\n",
    "        if seq:\n",
    "            return seq, len(seq)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {uniprot_acc}: {e}\")\n",
    "    return None, -1\n",
    "\n",
    "# Helper function to extract domains from a specific source\n",
    "def _extract_domains_from_source(uniprot_acc, source, representative_only=False):\n",
    "    \"\"\"\n",
    "    Extract domains from a specific InterPro database source with pagination support.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    uniprot_acc : str\n",
    "        UniProt accession\n",
    "    source : str\n",
    "        Database source ('pfam', 'smart', etc.)\n",
    "    representative_only : bool\n",
    "        If True, only return representative domain hits\n",
    "    \"\"\"\n",
    "    url = f\"{INTERPRO_URL}/entry/{source}/protein/UniProt/{uniprot_acc}?page_size=200\"\n",
    "    domains = []\n",
    "\n",
    "    while url:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=30)\n",
    "            if response.status_code == 404:\n",
    "                break\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            results = data.get(\"results\", []) if isinstance(data, dict) else data\n",
    "\n",
    "            for entry in results:\n",
    "                # Get entry metadata\n",
    "                meta = entry.get(\"metadata\", {})\n",
    "                entry_acc = meta.get(\"accession\")\n",
    "                entry_name = meta.get(\"name\", \"\")\n",
    "                entry_type = meta.get(\"type\")\n",
    "                source_db = meta.get(\"source_database\")\n",
    "\n",
    "                # Find target protein and extract coordinates\n",
    "                for protein in entry.get(\"proteins\", []):\n",
    "                    protein_acc = protein.get(\"accession\")\n",
    "                    if protein_acc and protein_acc.upper() == uniprot_acc.upper():\n",
    "                        # Check for matches (entry_protein_locations)\n",
    "                        for location in protein.get(\"entry_protein_locations\", []):\n",
    "                            # KEY: representative flag is at the LOCATION level, not fragment level\n",
    "                            is_representative = bool(location.get(\"representative\"))\n",
    "\n",
    "                            # KEY FILTER: Skip non-representative locations if filtering\n",
    "                            if representative_only and not is_representative:\n",
    "                                continue\n",
    "\n",
    "                            for fragment in location.get(\"fragments\", []):\n",
    "                                # Skip domains with null/None/empty names\n",
    "                                if entry_name and entry_name != '':\n",
    "                                    domains.append({\n",
    "                                        \"accession\": entry_acc,\n",
    "                                        \"name\": entry_name,\n",
    "                                        \"type\": entry_type,\n",
    "                                        \"source\": source_db,\n",
    "                                        \"start\": fragment.get(\"start\"),\n",
    "                                        \"end\": fragment.get(\"end\"),\n",
    "                                        \"score\": location.get(\"score\"),\n",
    "                                        \"representative\": is_representative\n",
    "                                    })\n",
    "\n",
    "            # Follow pagination\n",
    "            url = data.get(\"next\") if isinstance(data, dict) else None\n",
    "\n",
    "        except requests.exceptions.RequestException:\n",
    "            break\n",
    "\n",
    "    return domains\n",
    "\n",
    "def _get_interpro_domains(uniprot_acc, source=None, representative_only=False):\n",
    "    \"\"\"\n",
    "    Get protein domain coordinates from InterPro API.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    uniprot_acc : str\n",
    "        UniProt accession (e.g., 'P28482')\n",
    "    source : str, list, or None\n",
    "        - str: Query a specific database ('pfam', 'smart', 'prosite', etc.)\n",
    "        - list: Query multiple specific databases (['pfam', 'smart'])\n",
    "        - None: Query all available databases (default: representative hits only)\n",
    "    representative_only : bool or None\n",
    "        - True: Only return representative domain hits (reduces redundancy)\n",
    "        - False: Return all hits\n",
    "        - None: Auto-decide (True if source=None, False if source is specified)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list of dict\n",
    "        Each domain contains: accession, name, type, source, start, end, score, representative\n",
    "        Sorted by start coordinate, then end coordinate\n",
    "    \"\"\"\n",
    "\n",
    "    if source:\n",
    "        # Handle list of databases\n",
    "        if isinstance(source, list):\n",
    "            all_domains = []\n",
    "            available_dbs = []\n",
    "\n",
    "            print(f\"Querying specified databases for {uniprot_acc}: {', '.join(source)}\")\n",
    "            if representative_only:\n",
    "                print(\"(filtering for representative hits only)\")\n",
    "\n",
    "            for db in source:\n",
    "                domains = _extract_domains_from_source(uniprot_acc, db, representative_only)\n",
    "                if domains:\n",
    "                    available_dbs.append(f\"{db}({len(domains)})\")\n",
    "                    all_domains.extend(domains)\n",
    "\n",
    "            if available_dbs:\n",
    "                print(f\"Found data in: {', '.join(available_dbs)}\")\n",
    "            else:\n",
    "                print(f\"No data found in specified databases\")\n",
    "\n",
    "            # Sort by start, then end coordinate\n",
    "            return sorted(all_domains, key=lambda x: (x[\"start\"] if x[\"start\"] is not None else float('inf'),\n",
    "                                                       x[\"end\"] if x[\"end\"] is not None else float('inf')))\n",
    "\n",
    "        # Handle single database (string)\n",
    "        else:\n",
    "            domains = _extract_domains_from_source(uniprot_acc, source, representative_only)\n",
    "            # Sort by start, then end coordinate\n",
    "            return sorted(domains, key=lambda x: (x[\"start\"] if x[\"start\"] is not None else float('inf'),\n",
    "                                                   x[\"end\"] if x[\"end\"] is not None else float('inf')))\n",
    "\n",
    "    else:\n",
    "        # Query all known databases (default: representative only)\n",
    "        all_domains = []\n",
    "        available_dbs = []\n",
    "\n",
    "        print(f\"Scanning all databases for {uniprot_acc}...\")\n",
    "        if representative_only:\n",
    "            print(\"(filtering for representative hits only)\")\n",
    "\n",
    "        for db in KNOWN_DATABASES:\n",
    "            domains = _extract_domains_from_source(uniprot_acc, db, representative_only)\n",
    "            if domains:\n",
    "                available_dbs.append(f\"{db}({len(domains)})\")\n",
    "                all_domains.extend(domains)\n",
    "\n",
    "        print(f\"Found data in: {', '.join(available_dbs)}\")\n",
    "        # Sort by start, then end coordinate\n",
    "        return sorted(all_domains, key=lambda x: (x[\"start\"] if x[\"start\"] is not None else float('inf'),\n",
    "                                                   x[\"end\"] if x[\"end\"] is not None else float('inf')))\n",
    "\n",
    "\n",
    "def filter_proteins_without_domains(proteins_dict, domains_list=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Filter out proteins that have no domain annotations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    proteins_dict : dict\n",
    "        Dictionary with protein accessions as keys and domain/linker info as values\n",
    "        Format: {acc: {'domains': [...], 'linkers': [...]}}\n",
    "    domains_list : list, optional\n",
    "        Analysis-ready list of domain data to filter in parallel\n",
    "    save_path : str, optional\n",
    "        Path to save filtered proteins_dict as JSON\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple of (filtered_proteins_dict, filtered_domains_list, stats)\n",
    "        - filtered_proteins_dict: Dict with only proteins having domains\n",
    "        - filtered_domains_list: Filtered domains list (None if not provided)\n",
    "        - stats: Dict with 'total_before', 'no_domains', 'total_after', 'removed'\n",
    "    \"\"\"\n",
    "    # Count before filtering\n",
    "    total_before = len(proteins_dict)\n",
    "    no_domains_count = sum(1 for data in proteins_dict.values() if len(data['domains']) == 0)\n",
    "\n",
    "    # Filter proteins_dict - keep only proteins with at least one domain\n",
    "    filtered_proteins = {\n",
    "        accession: data\n",
    "        for accession, data in proteins_dict.items()\n",
    "        if len(data['domains']) > 0\n",
    "    }\n",
    "\n",
    "    # Filter domains list if provided\n",
    "    filtered_domains = None\n",
    "    if domains_list is not None:\n",
    "        filtered_domains = [\n",
    "            protein_data\n",
    "            for protein_data in domains_list\n",
    "            if len(protein_data['domains']) > 0\n",
    "        ]\n",
    "\n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        'total_before': total_before,\n",
    "        'no_domains': no_domains_count,\n",
    "        'total_after': len(filtered_proteins),\n",
    "        'removed': total_before - len(filtered_proteins)\n",
    "    }\n",
    "\n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        import json\n",
    "        with open(os.path.expanduser(save_path), 'w') as f:\n",
    "            json.dump(filtered_proteins, f, indent=2)\n",
    "\n",
    "    return filtered_proteins, filtered_domains, stats\n",
    "\n",
    "def summarize_protein_domains_dict(uniprot_acc, source=None, representative_only=True, skip_if_no_domains=False):\n",
    "    \"\"\"\n",
    "    Return a dict with protein name, domains (with coords and scores), and protein length.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    uniprot_acc : str\n",
    "        UniProt accession\n",
    "    source : str, list, or None\n",
    "        - str: Query a specific database\n",
    "        - list: Query multiple specific databases\n",
    "        - None: Query all available databases\n",
    "    representative_only : bool or None\n",
    "        - True: Only return representative domain hits\n",
    "        - False: Return all hits\n",
    "    skip_if_no_domains : bool\n",
    "        - True: Return None if protein has no domains (default: False)\n",
    "        - False: Return result even if no domains found\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict with keys: uniprot_acc, protein_length, domains (sorted by start coordinate), available_databases\n",
    "    Or None if skip_if_no_domains=True and no domains found\n",
    "    \"\"\"\n",
    "    # Get protein length\n",
    "    seq, length = _get_protein_seq(uniprot_acc)\n",
    "    if length <= 0:\n",
    "        return None\n",
    "\n",
    "    # Get domains (this will scan databases if source=None)\n",
    "    # Domains are already sorted by get_interpro_domains\n",
    "    domains = _get_interpro_domains(uniprot_acc, source=source, representative_only=representative_only)\n",
    "\n",
    "    # Skip if no domains found and filtering is enabled\n",
    "    if skip_if_no_domains and len(domains) == 0:\n",
    "        return None\n",
    "\n",
    "    # Build available databases dict from domains already retrieved\n",
    "    if source:\n",
    "        if isinstance(source, list):\n",
    "            databases = {}\n",
    "            for domain in domains:\n",
    "                db = domain.get('source')\n",
    "                if db:\n",
    "                    databases[db] = databases.get(db, 0) + 1\n",
    "        else:\n",
    "            databases = {source: len(domains)}\n",
    "    else:\n",
    "        # Extract database info from the domains we already have\n",
    "        databases = {}\n",
    "        for domain in domains:\n",
    "            db = domain.get('source')\n",
    "            if db:\n",
    "                databases[db] = databases.get(db, 0) + 1\n",
    "\n",
    "    # Build output dict\n",
    "    result = {\n",
    "        \"uniprot_acc\": uniprot_acc,\n",
    "        \"protein_length\": length,\n",
    "        \"domains\": domains,\n",
    "        \"available_databases\": databases,\n",
    "        \"sequence\": seq\n",
    "    }\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d1e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. Linker Calculation Functions\n",
    "\n",
    "These functions compute linker regions by finding gaps between protein domains.\n",
    "\n",
    "**Key functions:**\n",
    "- `_merge_overlapping_domains()`: Merges overlapping/nested domains to avoid spurious linkers\n",
    "- `compute_linker_regions()`: Calculates regions not covered by any domain\n",
    "- `add_linkers_to_result()`: Adds linker information to domain data\n",
    "- `get_linker_aa_count()`: Counts amino acids in a linker region\n",
    "\n",
    "**Critical feature:**\n",
    "Domain overlap handling ensures linkers are only calculated in true gaps between domains, not within overlapping domain annotations.\n",
    "\"\"\"\n",
    "def _merge_overlapping_domains(domains):\n",
    "    \"\"\"\n",
    "    Merge overlapping or nested domains into non-overlapping regions.\n",
    "\n",
    "    This ensures linkers are calculated only in regions NOT covered by any domain.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    domains : list of dict\n",
    "        List of domain dictionaries with 'start' and 'end' keys\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list of dict\n",
    "        Merged non-overlapping domain regions, sorted by start coordinate\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    Input:  [{'start': 3, 'end': 330}, {'start': 60, 'end': 70}, {'start': 120, 'end': 150}]\n",
    "    Output: [{'start': 3, 'end': 330}]  # nested domains merged into parent\n",
    "\n",
    "    Input:  [{'start': 10, 'end': 50}, {'start': 45, 'end': 80}, {'start': 100, 'end': 120}]\n",
    "    Output: [{'start': 10, 'end': 80}, {'start': 100, 'end': 120}]  # overlapping merged\n",
    "    \"\"\"\n",
    "    if not domains:\n",
    "        return []\n",
    "\n",
    "    # Sort domains by start position, then by end position (descending for tie-breaking)\n",
    "    sorted_domains = sorted(domains, key=lambda x: (x[\"start\"] if x[\"start\"] is not None else float('inf'),\n",
    "                                                     -(x[\"end\"] if x[\"end\"] is not None else 0)))\n",
    "\n",
    "    merged = []\n",
    "    current_start = sorted_domains[0][\"start\"]\n",
    "    current_end = sorted_domains[0][\"end\"]\n",
    "\n",
    "    for domain in sorted_domains[1:]:\n",
    "        # Check if this domain overlaps or is nested within the current merged region\n",
    "        if domain[\"start\"] <= current_end + 1:  # Overlapping or adjacent (allow 1 gap to merge adjacent)\n",
    "            # Extend the current merged region to include this domain\n",
    "            current_end = max(current_end, domain[\"end\"])\n",
    "        else:\n",
    "            # No overlap, save current merged region and start a new one\n",
    "            merged.append({\"start\": current_start, \"end\": current_end})\n",
    "            current_start = domain[\"start\"]\n",
    "            current_end = domain[\"end\"]\n",
    "\n",
    "    # Add the last merged region\n",
    "    merged.append({\"start\": current_start, \"end\": current_end})\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "def compute_linker_regions(domains, protein_length=None):\n",
    "    \"\"\"\n",
    "    Compute linker regions between domains, after merging overlapping/nested domains.\n",
    "\n",
    "    CRITICAL: Overlapping and nested domains are merged first to ensure linkers\n",
    "    are only in regions NOT covered by any domain.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    domains : list of dict\n",
    "        List of domain dictionaries with 'start' and 'end' keys\n",
    "        (e.g., result['domains'] from summarize_protein_domains_dict)\n",
    "    protein_length : int, optional\n",
    "        Total protein length. If provided, includes N-terminal and C-terminal regions.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list of dict\n",
    "        Each linker region contains: start, end\n",
    "        Sorted by start coordinate\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    Domains: [{'start': 3, 'end': 330}, {'start': 60, 'end': 70}]\n",
    "    After merge: [{'start': 3, 'end': 330}]  # domain 60-70 is nested, merged\n",
    "    Linkers: [{'start': 1, 'end': 2}, {'start': 331, 'end': protein_length}]\n",
    "    \"\"\"\n",
    "    if not domains:\n",
    "        if protein_length:\n",
    "            return [{\"start\": 1, \"end\": protein_length}]\n",
    "        return []\n",
    "\n",
    "    # CRITICAL FIX: Merge overlapping/nested domains first\n",
    "    merged_domains = _merge_overlapping_domains(domains)\n",
    "\n",
    "    linkers = []\n",
    "\n",
    "    # Check for N-terminal linker (before first merged domain)\n",
    "    first_domain = merged_domains[0]\n",
    "    if first_domain[\"start\"] > 1:\n",
    "        linker_start = 1\n",
    "        linker_end = first_domain[\"start\"] - 1\n",
    "        # Only add if start <= end (length >= 1)\n",
    "        if linker_start <= linker_end:\n",
    "            linkers.append({\n",
    "                \"start\": linker_start,\n",
    "                \"end\": linker_end\n",
    "            })\n",
    "\n",
    "    # Find gaps between consecutive merged domains\n",
    "    for i in range(len(merged_domains) - 1):\n",
    "        current_domain = merged_domains[i]\n",
    "        next_domain = merged_domains[i + 1]\n",
    "\n",
    "        # Check if there's a gap between current domain end and next domain start\n",
    "        gap_start = current_domain[\"end\"] + 1\n",
    "        gap_end = next_domain[\"start\"] - 1\n",
    "\n",
    "        # Only add if start <= end (length >= 1)\n",
    "        if gap_start <= gap_end:\n",
    "            linkers.append({\n",
    "                \"start\": gap_start,\n",
    "                \"end\": gap_end\n",
    "            })\n",
    "\n",
    "    # Check for C-terminal linker (after last merged domain)\n",
    "    if protein_length:\n",
    "        last_domain = merged_domains[-1]\n",
    "        if last_domain[\"end\"] < protein_length:\n",
    "            linker_start = last_domain[\"end\"] + 1\n",
    "            linker_end = protein_length\n",
    "            # Only add if start <= end (length >= 1)\n",
    "            if linker_start <= linker_end:\n",
    "                linkers.append({\n",
    "                    \"start\": linker_start,\n",
    "                    \"end\": linker_end\n",
    "                })\n",
    "\n",
    "    return linkers\n",
    "\n",
    "def add_linkers_to_result(result):\n",
    "    \"\"\"\n",
    "    Add linker regions to a result dict from summarize_protein_domains_dict.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    result : dict\n",
    "        Result dictionary from summarize_protein_domains_dict with keys:\n",
    "        'domains', 'protein_length', etc.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Same result dict with added 'linkers' key containing linker regions\n",
    "    \"\"\"\n",
    "    linkers = compute_linker_regions(result['domains'], result.get('protein_length'))\n",
    "    result['linkers'] = linkers\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_linker_aa_count(linker, seq):\n",
    "    \"\"\"\n",
    "    Count amino acids in a linker region.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    linker : dict\n",
    "        Linker dictionary with 'start' and 'end' keys (1-indexed)\n",
    "    seq : str\n",
    "        Full protein sequence\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with amino acid single-letter codes as keys and counts as values\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "\n",
    "    # Extract linker sequence (convert to 0-indexed)\n",
    "    start = linker['start'] - 1\n",
    "    end = linker['end']\n",
    "    linker_seq = seq[start:end]\n",
    "\n",
    "    # Count amino acids\n",
    "    aa_count = Counter(linker_seq)\n",
    "\n",
    "    return dict(aa_count)\n",
    "\n",
    "\n",
    "def get_multiple_linkers_amino_acid_count(linker_seq_pairs):\n",
    "    \"\"\"\n",
    "    Count amino acids across multiple linker regions.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    linker_seq_pairs : list of tuples\n",
    "        List of (linker, seq) pairs where:\n",
    "        - linker is a dict with 'start' and 'end' keys (1-indexed)\n",
    "        - seq is the full protein sequence string\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with amino acid single-letter codes as keys and total counts as values\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "\n",
    "    total_count = Counter()\n",
    "\n",
    "    for linker, seq in linker_seq_pairs:\n",
    "        linker_aa_count = get_linker_aa_count(linker, seq)\n",
    "        total_count.update(linker_aa_count)\n",
    "\n",
    "    return dict(total_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595f0321",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. Data Formatting Functions\n",
    "\n",
    "Converts domain and linker data into simplified, analysis-ready formats.\n",
    "\n",
    "**Key function:**\n",
    "- `format_protein_structure()`: Simplifies protein data structure\n",
    "\n",
    "**Output format:**\n",
    "```python\n",
    "{\n",
    "    'domains': [('Domain_name', start, end), ...],\n",
    "    'linkers': [('linker_type', start, end), ...]\n",
    "}\n",
    "```\n",
    "\n",
    "Where linker_type is:\n",
    "- `'n-terminus'`: Before first domain\n",
    "- `'c-terminus'`: After last domain\n",
    "- `'inner'`: Between domains\n",
    "\"\"\"\n",
    "def format_protein_structure(result):\n",
    "    \"\"\"\n",
    "    Format protein domain and linker data into simplified structure.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    result : dict\n",
    "        Result from summarize_protein_domains_dict with linkers added\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict with keys:\n",
    "        - domains: list of tuples (domain_name, start, end)\n",
    "        - linkers: list of tuples (type, start, end)\n",
    "            where type is 'n-terminus', 'c-terminus', or 'inner' for inter-domain\n",
    "    \"\"\"\n",
    "    if not result:\n",
    "        return None\n",
    "\n",
    "    protein_length = result.get('protein_length')\n",
    "    domains_list = result.get('domains', [])\n",
    "    linkers_list = result.get('linkers', [])\n",
    "\n",
    "    # Format domains as (name, start, end), excluding domains with null/None names\n",
    "    formatted_domains = [\n",
    "        (domain['name'], domain['start'], domain['end']) for domain in domains_list\n",
    "        if domain.get('name') is not None and domain.get('name') != ''\n",
    "    ]\n",
    "\n",
    "    # Format linkers as (type, start, end)\n",
    "    formatted_linkers = []\n",
    "    for linker in linkers_list:\n",
    "        start = linker['start']\n",
    "        end = linker['end']\n",
    "\n",
    "        # Determine if n-terminus, c-terminus, or inner (between domains)\n",
    "        if start == 1:\n",
    "            linker_type = 'n-terminus'\n",
    "        elif end == protein_length:\n",
    "            linker_type = 'c-terminus'\n",
    "        else:\n",
    "            linker_type = 'inner'\n",
    "\n",
    "        formatted_linkers.append((linker_type, start, end))\n",
    "\n",
    "    return {\n",
    "        'uniprot_acc': result.get('uniprot_acc'),\n",
    "        'domains': formatted_domains,\n",
    "        'linkers': formatted_linkers\n",
    "    }\n",
    "#----------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a26d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4. Analysis and Visualization Functions\n",
    "\n",
    "Comprehensive analysis and plotting functions for linkers and domains.\n",
    "\n",
    "**Analysis functions:**\n",
    "- `analyze_linkers()`: Amino acid composition and length statistics for N/C/inner linkers\n",
    "- `analyze_domains()`: Amino acid composition and length statistics for domains\n",
    "- `compare_domains_vs_linkers()`: Direct comparison between domains and linkers\n",
    "\n",
    "**Visualization functions:**\n",
    "- `visualize_linker_composition()`: Creates multiple plots for linker analysis\n",
    "  - Individual amino acid distributions by linker type\n",
    "  - Percentage composition comparisons\n",
    "  - Length distribution box plots\n",
    "- `visualize_domain_composition()`: Domain-specific visualizations\n",
    "- `visualize_linker_clusters()`: Length-based cluster visualizations\n",
    "\n",
    "**Output:**\n",
    "All plots saved as high-resolution PNG files (300 DPI) to specified directory.\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# ================================\n",
    "# Helper Functions\n",
    "# ================================\n",
    "\n",
    "def _collect_domains(sequences_dict, domains):\n",
    "    \"\"\"Collect all domains with their sequences and metadata.\"\"\"\n",
    "    domain_list = []\n",
    "    domain_lengths = []\n",
    "    for protein_data in domains:\n",
    "        accession = protein_data['uniprot_acc']\n",
    "        seq = sequences_dict.get(accession)\n",
    "        if seq:\n",
    "            for domain in protein_data['domains']:\n",
    "                start, end = domain['start'], domain['end']\n",
    "                length = end - start + 1\n",
    "                domain_list.append((domain, seq))\n",
    "                domain_lengths.append(length)\n",
    "    return domain_list, domain_lengths\n",
    "\n",
    "\n",
    "def _collect_linkers_by_type(proteins_dict, sequences_dict, domains):\n",
    "    \"\"\"Collect and categorize linkers by type from protein data.\"\"\"\n",
    "    n_terminus_linkers, c_terminus_linkers, inner_linkers = [], [], []\n",
    "    n_terminus_lengths, c_terminus_lengths, inner_lengths = [], [], []\n",
    "\n",
    "    for accession, data in proteins_dict.items():\n",
    "        linkers_formatted = data['linkers']\n",
    "        protein_data = next((d for d in domains if d['uniprot_acc'] == accession), None)\n",
    "        if protein_data:\n",
    "            linkers_with_coords = protein_data['linkers']\n",
    "            seq = sequences_dict.get(accession)\n",
    "            if seq and linkers_with_coords:\n",
    "                for i, linker_coord in enumerate(linkers_with_coords):\n",
    "                    if i < len(linkers_formatted):\n",
    "                        linker_type, start, end = linkers_formatted[i]\n",
    "                        linker_length = end - start + 1\n",
    "                        if linker_type == 'n-terminus':\n",
    "                            n_terminus_linkers.append((linker_coord, seq))\n",
    "                            n_terminus_lengths.append(linker_length)\n",
    "                        elif linker_type == 'c-terminus':\n",
    "                            c_terminus_linkers.append((linker_coord, seq))\n",
    "                            c_terminus_lengths.append(linker_length)\n",
    "                        elif linker_type == 'inner':\n",
    "                            inner_linkers.append((linker_coord, seq))\n",
    "                            inner_lengths.append(linker_length)\n",
    "    return (n_terminus_linkers, c_terminus_linkers, inner_linkers,\n",
    "            n_terminus_lengths, c_terminus_lengths, inner_lengths)\n",
    "\n",
    "\n",
    "def _count_domain_amino_acids(domain_list):\n",
    "    \"\"\"Count amino acids in all domains.\"\"\"\n",
    "    domain_counter = Counter()\n",
    "    for domain, seq in domain_list:\n",
    "        start, end = domain['start'] - 1, domain['end']\n",
    "        domain_seq = seq[start:end]\n",
    "        domain_counter.update(domain_seq)\n",
    "    return domain_counter\n",
    "\n",
    "\n",
    "def _count_amino_acids_by_type(n_terminus_linkers, c_terminus_linkers, inner_linkers):\n",
    "    \"\"\"Count amino acids for each linker type.\"\"\"\n",
    "    n_terminus_counter = Counter()\n",
    "    c_terminus_counter = Counter()\n",
    "    inner_counter = Counter()\n",
    "    for linker, seq in n_terminus_linkers:\n",
    "        aa_count = get_linker_aa_count(linker, seq)\n",
    "        n_terminus_counter.update(aa_count)\n",
    "    for linker, seq in c_terminus_linkers:\n",
    "        aa_count = get_linker_aa_count(linker, seq)\n",
    "        c_terminus_counter.update(aa_count)\n",
    "    for linker, seq in inner_linkers:\n",
    "        aa_count = get_linker_aa_count(linker, seq)\n",
    "        inner_counter.update(aa_count)\n",
    "    return n_terminus_counter, c_terminus_counter, inner_counter\n",
    "\n",
    "\n",
    "# ================================\n",
    "# Main Analysis Functions\n",
    "# ================================\n",
    "\n",
    "def analyze_domains(sequences_dict, domains):\n",
    "    \"\"\"Analyze domain amino acid composition and lengths.\"\"\"\n",
    "    domain_list, domain_lengths = _collect_domains(sequences_dict, domains)\n",
    "    domain_counter = _count_domain_amino_acids(domain_list)\n",
    "    print(f\"\\nDomain amino acid counts ({len(domain_list)} domains):\")\n",
    "    print(dict(domain_counter))\n",
    "    return domain_counter, len(domain_list), domain_lengths\n",
    "\n",
    "\n",
    "def analyze_linkers(proteins_dict, sequences_dict, domains):\n",
    "    \"\"\"Analyze linker amino acid composition.\"\"\"\n",
    "    (n_terminus_linkers, c_terminus_linkers, inner_linkers,\n",
    "     n_terminus_lengths, c_terminus_lengths, inner_lengths) = _collect_linkers_by_type(\n",
    "        proteins_dict, sequences_dict, domains)\n",
    "    n_terminus_counter, c_terminus_counter, inner_counter = _count_amino_acids_by_type(\n",
    "        n_terminus_linkers, c_terminus_linkers, inner_linkers)\n",
    "    print(f\"\\nN-terminus amino acid counts ({len(n_terminus_linkers)} linkers):\")\n",
    "    print(dict(n_terminus_counter))\n",
    "    print(f\"\\nC-terminus amino acid counts ({len(c_terminus_linkers)} linkers):\")\n",
    "    print(dict(c_terminus_counter))\n",
    "    print(f\"\\nInner linker amino acid counts ({len(inner_linkers)} linkers):\")\n",
    "    print(dict(inner_counter))\n",
    "    return (n_terminus_counter, c_terminus_counter, inner_counter,\n",
    "            len(n_terminus_linkers), len(c_terminus_linkers), len(inner_linkers),\n",
    "            n_terminus_lengths, c_terminus_lengths, inner_lengths)\n",
    "\n",
    "\n",
    "# ================================\n",
    "# Visualization Functions\n",
    "# ================================\n",
    "\n",
    "def visualize_linker_composition(n_term_count, c_term_count, inner_count, n_count, c_count, i_count,\n",
    "                                n_lengths=None, c_lengths=None, i_lengths=None,\n",
    "                                domain_counter=None, domain_count=0, domain_lengths=None,\n",
    "                                save_dir=None):\n",
    "    \"\"\"Create visualizations for linker amino acid composition.\"\"\"\n",
    "    if save_dir:\n",
    "        save_dir = os.path.expanduser(save_dir)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare amino acid data\n",
    "    amino_acids = sorted(set(list(n_term_count.keys()) + list(c_term_count.keys()) + list(inner_count.keys())))\n",
    "    n_counts = [n_term_count.get(aa, 0) for aa in amino_acids]\n",
    "    c_counts = [c_term_count.get(aa, 0) for aa in amino_acids]\n",
    "    i_counts = [inner_count.get(aa, 0) for aa in amino_acids]\n",
    "\n",
    "    # Create combined comparison plot\n",
    "    _, ax = plt.subplots(figsize=(18, 6))\n",
    "    x = range(len(amino_acids))\n",
    "    width = 0.25\n",
    "    ax.bar([i - width for i in x], n_counts, width, label='N-terminus', color='steelblue', edgecolor='black')\n",
    "    ax.bar([i for i in x], c_counts, width, label='C-terminus', color='coral', edgecolor='black')\n",
    "    ax.bar([i + width for i in x], i_counts, width, label='Inner', color='mediumseagreen', edgecolor='black')\n",
    "    ax.set_xlabel('Amino Acid', fontsize=12)\n",
    "    ax.set_ylabel('Count', fontsize=12)\n",
    "    ax.set_title('Amino Acid Composition: N-terminus vs C-terminus vs Inner Linkers', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(amino_acids)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    if save_dir:\n",
    "        plt.savefig(os.path.join(save_dir, 'linker_aa_composition.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot length distribution if length data is provided\n",
    "    if n_lengths is not None and c_lengths is not None and i_lengths is not None:\n",
    "        data_to_plot = [n_lengths, c_lengths, i_lengths]\n",
    "        labels = ['N-terminus', 'C-terminus', 'Inner']\n",
    "        colors = ['steelblue', 'coral', 'mediumseagreen']\n",
    "\n",
    "        _, ax = plt.subplots(figsize=(12, 6))\n",
    "        bp = ax.boxplot(data_to_plot, labels=labels, patch_artist=True, showmeans=True,\n",
    "                        meanprops=dict(marker='D', markerfacecolor='red', markeredgecolor='darkred', markersize=10),\n",
    "                        medianprops=dict(color='black', linewidth=2))\n",
    "        for patch, color in zip(bp['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        ax.set_ylabel('Length (amino acids)', fontsize=12)\n",
    "        ax.set_title('Linker Length Distributions', fontsize=14, fontweight='bold')\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'linker_length_distributions.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def visualize_domain_composition(domain_counter, domain_count, domain_lengths, save_dir=None):\n",
    "    \"\"\"Create visualizations for domain amino acid composition.\"\"\"\n",
    "    if save_dir:\n",
    "        save_dir = os.path.expanduser(save_dir)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    amino_acids = sorted(domain_counter.keys())\n",
    "    domain_counts = [domain_counter.get(aa, 0) for aa in amino_acids]\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(amino_acids, domain_counts, color='purple', edgecolor='black', alpha=0.7)\n",
    "    ax.set_xlabel('Amino Acid', fontsize=12)\n",
    "    ax.set_ylabel('Count', fontsize=12)\n",
    "    ax.set_title(f'Domain Amino Acid Distribution ({domain_count} domains)', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    if save_dir:\n",
    "        plt.savefig(os.path.join(save_dir, 'domain_aa_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compare_domains_vs_linkers(domain_counter, domain_count, n_term_count, c_term_count,\n",
    "                                inner_count, n_count, c_count, i_count, save_dir=None):\n",
    "    \"\"\"Create comparison visualizations between domains and linkers.\"\"\"\n",
    "    if save_dir:\n",
    "        save_dir = os.path.expanduser(save_dir)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    all_linker_counter = Counter()\n",
    "    all_linker_counter.update(n_term_count)\n",
    "    all_linker_counter.update(c_term_count)\n",
    "    all_linker_counter.update(inner_count)\n",
    "    total_linker_count = n_count + c_count + i_count\n",
    "\n",
    "    all_amino_acids = sorted(set(list(domain_counter.keys()) + list(all_linker_counter.keys())))\n",
    "    domain_total = sum(domain_counter.values())\n",
    "    linker_total = sum(all_linker_counter.values())\n",
    "    domain_pcts = [(domain_counter.get(aa, 0) / domain_total * 100) if domain_total > 0 else 0 for aa in all_amino_acids]\n",
    "    linker_pcts = [(all_linker_counter.get(aa, 0) / linker_total * 100) if linker_total > 0 else 0 for aa in all_amino_acids]\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(16, 6))\n",
    "    x = range(len(all_amino_acids))\n",
    "    width = 0.35\n",
    "    ax.bar([i - width/2 for i in x], domain_pcts, width, label=f'Domains (n={domain_count})', color='purple', edgecolor='black', alpha=0.7)\n",
    "    ax.bar([i + width/2 for i in x], linker_pcts, width, label=f'Linkers (n={total_linker_count})', color='orange', edgecolor='black', alpha=0.7)\n",
    "    ax.set_xlabel('Amino Acid', fontsize=12)\n",
    "    ax.set_ylabel('Percentage (%)', fontsize=12)\n",
    "    ax.set_title('Amino Acid Composition: Domains vs Linkers', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(all_amino_acids)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    if save_dir:\n",
    "        plt.savefig(os.path.join(save_dir, 'domains_vs_linkers_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "def cluster_linkers_by_length(proteins_dict, method='percentile', custom_thresholds=None):\n",
    "    \"\"\"\n",
    "    Cluster linkers into 3 length categories: short, medium, long.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    proteins_dict : dict\n",
    "        Dictionary with protein accessions as keys and domains/linkers as values\n",
    "        Format: {acc: {'domains': [(name, start, end)...], 'linkers': [(type, start, end)...]}}\n",
    "    method : str\n",
    "        Clustering method:\n",
    "        - 'percentile': Use 33rd and 67th percentiles (default)\n",
    "        - 'kmeans': Use K-means clustering (data-driven)\n",
    "        - 'std': Use mean ± 0.5*std for boundaries\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict with keys:\n",
    "        - 'clusters': dict with 'short', 'medium', 'long' keys, each containing list of\n",
    "          (accession, linker_type, start, end, length) tuples\n",
    "        - 'thresholds': dict with 'short_max', 'medium_max' values\n",
    "        - 'statistics': dict with stats for each cluster\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Collect all linker lengths\n",
    "    all_lengths = []\n",
    "    linker_data = []  # (accession, linker_type, start, end, length)\n",
    "\n",
    "    for accession, data in proteins_dict.items():\n",
    "        for linker in data['linkers']:\n",
    "            linker_type, start, end = linker\n",
    "            length = end - start + 1\n",
    "            all_lengths.append(length)\n",
    "            linker_data.append((accession, linker_type, start, end, length))\n",
    "\n",
    "    if not all_lengths:\n",
    "        return {'clusters': {'short': [], 'medium': [], 'long': []},\n",
    "                'thresholds': {}, 'statistics': {}}\n",
    "\n",
    "    all_lengths_array = np.array(all_lengths)\n",
    "\n",
    "    # Determine thresholds based on method\n",
    "    if method == 'percentile':\n",
    "        threshold1 = np.percentile(all_lengths_array, 33.33)\n",
    "        threshold2 = np.percentile(all_lengths_array, 66.67)\n",
    "        print(f\"Percentile method: 33rd percentile = {threshold1:.1f}, 67th percentile = {threshold2:.1f}\")\n",
    "\n",
    "    elif method == 'kmeans':\n",
    "        from sklearn.cluster import KMeans\n",
    "        # Reshape for sklearn\n",
    "        lengths_reshaped = all_lengths_array.reshape(-1, 1)\n",
    "        kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "        kmeans.fit(lengths_reshaped)\n",
    "\n",
    "        # Get cluster centers and sort them\n",
    "        centers = sorted(kmeans.cluster_centers_.flatten())\n",
    "        # Use midpoints between centers as thresholds\n",
    "        threshold1 = (centers[0] + centers[1]) / 2\n",
    "        threshold2 = (centers[1] + centers[2]) / 2\n",
    "        print(f\"K-means method: cluster centers = {centers}\")\n",
    "        print(f\"Thresholds: {threshold1:.1f}, {threshold2:.1f}\")\n",
    "\n",
    "    elif method == 'std':\n",
    "        mean_len = np.mean(all_lengths_array)\n",
    "        std_len = np.std(all_lengths_array)\n",
    "        # Use mean - std and mean + std for better separation\n",
    "        threshold1 = max(1, mean_len - std_len)  # Ensure threshold is at least 1\n",
    "        threshold2 = mean_len + std_len\n",
    "        print(f\"Std method: mean = {mean_len:.1f}, std = {std_len:.1f}\")\n",
    "        print(f\"Thresholds: short <= {threshold1:.1f}, medium {threshold1:.1f}-{threshold2:.1f}, long > {threshold2:.1f}\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}. Use 'percentile', 'kmeans', 'std', or 'custom'\")\n",
    "\n",
    "    # Cluster the linkers\n",
    "    clusters = {'short': [], 'medium': [], 'long': []}\n",
    "\n",
    "    for linker_info in linker_data:\n",
    "        accession, linker_type, start, end, length = linker_info\n",
    "\n",
    "        if length <= threshold1:\n",
    "            clusters['short'].append(linker_info)\n",
    "        elif length <= threshold2:\n",
    "            clusters['medium'].append(linker_info)\n",
    "        else:\n",
    "            clusters['long'].append(linker_info)\n",
    "\n",
    "    # Calculate statistics for each cluster\n",
    "    statistics = {}\n",
    "    for cluster_name, cluster_data in clusters.items():\n",
    "        if cluster_data:\n",
    "            cluster_lengths = [item[4] for item in cluster_data]  # length is index 4\n",
    "            statistics[cluster_name] = {\n",
    "                'count': len(cluster_lengths),\n",
    "                'min': np.min(cluster_lengths),\n",
    "                'max': np.max(cluster_lengths),\n",
    "                'mean': np.mean(cluster_lengths),\n",
    "                'median': np.median(cluster_lengths),\n",
    "                'std': np.std(cluster_lengths)\n",
    "            }\n",
    "        else:\n",
    "            statistics[cluster_name] = {\n",
    "                'count': 0, 'min': None, 'max': None,\n",
    "                'mean': None, 'median': None, 'std': None\n",
    "            }\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n=== Linker Clustering Summary ===\")\n",
    "    print(f\"Total linkers: {len(linker_data)}\")\n",
    "    for cluster_name in ['short', 'medium', 'long']:\n",
    "        stats = statistics[cluster_name]\n",
    "        if stats['count'] > 0:\n",
    "            print(f\"\\n{cluster_name.upper()}: {stats['count']} linkers\")\n",
    "            print(f\"  Range: {stats['min']:.0f} - {stats['max']:.0f} aa\")\n",
    "            print(f\"  Mean: {stats['mean']:.1f} ± {stats['std']:.1f} aa\")\n",
    "            print(f\"  Median: {stats['median']:.1f} aa\")\n",
    "\n",
    "    return {\n",
    "        'clusters': clusters,\n",
    "        'thresholds': {'short_max': threshold1, 'medium_max': threshold2},\n",
    "        'statistics': statistics\n",
    "    }\n",
    "\n",
    "\n",
    "def visualize_linker_clusters(cluster_results, save_dir=None):\n",
    "    \"\"\"\n",
    "    Visualize linker length clusters.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    cluster_results : dict\n",
    "        Results from cluster_linkers_by_length()\n",
    "    save_dir : str, optional\n",
    "        Directory to save plots\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    clusters = cluster_results['clusters']\n",
    "    stats = cluster_results['statistics']\n",
    "    thresholds = cluster_results['thresholds']\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    short_lengths = [item[4] for item in clusters['short']]\n",
    "    medium_lengths = [item[4] for item in clusters['medium']]\n",
    "    long_lengths = [item[4] for item in clusters['long']]\n",
    "\n",
    "    # Create figure with 2 subplots\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Plot 1: Box plot comparison\n",
    "    data_to_plot = [short_lengths, medium_lengths, long_lengths]\n",
    "    labels = ['Short', 'Medium', 'Long']\n",
    "    colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "\n",
    "    bp = ax1.boxplot(data_to_plot, labels=labels, patch_artist=True,\n",
    "                     showmeans=True,\n",
    "                     meanprops=dict(marker='D', markerfacecolor='red',\n",
    "                                   markeredgecolor='darkred', markersize=8),\n",
    "                     medianprops=dict(color='black', linewidth=2),\n",
    "                     flierprops=dict(marker='o', markerfacecolor='gray',\n",
    "                                    markersize=4, alpha=0.5))\n",
    "\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "\n",
    "    # Add statistics as text\n",
    "    for i, (cluster_name, color) in enumerate(zip(['short', 'medium', 'long'], colors), 1):\n",
    "        s = stats[cluster_name]\n",
    "        if s['count'] > 0:\n",
    "            ax1.text(i, s['mean'], f\"{s['mean']:.1f}±{s['std']:.1f}\",\n",
    "                    ha='left', va='center', fontsize=9, fontweight='bold',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "\n",
    "    ax1.set_ylabel('Length (amino acids)', fontsize=12)\n",
    "    ax1.set_title('Linker Length Clusters - Box Plot Comparison',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    ax1.legend(['Mean'], loc='upper right')\n",
    "\n",
    "    # Plot 2: Histogram with thresholds\n",
    "\n",
    "    ax2.hist([short_lengths, medium_lengths, long_lengths],\n",
    "             bins=30, label=labels, color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "    # Add threshold lines\n",
    "    ax2.axvline(thresholds['short_max'], color='red', linestyle='--',\n",
    "                linewidth=2, label=f\"Threshold 1: {thresholds['short_max']:.1f}\")\n",
    "    ax2.axvline(thresholds['medium_max'], color='darkred', linestyle='--',\n",
    "                linewidth=2, label=f\"Threshold 2: {thresholds['medium_max']:.1f}\")\n",
    "\n",
    "    ax2.set_xlabel('Length (amino acids)', fontsize=12)\n",
    "    ax2.set_ylabel('Frequency', fontsize=12)\n",
    "    ax2.set_title('Linker Length Distribution with Cluster Thresholds',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_dir:\n",
    "        save_dir = os.path.expanduser(save_dir)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(save_dir, 'linker_length_clusters.png'),\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Create separate plot for counts by linker type\n",
    "    _, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Count linkers by type and cluster\n",
    "    type_cluster_counts = {\n",
    "        'n-terminus': {'short': 0, 'medium': 0, 'long': 0},\n",
    "        'c-terminus': {'short': 0, 'medium': 0, 'long': 0},\n",
    "        'inner': {'short': 0, 'medium': 0, 'long': 0}\n",
    "    }\n",
    "\n",
    "    for cluster_name, cluster_data in clusters.items():\n",
    "        for item in cluster_data:\n",
    "            linker_type = item[1]  # linker_type is index 1\n",
    "            type_cluster_counts[linker_type][cluster_name] += 1\n",
    "\n",
    "    # Prepare data for grouped bar chart\n",
    "    linker_types = ['n-terminus', 'c-terminus', 'inner']\n",
    "    short_counts = [type_cluster_counts[t]['short'] for t in linker_types]\n",
    "    medium_counts = [type_cluster_counts[t]['medium'] for t in linker_types]\n",
    "    long_counts = [type_cluster_counts[t]['long'] for t in linker_types]\n",
    "\n",
    "    x = np.arange(len(linker_types))\n",
    "    width = 0.25\n",
    "\n",
    "    ax.bar(x - width, short_counts, width, label='Short', color='lightblue', edgecolor='black')\n",
    "    ax.bar(x, medium_counts, width, label='Medium', color='lightgreen', edgecolor='black')\n",
    "    ax.bar(x + width, long_counts, width, label='Long', color='lightcoral', edgecolor='black')\n",
    "\n",
    "    ax.set_xlabel('Linker Type', fontsize=12)\n",
    "    ax.set_ylabel('Count', fontsize=12)\n",
    "    ax.set_title('Linker Length Clusters by Type', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(linker_types)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_dir:\n",
    "        plt.savefig(os.path.join(save_dir, 'linker_clusters_by_type.png'),\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    #----------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12d7fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5. Linker Length Clustering Functions\n",
    "\n",
    "Cluster linkers into 3 categories (short, medium, long) using different methods.\n",
    "\n",
    "**Clustering methods available:**\n",
    "\n",
    "1. **`percentile`** (default): Uses 33rd and 67th percentiles\n",
    "   - Ensures equal-sized groups\n",
    "   - Good for exploratory analysis\n",
    "\n",
    "2. **`kmeans`**: Data-driven K-means clustering\n",
    "   - Finds natural groupings in the data\n",
    "   - Best when data has distinct length populations\n",
    "\n",
    "3. **`std`**: Statistical approach using mean ± 1 standard deviation\n",
    "   - Short: < mean - std\n",
    "   - Medium: between mean - std and mean + std\n",
    "   - Long: > mean + std\n",
    "\n",
    "**Functions:**\n",
    "- `add_length_category_to_linkers()`: Adds 'length_category' field to each linker in place\n",
    "- `visualize_linker_clusters()`: Creates visualizations of clusters\n",
    "\"\"\"\n",
    "def add_length_category_to_linkers(proteins_dict, method='percentile', verbose=True):\n",
    "    \"\"\"\n",
    "    Add length category ('short', 'medium', 'long') to each linker in the proteins dictionary.\n",
    "\n",
    "    This function modifies the input dictionary in place, adding a 'length_category' field\n",
    "    to each linker based on its length.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    proteins_dict : dict\n",
    "        Dictionary with protein accessions as keys and domains/linkers as values\n",
    "        Format: {acc: {'domains': [...], 'linkers': [{'type': ..., 'start': ..., 'end': ..., 'motifs': ...}]}}\n",
    "    method : str\n",
    "        Clustering method:\n",
    "        - 'percentile': Use 33rd and 67th percentiles (default)\n",
    "        - 'kmeans': Use K-means clustering (data-driven)\n",
    "        - 'std': Use mean ± std for boundaries\n",
    "    verbose : bool\n",
    "        If True, print statistics\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        The modified proteins_dict with 'length_category' added to each linker\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Collect all linker lengths\n",
    "    all_lengths = []\n",
    "    linker_refs = []  # Store references to linkers for updating\n",
    "\n",
    "    for accession, data in proteins_dict.items():\n",
    "        if 'linkers' in data:\n",
    "            for linker in data['linkers']:\n",
    "                start = linker['start']\n",
    "                end = linker['end']\n",
    "                length = end - start + 1\n",
    "                all_lengths.append(length)\n",
    "                linker_refs.append((accession, linker, length))\n",
    "\n",
    "    if not all_lengths:\n",
    "        if verbose:\n",
    "            print(\"No linkers found in proteins_dict\")\n",
    "        return proteins_dict\n",
    "\n",
    "    all_lengths_array = np.array(all_lengths)\n",
    "\n",
    "    # Determine thresholds based on method\n",
    "    if method == 'percentile':\n",
    "        threshold1 = np.percentile(all_lengths_array, 33.33)\n",
    "        threshold2 = np.percentile(all_lengths_array, 66.67)\n",
    "        if verbose:\n",
    "            print(f\"Percentile method: 33rd percentile = {threshold1:.1f}, 67th percentile = {threshold2:.1f}\")\n",
    "\n",
    "    elif method == 'kmeans':\n",
    "        from sklearn.cluster import KMeans\n",
    "        lengths_reshaped = all_lengths_array.reshape(-1, 1)\n",
    "        kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "        kmeans.fit(lengths_reshaped)\n",
    "\n",
    "        centers = sorted(kmeans.cluster_centers_.flatten())\n",
    "        threshold1 = (centers[0] + centers[1]) / 2\n",
    "        threshold2 = (centers[1] + centers[2]) / 2\n",
    "        if verbose:\n",
    "            print(f\"K-means method: cluster centers = {centers}\")\n",
    "            print(f\"Thresholds: {threshold1:.1f}, {threshold2:.1f}\")\n",
    "\n",
    "    elif method == 'std':\n",
    "        mean_len = np.mean(all_lengths_array)\n",
    "        std_len = np.std(all_lengths_array)\n",
    "        threshold1 = max(1, mean_len - std_len)\n",
    "        threshold2 = mean_len + std_len\n",
    "        if verbose:\n",
    "            print(f\"Std method: mean = {mean_len:.1f}, std = {std_len:.1f}\")\n",
    "            print(f\"Thresholds: short <= {threshold1:.1f}, medium {threshold1:.1f}-{threshold2:.1f}, long > {threshold2:.1f}\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}. Use 'percentile', 'kmeans', or 'std'\")\n",
    "\n",
    "    # Add length category to each linker\n",
    "    category_counts = {'short': 0, 'medium': 0, 'long': 0}\n",
    "\n",
    "    for accession, linker, length in linker_refs:\n",
    "        if length <= threshold1:\n",
    "            category = 'short'\n",
    "        elif length <= threshold2:\n",
    "            category = 'medium'\n",
    "        else:\n",
    "            category = 'long'\n",
    "\n",
    "        linker['length_category'] = category\n",
    "        category_counts[category] += 1\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\n=== Length Category Summary ===\")\n",
    "        print(f\"Total linkers: {len(linker_refs)}\")\n",
    "        print(f\"  Short: {category_counts['short']} ({category_counts['short']/len(linker_refs)*100:.1f}%)\")\n",
    "        print(f\"  Medium: {category_counts['medium']} ({category_counts['medium']/len(linker_refs)*100:.1f}%)\")\n",
    "        print(f\"  Long: {category_counts['long']} ({category_counts['long']/len(linker_refs)*100:.1f}%)\")\n",
    "\n",
    "    return proteins_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49993803",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6. Linker Outlier Removal Functions\n",
    "\n",
    "Functions to remove outlier linkers based on statistical methods.\n",
    "\n",
    "**Main function:**\n",
    "- `remove_linker_outliers()`: Main interface for outlier removal\n",
    "\n",
    "**Method-specific functions:**\n",
    "- `remove_linker_outliers_percentile()`: Boxplot whisker method (Q1 - 1.5×IQR, Q3 + 1.5×IQR)\n",
    "- `remove_linker_outliers_zscore()`: Z-score method (|z| > threshold)\n",
    "- `remove_linker_outliers_iqr()`: Custom IQR method (Q1 - k×IQR, Q3 + k×IQR)\n",
    "\n",
    "**Important:**\n",
    "- Only removes LINKERS, never domains\n",
    "- Can operate per-linker-type (N/C/inner) or globally\n",
    "- Returns filtered proteins_dict and detailed statistics\n",
    "- Can save filtered results to JSON file\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "def _collect_linker_data(proteins_dict):\n",
    "    \"\"\"\n",
    "    Helper function to collect all linker data from proteins_dict.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple of (linker_data, linker_type_data)\n",
    "        - linker_data: list of (accession, linker_type, start, end, length) tuples\n",
    "        - linker_type_data: dict with keys 'n-terminus', 'c-terminus', 'inner', 'all'\n",
    "    \"\"\"\n",
    "    linker_data = []\n",
    "    linker_type_data = {\n",
    "        'n-terminus': [],\n",
    "        'c-terminus': [],\n",
    "        'inner': [],\n",
    "        'all': []\n",
    "    }\n",
    "\n",
    "    for accession, data in proteins_dict.items():\n",
    "        for linker in data['linkers']:\n",
    "            linker_type, start, end = linker\n",
    "            length = end - start + 1\n",
    "            linker_info = (accession, linker_type, start, end, length)\n",
    "            linker_data.append(linker_info)\n",
    "            linker_type_data[linker_type].append(linker_info)\n",
    "            linker_type_data['all'].append(linker_info)\n",
    "\n",
    "    return linker_data, linker_type_data\n",
    "\n",
    "\n",
    "def _calculate_bounds_percentile(lengths, multiplier=1.5):\n",
    "    \"\"\"Calculate bounds using percentile/IQR method.\"\"\"\n",
    "    q1 = np.percentile(lengths, 25)\n",
    "    q3 = np.percentile(lengths, 75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - multiplier * iqr\n",
    "    upper_bound = q3 + multiplier * iqr\n",
    "    return lower_bound, upper_bound, {'q1': q1, 'q3': q3, 'iqr': iqr}\n",
    "\n",
    "\n",
    "def _calculate_bounds_zscore(lengths, threshold=3.0):\n",
    "    \"\"\"Calculate bounds using z-score method.\"\"\"\n",
    "    mean_len = np.mean(lengths)\n",
    "    std_len = np.std(lengths)\n",
    "    lower_bound = mean_len - threshold * std_len\n",
    "    upper_bound = mean_len + threshold * std_len\n",
    "    return lower_bound, upper_bound, {'mean': mean_len, 'std': std_len}\n",
    "\n",
    "\n",
    "def _filter_proteins_dict(proteins_dict, outlier_sets):\n",
    "    \"\"\"\n",
    "    Create filtered proteins_dict by removing outlier linkers.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    proteins_dict : dict\n",
    "        Original proteins dictionary\n",
    "    outlier_sets : dict\n",
    "        Dictionary with (accession, linker_type, start, end) as keys and length as values\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple of (filtered_proteins, removed_linkers)\n",
    "    \"\"\"\n",
    "    filtered_proteins = {}\n",
    "    removed_linkers = []\n",
    "\n",
    "    for accession, data in proteins_dict.items():\n",
    "        filtered_linkers = []\n",
    "\n",
    "        for linker in data['linkers']:\n",
    "            linker_type, start, end = linker\n",
    "            linker_key = (accession, linker_type, start, end)\n",
    "\n",
    "            if linker_key in outlier_sets:\n",
    "                removed_linkers.append({\n",
    "                    'accession': accession,\n",
    "                    'type': linker_type,\n",
    "                    'start': start,\n",
    "                    'end': end,\n",
    "                    'length': outlier_sets[linker_key]\n",
    "                })\n",
    "            else:\n",
    "                filtered_linkers.append(linker)\n",
    "\n",
    "        filtered_proteins[accession] = {\n",
    "            'domains': data['domains'],\n",
    "            'linkers': filtered_linkers\n",
    "        }\n",
    "\n",
    "    return filtered_proteins, removed_linkers\n",
    "\n",
    "\n",
    "def _calculate_statistics(linker_data, linker_type_data, removed_linkers, by_type):\n",
    "    \"\"\"Calculate statistics for the outlier removal process.\"\"\"\n",
    "    total_linkers_before = len(linker_data)\n",
    "    total_removed = len(removed_linkers)\n",
    "    total_linkers_after = total_linkers_before - total_removed\n",
    "\n",
    "    statistics = {\n",
    "        'total_linkers_before': total_linkers_before,\n",
    "        'total_linkers_after': total_linkers_after,\n",
    "        'total_removed': total_removed,\n",
    "        'removal_percentage': (total_removed / total_linkers_before * 100) if total_linkers_before > 0 else 0\n",
    "    }\n",
    "\n",
    "    if by_type:\n",
    "        for linker_type in ['n-terminus', 'c-terminus', 'inner']:\n",
    "            type_before = len(linker_type_data[linker_type])\n",
    "            type_removed = sum(1 for item in removed_linkers if item['type'] == linker_type)\n",
    "            type_after = type_before - type_removed\n",
    "            statistics[linker_type] = {\n",
    "                'before': type_before,\n",
    "                'removed': type_removed,\n",
    "                'after': type_after,\n",
    "                'removal_percentage': (type_removed / type_before * 100) if type_before > 0 else 0\n",
    "            }\n",
    "\n",
    "    return statistics\n",
    "\n",
    "\n",
    "def _print_statistics(statistics, bounds_dict, method, by_type):\n",
    "    \"\"\"Print statistics summary.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"{method.upper()} METHOD - OUTLIER REMOVAL SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Print bounds\n",
    "    for group, bounds in bounds_dict.items():\n",
    "        print(f\"\\n{group.upper()}:\")\n",
    "        print(f\"  Bounds: [{bounds['lower']:.2f}, {bounds['upper']:.2f}]\")\n",
    "        if 'q1' in bounds:\n",
    "            print(f\"  Q1: {bounds['q1']:.2f}, Q3: {bounds['q3']:.2f}, IQR: {bounds['iqr']:.2f}\")\n",
    "        elif 'mean' in bounds:\n",
    "            print(f\"  Mean: {bounds['mean']:.2f}, Std: {bounds['std']:.2f}\")\n",
    "\n",
    "    print(f\"\\nTotal linkers before: {statistics['total_linkers_before']}\")\n",
    "    print(f\"Total linkers removed: {statistics['total_removed']} ({statistics['removal_percentage']:.1f}%)\")\n",
    "    print(f\"Total linkers after: {statistics['total_linkers_after']}\")\n",
    "\n",
    "    if by_type:\n",
    "        print(\"\\nBy linker type:\")\n",
    "        for linker_type in ['n-terminus', 'c-terminus', 'inner']:\n",
    "            if linker_type in statistics:\n",
    "                s = statistics[linker_type]\n",
    "                print(f\"  {linker_type}: {s['before']} → {s['after']} \"\n",
    "                      f\"(removed {s['removed']}, {s['removal_percentage']:.1f}%)\")\n",
    "\n",
    "\n",
    "def remove_linker_outliers_percentile(proteins_dict, by_type=True, verbose=True, save_path=None):\n",
    "    \"\"\"\n",
    "    Remove outlier linkers using percentile/boxplot method (Q1 - 1.5×IQR, Q3 + 1.5×IQR).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    proteins_dict : dict\n",
    "        Dictionary with protein accessions as keys and domains/linkers as values\n",
    "    by_type : bool\n",
    "        If True, calculate outliers separately for each linker type\n",
    "    verbose : bool\n",
    "        If True, print detailed statistics\n",
    "    save_path : str, optional\n",
    "        Path to save filtered proteins_dict as JSON\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict with keys: 'filtered_proteins', 'removed_linkers', 'statistics', 'bounds'\n",
    "    \"\"\"\n",
    "    linker_data, linker_type_data = _collect_linker_data(proteins_dict)\n",
    "\n",
    "    if not linker_data:\n",
    "        return {\n",
    "            'filtered_proteins': proteins_dict,\n",
    "            'removed_linkers': [],\n",
    "            'statistics': {},\n",
    "            'bounds': {}\n",
    "        }\n",
    "\n",
    "    groups_to_process = ['n-terminus', 'c-terminus', 'inner'] if by_type else ['all']\n",
    "    bounds_dict = {}\n",
    "    outlier_sets = {}\n",
    "\n",
    "    for group in groups_to_process:\n",
    "        group_data = linker_type_data[group]\n",
    "        if not group_data:\n",
    "            continue\n",
    "\n",
    "        lengths = np.array([item[4] for item in group_data])\n",
    "        lower_bound, upper_bound, stats_info = _calculate_bounds_percentile(lengths)\n",
    "\n",
    "        bounds_dict[group] = {\n",
    "            'lower': lower_bound,\n",
    "            'upper': upper_bound,\n",
    "            **stats_info\n",
    "        }\n",
    "\n",
    "        for item in group_data:\n",
    "            accession, linker_type, start, end, length = item\n",
    "            if length < lower_bound or length > upper_bound:\n",
    "                outlier_sets[(accession, linker_type, start, end)] = length\n",
    "\n",
    "    filtered_proteins, removed_linkers = _filter_proteins_dict(proteins_dict, outlier_sets)\n",
    "    statistics = _calculate_statistics(linker_data, linker_type_data, removed_linkers, by_type)\n",
    "\n",
    "    if verbose:\n",
    "        _print_statistics(statistics, bounds_dict, 'percentile', by_type)\n",
    "\n",
    "    if save_path:\n",
    "        save_path = os.path.expanduser(save_path)\n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(filtered_proteins, f, indent=2)\n",
    "        if verbose:\n",
    "            print(f\"\\n✓ Saved filtered proteins to: {save_path}\")\n",
    "\n",
    "    return {\n",
    "        'filtered_proteins': filtered_proteins,\n",
    "        'removed_linkers': removed_linkers,\n",
    "        'statistics': statistics,\n",
    "        'bounds': bounds_dict\n",
    "    }\n",
    "\n",
    "\n",
    "def remove_linker_outliers_zscore(proteins_dict, threshold=3.0, by_type=True,\n",
    "                                   verbose=True, save_path=None):\n",
    "    \"\"\"\n",
    "    Remove outlier linkers using z-score method (|z| > threshold).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    proteins_dict : dict\n",
    "        Dictionary with protein accessions as keys and domains/linkers as values\n",
    "    threshold : float\n",
    "        Z-score threshold (default: 3.0)\n",
    "    by_type : bool\n",
    "        If True, calculate outliers separately for each linker type\n",
    "    verbose : bool\n",
    "        If True, print detailed statistics\n",
    "    save_path : str, optional\n",
    "        Path to save filtered proteins_dict as JSON\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict with keys: 'filtered_proteins', 'removed_linkers', 'statistics', 'bounds'\n",
    "    \"\"\"\n",
    "    linker_data, linker_type_data = _collect_linker_data(proteins_dict)\n",
    "\n",
    "    if not linker_data:\n",
    "        return {\n",
    "            'filtered_proteins': proteins_dict,\n",
    "            'removed_linkers': [],\n",
    "            'statistics': {},\n",
    "            'bounds': {}\n",
    "        }\n",
    "\n",
    "    groups_to_process = ['n-terminus', 'c-terminus', 'inner'] if by_type else ['all']\n",
    "    bounds_dict = {}\n",
    "    outlier_sets = {}\n",
    "\n",
    "    for group in groups_to_process:\n",
    "        group_data = linker_type_data[group]\n",
    "        if not group_data:\n",
    "            continue\n",
    "\n",
    "        lengths = np.array([item[4] for item in group_data])\n",
    "        lower_bound, upper_bound, stats_info = _calculate_bounds_zscore(lengths, threshold)\n",
    "\n",
    "        bounds_dict[group] = {\n",
    "            'lower': lower_bound,\n",
    "            'upper': upper_bound,\n",
    "            **stats_info\n",
    "        }\n",
    "\n",
    "        for item in group_data:\n",
    "            accession, linker_type, start, end, length = item\n",
    "            if length < lower_bound or length > upper_bound:\n",
    "                outlier_sets[(accession, linker_type, start, end)] = length\n",
    "\n",
    "    filtered_proteins, removed_linkers = _filter_proteins_dict(proteins_dict, outlier_sets)\n",
    "    statistics = _calculate_statistics(linker_data, linker_type_data, removed_linkers, by_type)\n",
    "\n",
    "    if verbose:\n",
    "        _print_statistics(statistics, bounds_dict, f'zscore (threshold={threshold})', by_type)\n",
    "\n",
    "    if save_path:\n",
    "        save_path = os.path.expanduser(save_path)\n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(filtered_proteins, f, indent=2)\n",
    "        if verbose:\n",
    "            print(f\"\\n✓ Saved filtered proteins to: {save_path}\")\n",
    "\n",
    "    return {\n",
    "        'filtered_proteins': filtered_proteins,\n",
    "        'removed_linkers': removed_linkers,\n",
    "        'statistics': statistics,\n",
    "        'bounds': bounds_dict\n",
    "    }\n",
    "\n",
    "\n",
    "def remove_linker_outliers_iqr(proteins_dict, k=1.5, by_type=True,\n",
    "                                verbose=True, save_path=None):\n",
    "    \"\"\"\n",
    "    Remove outlier linkers using custom IQR method (Q1 - k×IQR, Q3 + k×IQR).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    proteins_dict : dict\n",
    "        Dictionary with protein accessions as keys and domains/linkers as values\n",
    "    k : float\n",
    "        IQR multiplier (default: 1.5, use 2.0 for more permissive filtering)\n",
    "    by_type : bool\n",
    "        If True, calculate outliers separately for each linker type\n",
    "    verbose : bool\n",
    "        If True, print detailed statistics\n",
    "    save_path : str, optional\n",
    "        Path to save filtered proteins_dict as JSON\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict with keys: 'filtered_proteins', 'removed_linkers', 'statistics', 'bounds'\n",
    "    \"\"\"\n",
    "    linker_data, linker_type_data = _collect_linker_data(proteins_dict)\n",
    "\n",
    "    if not linker_data:\n",
    "        return {\n",
    "            'filtered_proteins': proteins_dict,\n",
    "            'removed_linkers': [],\n",
    "            'statistics': {},\n",
    "            'bounds': {}\n",
    "        }\n",
    "\n",
    "    groups_to_process = ['n-terminus', 'c-terminus', 'inner'] if by_type else ['all']\n",
    "    bounds_dict = {}\n",
    "    outlier_sets = {}\n",
    "\n",
    "    for group in groups_to_process:\n",
    "        group_data = linker_type_data[group]\n",
    "        if not group_data:\n",
    "            continue\n",
    "\n",
    "        lengths = np.array([item[4] for item in group_data])\n",
    "        lower_bound, upper_bound, stats_info = _calculate_bounds_percentile(lengths, multiplier=k)\n",
    "\n",
    "        bounds_dict[group] = {\n",
    "            'lower': lower_bound,\n",
    "            'upper': upper_bound,\n",
    "            **stats_info\n",
    "        }\n",
    "\n",
    "        for item in group_data:\n",
    "            accession, linker_type, start, end, length = item\n",
    "            if length < lower_bound or length > upper_bound:\n",
    "                outlier_sets[(accession, linker_type, start, end)] = length\n",
    "\n",
    "    filtered_proteins, removed_linkers = _filter_proteins_dict(proteins_dict, outlier_sets)\n",
    "    statistics = _calculate_statistics(linker_data, linker_type_data, removed_linkers, by_type)\n",
    "\n",
    "    if verbose:\n",
    "        _print_statistics(statistics, bounds_dict, f'iqr (k={k})', by_type)\n",
    "\n",
    "    if save_path:\n",
    "        save_path = os.path.expanduser(save_path)\n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump(filtered_proteins, f, indent=2)\n",
    "        if verbose:\n",
    "            print(f\"\\n✓ Saved filtered proteins to: {save_path}\")\n",
    "\n",
    "    return {\n",
    "        'filtered_proteins': filtered_proteins,\n",
    "        'removed_linkers': removed_linkers,\n",
    "        'statistics': statistics,\n",
    "        'bounds': bounds_dict\n",
    "    }\n",
    "\n",
    "\n",
    "def remove_linker_outliers(proteins_dict, method='percentile', threshold=3.0, k=1.5,\n",
    "                           by_type=True, verbose=True, save_path=None):\n",
    "    \"\"\"\n",
    "    Main interface for removing outlier linkers from proteins_dict.\n",
    "\n",
    "    IMPORTANT: This function only removes LINKERS, never domains.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    proteins_dict : dict\n",
    "        Dictionary with protein accessions as keys and domains/linkers as values\n",
    "        Format: {acc: {'domains': [(name, start, end)...], 'linkers': [(type, start, end)...]}}\n",
    "    method : str\n",
    "        Outlier detection method:\n",
    "        - 'percentile': Boxplot whiskers (Q1 - 1.5×IQR, Q3 + 1.5×IQR) - DEFAULT\n",
    "        - 'zscore': Z-score method (|z| > threshold)\n",
    "        - 'iqr': Custom IQR method (Q1 - k×IQR, Q3 + k×IQR)\n",
    "    threshold : float\n",
    "        Z-score threshold for 'zscore' method (default: 3.0)\n",
    "    k : float\n",
    "        IQR multiplier for 'iqr' method (default: 1.5)\n",
    "    by_type : bool\n",
    "        If True, calculate outliers separately for each linker type (N/C/inner)\n",
    "        If False, calculate outliers across all linkers together\n",
    "    verbose : bool\n",
    "        If True, print detailed statistics\n",
    "    save_path : str, optional\n",
    "        Path to save filtered proteins_dict as JSON\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict with keys:\n",
    "        - 'filtered_proteins': proteins_dict with outlier linkers removed\n",
    "        - 'removed_linkers': list of removed linkers with details\n",
    "        - 'statistics': dict with before/after stats\n",
    "        - 'bounds': dict with lower/upper bounds used for filtering\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    result = remove_linker_outliers(proteins_dict, method='percentile', save_path='filtered.json')\n",
    "    filtered_proteins = result['filtered_proteins']\n",
    "    print(f\"Removed {len(result['removed_linkers'])} outlier linkers\")\n",
    "    \"\"\"\n",
    "    if method == 'percentile':\n",
    "        return remove_linker_outliers_percentile(proteins_dict, by_type=by_type,\n",
    "                                                 verbose=verbose, save_path=save_path)\n",
    "    elif method == 'zscore':\n",
    "        return remove_linker_outliers_zscore(proteins_dict, threshold=threshold,\n",
    "                                             by_type=by_type, verbose=verbose, save_path=save_path)\n",
    "    elif method == 'iqr':\n",
    "        return remove_linker_outliers_iqr(proteins_dict, k=k, by_type=by_type,\n",
    "                                         verbose=verbose, save_path=save_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}. Use 'percentile', 'zscore', or 'iqr'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e05f497",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "7. Data Loading Helper Functions\n",
    "\n",
    "Convenient functions to load protein sequences and pre-formatted domain/linker data.\n",
    "\n",
    "**Functions:**\n",
    "- `load_fasta()`: Loads protein sequences from FASTA file\n",
    "- `load_formatted_proteins()`: Loads JSON with domain/linker annotations\n",
    "\n",
    "**Purpose:**\n",
    "These functions simplify data loading and convert between storage formats (compact tuples) and analysis formats (dictionaries).\n",
    "\"\"\"\n",
    "def load_fasta(fasta_path):\n",
    "    \"\"\"\n",
    "    Load protein sequences from a FASTA file.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    fasta_path : str\n",
    "        Path to FASTA file\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with protein accessions as keys and sequences as values\n",
    "    \"\"\"\n",
    "    sequences_dict = {}\n",
    "\n",
    "    with open(os.path.expanduser(fasta_path), 'r') as f:\n",
    "        current_acc = None\n",
    "        current_seq = []\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                # Save previous sequence if exists\n",
    "                if current_acc:\n",
    "                    sequences_dict[current_acc] = ''.join(current_seq)\n",
    "\n",
    "                # Start new sequence\n",
    "                current_acc = line[1:]\n",
    "                current_seq = []\n",
    "            else:\n",
    "                current_seq.append(line)\n",
    "\n",
    "        # Save last sequence\n",
    "        if current_acc:\n",
    "            sequences_dict[current_acc] = ''.join(current_seq)\n",
    "\n",
    "    print(f\"✓ Loaded {len(sequences_dict)} sequences from {fasta_path}\")\n",
    "    return sequences_dict\n",
    "\n",
    "\n",
    "def load_protein_dict(json_path):\n",
    "    \"\"\"\n",
    "    Load formatted protein data and convert to analysis-ready structure.\n",
    "\n",
    "    This function loads the formatted proteins JSON file and converts the\n",
    "    compact tuple format into dictionaries suitable for analysis functions.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    json_path : str\n",
    "        Path to formatted_proteins.json file\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple of (proteins_dict, domains_list)\n",
    "        - proteins_dict: Original dict with accessions as keys, domains/linkers as values\n",
    "        - domains_list: Analysis-ready list with expanded domain/linker dictionaries\n",
    "\n",
    "    Example structure of domains_list:\n",
    "    [\n",
    "        {\n",
    "            'uniprot_acc': 'P12345',\n",
    "            'domains': [{'name': 'Domain1', 'start': 10, 'end': 100}, ...],\n",
    "            'linkers': [{'start': 1, 'end': 9}, ...]\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    # Load formatted proteins from JSON\n",
    "    with open(os.path.expanduser(json_path), 'r') as f:\n",
    "        proteins_dict = json.load(f)\n",
    "\n",
    "    print(f\"✓ Loaded {len(proteins_dict)} proteins from {json_path}\")\n",
    "\n",
    "    # Convert to analysis-ready structure\n",
    "    # formatted_proteins.json has domains as [name, start, end] and linkers as [type, start, end]\n",
    "    domains_list = []\n",
    "\n",
    "    for accession, data in proteins_dict.items():\n",
    "        # Convert domain tuples to dicts\n",
    "        domain_dicts = [\n",
    "            {'name': d[0], 'start': d[1], 'end': d[2]}\n",
    "            for d in data['domains']\n",
    "        ]\n",
    "\n",
    "        # Convert linker tuples (type, start, end) to dicts with coordinates\n",
    "        linker_coords = []\n",
    "        for linker in data['linkers']:\n",
    "            linker_type, start, end = linker\n",
    "            linker_coords.append({\n",
    "                'type': linker_type,\n",
    "                'start': start,\n",
    "                'end': end\n",
    "            })\n",
    "\n",
    "        domains_list.append({\n",
    "            'uniprot_acc': accession,\n",
    "            'domains': domain_dicts,\n",
    "            'linkers': linker_coords\n",
    "        })\n",
    "\n",
    "    print(f\"✓ Converted to analysis-ready structure: {len(domains_list)} proteins\")\n",
    "\n",
    "    return proteins_dict, domains_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6404b1",
   "metadata": {},
   "source": [
    "--\n",
    "** Basic Linker analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7098e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get domain info\n",
    "import requests, math, os, json\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "\n",
    "# ================================\n",
    "# Configuration\n",
    "# ================================\n",
    "workdir = \"~/Desktop/work/protein_linkers/input_2\"\n",
    "protein_file = os.path.expanduser(f\"{workdir}/proteins.tsv\")\n",
    "fasta_path = os.path.expanduser(f\"{workdir}/proteins.fa\")\n",
    "json_file = os.path.expanduser(f\"{workdir}/formatted_proteins.json\")\n",
    "\"\"\"\n",
    "\n",
    "# ================================\n",
    "# Linker + Domain info\n",
    "# ================================\n",
    "\n",
    "# Read protein accessions from TSV\n",
    "protein_accessions = []\n",
    "with open(protein_file, 'r') as f:\n",
    "    next(f)  # Skip header\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if parts and parts[0]:\n",
    "            protein_accessions.append(parts[0])\n",
    "\n",
    "print(f\"Found {len(protein_accessions)} proteins in {protein_file}\")\n",
    "\n",
    "# Generate new formatted proteins dict\n",
    "proteins_dict = {}\n",
    "for i, acc in enumerate(protein_accessions, 1):\n",
    "    print(f\"[{i}/{len(protein_accessions)}] Processing {acc}...\")\n",
    "\n",
    "    # Get protein data with domains\n",
    "    result = summarize_protein_domains_dict(acc, representative_only=True)\n",
    "    if result:\n",
    "        # Add linkers\n",
    "        result = add_linkers_to_result(result)\n",
    "\n",
    "        # Format to simplified structure with (type, start, end) for linkers\n",
    "        formatted = format_protein_structure(result)\n",
    "        if formatted:\n",
    "            proteins_dict[acc] = {\n",
    "                'domains': formatted['domains'],\n",
    "                'linkers': formatted['linkers']\n",
    "            }\n",
    "\n",
    "# Save to JSON\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(proteins_dict, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Saved {len(proteins_dict)} formatted proteins to {json_file}\")\n",
    "\"\"\"\n",
    "# ================================\n",
    "# Load the results\n",
    "# ================================\n",
    "\n",
    "# Load FASTA sequences\n",
    "sequences_dict = load_fasta(fasta_path)\n",
    "\n",
    "# Load formatted proteins and convert to analysis-ready structure\n",
    "proteins_dict, domains = load_protein_dict(json_file)\n",
    "\n",
    "# ================================\n",
    "# Run Analyses\n",
    "# ================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LINKER ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "n_terminus_counter, c_terminus_counter, inner_counter, n_count, c_count, i_count, n_lengths, c_lengths, i_lengths = analyze_linkers(\n",
    "    proteins_dict, sequences_dict, domains\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DOMAIN ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "domain_counter, domain_count, domain_lengths = analyze_domains(sequences_dict, domains)\n",
    "\n",
    "# ================================\n",
    "# Create Visualizations\n",
    "# ================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n[1/2] Creating combined linker and domain visualizations...\")\n",
    "visualize_linker_composition(n_terminus_counter, c_terminus_counter, inner_counter,\n",
    "                             n_count, c_count, i_count, n_lengths, c_lengths, i_lengths,\n",
    "                             domain_counter, domain_count, domain_lengths,\n",
    "                             save_dir=workdir)\n",
    "\n",
    "print(\"\\n[2/2] Creating domain vs linker comparison...\")\n",
    "compare_domains_vs_linkers(domain_counter, domain_count,\n",
    "                           n_terminus_counter, c_terminus_counter, inner_counter,\n",
    "                           n_count, c_count, i_count, save_dir=workdir)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ ANALYSIS COMPLETE!\")\n",
    "print(f\"All plots saved to: {os.path.expanduser(workdir)}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5452786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, math, os, json\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "\n",
    "# ================================\n",
    "# Configuration\n",
    "# ================================\n",
    "workdir = \"~/Desktop/work/protein_linkers/input_2\"\n",
    "protein_file = os.path.expanduser(f\"{workdir}/proteins.tsv\")\n",
    "fasta_path = os.path.expanduser(f\"{workdir}/proteins.fa\")\n",
    "json_file = os.path.expanduser(f\"{workdir}/formatted_proteins.json\")\n",
    "# ================================\n",
    "# Load the results\n",
    "# ================================\n",
    "\n",
    "# Load FASTA sequences\n",
    "sequences_dict = load_fasta(fasta_path)\n",
    "\n",
    "# Load formatted proteins and convert to analysis-ready structure\n",
    "proteins_dict, domains = load_protein_dict(json_file)\n",
    "\n",
    "# ================================\n",
    "# Filter out proteins with no domains\n",
    "# ================================\n",
    "\n",
    "# Use the filtering function\n",
    "proteins_dict, domains, filter_stats = filter_proteins_without_domains(\n",
    "    proteins_dict,\n",
    "    domains_list=domains,\n",
    "    save_path=f\"{workdir}/formatted_proteins.json\"\n",
    ")\n",
    "\n",
    "# Print domain filtering statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DOMAIN FILTERING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Before filtering: {filter_stats['total_before']} proteins\")\n",
    "print(f\"Proteins with no domains: {filter_stats['no_domains']}\")\n",
    "print(f\"After filtering: {filter_stats['total_after']} proteins\")\n",
    "print(f\"Removed: {filter_stats['removed']} proteins with no domains\")\n",
    "\n",
    "# ================================\n",
    "# Remove linker outliers\n",
    "# ================================\n",
    "\n",
    "result = remove_linker_outliers(\n",
    "    proteins_dict,\n",
    "    method='percentile',\n",
    "    by_type=True,\n",
    "    verbose=True,\n",
    "    save_path=f\"{workdir}/filtered_proteins.json\"\n",
    ")\n",
    "\n",
    "# Get filtered proteins\n",
    "filtered_proteins = result['filtered_proteins']\n",
    "# Create domains-only dictionary\n",
    "domains_dict = {}\n",
    "\n",
    "for accession, data in filtered_proteins.items():\n",
    "    if data['domains']:  # Only include proteins that have domains\n",
    "        domains_dict[accession] = {\n",
    "            'domains': data['domains'],\n",
    "            'linkers': []  # Empty linkers list for consistency with structure\n",
    "        }\n",
    "\n",
    "# Print outlier removal statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OUTLIER REMOVAL STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "stats = result['statistics']\n",
    "print(f\"Total linkers before: {stats['total_linkers_before']}\")\n",
    "print(f\"Total linkers removed: {stats['total_removed']} ({stats['removal_percentage']:.1f}%)\")\n",
    "print(f\"Total linkers after: {stats['total_linkers_after']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9272b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Get Basic Linker Statistics\n",
    "# ================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def collect_and_print_statistics(data_dict, title=\"LINKER STATISTICS\"):\n",
    "    \"\"\"\n",
    "    Collect linker data and print comprehensive statistics.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dict : dict\n",
    "        proteins_dict or filtered_proteins dictionary\n",
    "    title : str\n",
    "        Title for the statistics section\n",
    "    \"\"\"\n",
    "    # Collect all linker data with details\n",
    "    all_linker_data = []  # (accession, linker_type, start, end, length)\n",
    "    linker_type_data = {\n",
    "        'n-terminus': [],\n",
    "        'c-terminus': [],\n",
    "        'inner': []\n",
    "    }\n",
    "\n",
    "    for accession, data in data_dict.items():\n",
    "        for linker in data['linkers']:\n",
    "            linker_type, start, end = linker\n",
    "            length = end - start + 1\n",
    "            linker_info = (accession, linker_type, start, end, length)\n",
    "            all_linker_data.append(linker_info)\n",
    "            linker_type_data[linker_type].append(linker_info)\n",
    "\n",
    "    if not all_linker_data:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(title)\n",
    "        print(f\"{'='*60}\")\n",
    "        print(\"No linkers found in dataset\")\n",
    "        return\n",
    "\n",
    "    # Extract all lengths for statistics\n",
    "    all_linker_lengths = [item[4] for item in all_linker_data]\n",
    "\n",
    "    # Find min and max linkers\n",
    "    min_linker = min(all_linker_data, key=lambda x: x[4])\n",
    "    max_linker = max(all_linker_data, key=lambda x: x[4])\n",
    "\n",
    "    # Calculate overall statistics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(title)\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total linkers: {len(all_linker_lengths)}\")\n",
    "    print(f\"\\nMin length: {np.min(all_linker_lengths)} aa\")\n",
    "    print(f\"  → {min_linker[0]} ({min_linker[1]}): positions {min_linker[2]}-{min_linker[3]}\")\n",
    "    print(f\"\\nMax length: {np.max(all_linker_lengths)} aa\")\n",
    "    print(f\"  → {max_linker[0]} ({max_linker[1]}): positions {max_linker[2]}-{max_linker[3]}\")\n",
    "    print(f\"\\nMean length: {np.mean(all_linker_lengths):.2f} aa\")\n",
    "    print(f\"Median length: {np.median(all_linker_lengths):.2f} aa\")\n",
    "    print(f\"Std deviation: {np.std(all_linker_lengths):.2f} aa\")\n",
    "    print(f\"25th percentile (Q1): {np.percentile(all_linker_lengths, 25):.2f} aa\")\n",
    "    print(f\"75th percentile (Q3): {np.percentile(all_linker_lengths, 75):.2f} aa\")\n",
    "\n",
    "    # Calculate IQR for reference\n",
    "    q1 = np.percentile(all_linker_lengths, 25)\n",
    "    q3 = np.percentile(all_linker_lengths, 75)\n",
    "    iqr = q3 - q1\n",
    "    print(f\"IQR: {iqr:.2f} aa\")\n",
    "    print(f\"Boxplot whisker bounds: [{q1 - 1.5*iqr:.2f}, {q3 + 1.5*iqr:.2f}] aa\")\n",
    "\n",
    "    # Statistics by linker type\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"STATISTICS BY LINKER TYPE\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for linker_type in ['n-terminus', 'c-terminus', 'inner']:\n",
    "        type_data = linker_type_data[linker_type]\n",
    "        if type_data:\n",
    "            lengths = [item[4] for item in type_data]\n",
    "            min_item = min(type_data, key=lambda x: x[4])\n",
    "            max_item = max(type_data, key=lambda x: x[4])\n",
    "\n",
    "            # Calculate IQR for this type\n",
    "            q1_type = np.percentile(lengths, 25)\n",
    "            q3_type = np.percentile(lengths, 75)\n",
    "            iqr_type = q3_type - q1_type\n",
    "\n",
    "            print(f\"\\n{linker_type.upper()}:\")\n",
    "            print(f\"  Count: {len(lengths)}\")\n",
    "            print(f\"  Min: {np.min(lengths)} aa\")\n",
    "            print(f\"    → {min_item[0]}: positions {min_item[2]}-{min_item[3]}\")\n",
    "            print(f\"  Max: {np.max(lengths)} aa\")\n",
    "            print(f\"    → {max_item[0]}: positions {max_item[2]}-{max_item[3]}\")\n",
    "            print(f\"  Mean: {np.mean(lengths):.2f} ± {np.std(lengths):.2f} aa\")\n",
    "            print(f\"  Median: {np.median(lengths):.2f} aa\")\n",
    "            print(f\"  Q1: {q1_type:.2f} aa, Q3: {q3_type:.2f} aa, IQR: {iqr_type:.2f} aa\")\n",
    "            print(f\"  Boxplot bounds: [{q1_type - 1.5*iqr_type:.2f}, {q3_type + 1.5*iqr_type:.2f}] aa\")\n",
    "        else:\n",
    "            print(f\"\\n{linker_type.upper()}: No linkers found\")\n",
    "\n",
    "\n",
    "# ================================\n",
    "# Statistics for ORIGINAL proteins_dict (after domain filtering)\n",
    "# ================================\n",
    "collect_and_print_statistics(proteins_dict, title=\"ORIGINAL LINKER STATISTICS (No Outlier Removal)\")\n",
    "\n",
    "# ================================\n",
    "# Statistics for FILTERED proteins (outliers removed)\n",
    "# ================================\n",
    "collect_and_print_statistics(filtered_proteins, title=\"FILTERED LINKER STATISTICS (Outliers Removed)\")\n",
    "\n",
    "# ================================\n",
    "# Comparison Summary\n",
    "# ================================\n",
    "original_count = sum(len(data['linkers']) for data in proteins_dict.values())\n",
    "filtered_count = sum(len(data['linkers']) for data in filtered_proteins.values())\n",
    "removed_count = original_count - filtered_count\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Original linkers: {original_count}\")\n",
    "print(f\"Filtered linkers: {filtered_count}\")\n",
    "print(f\"Removed linkers: {removed_count} ({removed_count/original_count*100:.1f}%)\")\n",
    "print(f\"Proteins in dataset: {len(proteins_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf6a524",
   "metadata": {},
   "source": [
    "--\n",
    "** ELM analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433567d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir = \"~/Desktop/work/protein_linkers/input_1\"\n",
    "fasta_path = os.path.expanduser(f\"{workdir}/proteins.fa\")\n",
    "proteins_with_motifs_path = os.path.join(workdir, 'proteins_elm_motifs.json')\n",
    "\n",
    "with open(os.path.expanduser(proteins_with_motifs_path), 'r') as f:\n",
    "        proteins_with_motifs = json.load(f)\n",
    "fasta_dict = load_fasta(fasta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cbe0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add length categories to linkers in the dictionary\n",
    "# ====================================================\n",
    "\n",
    "# Add 'length_category' field to each linker (modifies dict in place)\n",
    "proteins_with_length = add_length_category_to_linkers(\n",
    "    proteins_with_motifs,\n",
    "    method='percentile',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Length categories added to all linkers!\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXAMPLE: Linkers with length categories\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find a protein with multiple linkers to show example\n",
    "for acc, data in proteins_with_length.items():\n",
    "    if len(data['linkers']) >= 2:\n",
    "        print(f\"\\nProtein: {acc}\")\n",
    "        print(f\"Number of linkers: {len(data['linkers'])}\")\n",
    "        for i, linker in enumerate(data['linkers'], 1):\n",
    "            length = linker['end'] - linker['start'] + 1\n",
    "            print(f\"  {i}. Type: {linker['type']:<12} | \"\n",
    "                  f\"Position: {linker['start']:>4}-{linker['end']:<4} | \"\n",
    "                  f\"Length: {length:>3} aa | \"\n",
    "                  f\"Category: {linker['length_category']}\")\n",
    "        if len(data['linkers']) >= 2:\n",
    "            break\n",
    "\n",
    "# Count linkers by category\n",
    "category_counts = {'short': 0, 'medium': 0, 'long': 0}\n",
    "category_by_type = {\n",
    "    'n-terminus': {'short': 0, 'medium': 0, 'long': 0},\n",
    "    'c-terminus': {'short': 0, 'medium': 0, 'long': 0},\n",
    "    'inner': {'short': 0, 'medium': 0, 'long': 0}\n",
    "}\n",
    "\n",
    "for acc, data in proteins_with_length.items():\n",
    "    for linker in data['linkers']:\n",
    "        category = linker['length_category']\n",
    "        linker_type = linker['type']\n",
    "        category_counts[category] += 1\n",
    "        category_by_type[linker_type][category] += 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LENGTH CATEGORY DISTRIBUTION BY LINKER TYPE\")\n",
    "print(\"=\"*60)\n",
    "for linker_type in ['n-terminus', 'c-terminus', 'inner']:\n",
    "    counts = category_by_type[linker_type]\n",
    "    total = sum(counts.values())\n",
    "    if total > 0:\n",
    "        print(f\"\\n{linker_type.upper()}:\")\n",
    "        print(f\"  Short:  {counts['short']:>4} ({counts['short']/total*100:>5.1f}%)\")\n",
    "        print(f\"  Medium: {counts['medium']:>4} ({counts['medium']/total*100:>5.1f}%)\")\n",
    "        print(f\"  Long:   {counts['long']:>4} ({counts['long']/total*100:>5.1f}%)\")\n",
    "        print(f\"  Total:  {total:>4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f504875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize length categories\n",
    "# ============================\n",
    "\n",
    "def visualize_length_categories(proteins_dict, save_dir=workdir):\n",
    "    \"\"\"\n",
    "    Create visualizations for linker length categories.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    proteins_dict : dict\n",
    "        Dictionary with linkers that have 'length_category' field\n",
    "    save_dir : str, optional\n",
    "        Directory to save plots\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from collections import defaultdict\n",
    "\n",
    "    if save_dir:\n",
    "        save_dir = os.path.expanduser(save_dir)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Collect data\n",
    "    data_by_category = {'short': [], 'medium': [], 'long': []}\n",
    "    data_by_type_and_category = {\n",
    "        'n-terminus': {'short': [], 'medium': [], 'long': []},\n",
    "        'c-terminus': {'short': [], 'medium': [], 'long': []},\n",
    "        'inner': {'short': [], 'medium': [], 'long': []}\n",
    "    }\n",
    "\n",
    "    for acc, data in proteins_dict.items():\n",
    "        for linker in data['linkers']:\n",
    "            length = linker['end'] - linker['start'] + 1\n",
    "            category = linker.get('length_category', 'unknown')\n",
    "            linker_type = linker['type']\n",
    "\n",
    "            if category in data_by_category:\n",
    "                data_by_category[category].append(length)\n",
    "                data_by_type_and_category[linker_type][category].append(length)\n",
    "\n",
    "    # Plot 1: Bar chart of counts by category and type\n",
    "    _, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    categories = ['short', 'medium', 'long']\n",
    "    linker_types = ['n-terminus', 'c-terminus', 'inner']\n",
    "\n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.25\n",
    "\n",
    "    colors = {'n-terminus': 'steelblue', 'c-terminus': 'coral', 'inner': 'mediumseagreen'}\n",
    "\n",
    "    for i, linker_type in enumerate(linker_types):\n",
    "        counts = [len(data_by_type_and_category[linker_type][cat]) for cat in categories]\n",
    "        offset = (i - 1) * width\n",
    "        ax.bar(x + offset, counts, width, label=linker_type.capitalize(),\n",
    "               color=colors[linker_type], edgecolor='black', alpha=0.7)\n",
    "\n",
    "    ax.set_xlabel('Length Category', fontsize=12)\n",
    "    ax.set_ylabel('Count', fontsize=12)\n",
    "    ax.set_title('Linker Count by Length Category and Type', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([cat.capitalize() for cat in categories])\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_dir:\n",
    "        plt.savefig(os.path.join(save_dir, 'length_category_counts.png'), dpi=300, bbox_inches='tight')\n",
    "        print(f\"✓ Saved: length_category_counts.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot 2: Box plots showing length distribution per category\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    data_to_plot = [data_by_category['short'], data_by_category['medium'], data_by_category['long']]\n",
    "    labels = ['Short', 'Medium', 'Long']\n",
    "    colors_list = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "\n",
    "    bp = ax.boxplot(data_to_plot, labels=labels, patch_artist=True, showmeans=True,\n",
    "                    meanprops=dict(marker='D', markerfacecolor='red', markeredgecolor='darkred', markersize=8),\n",
    "                    medianprops=dict(color='black', linewidth=2))\n",
    "\n",
    "    for patch, color in zip(bp['boxes'], colors_list):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "\n",
    "    # Add statistics as text\n",
    "    for i, (category, data) in enumerate(zip(['short', 'medium', 'long'], data_to_plot), 1):\n",
    "        if data:\n",
    "            mean_val = np.mean(data)\n",
    "            std_val = np.std(data)\n",
    "            ax.text(i, mean_val, f\"{mean_val:.1f}±{std_val:.1f}\",\n",
    "                   ha='left', va='center', fontsize=9, fontweight='bold',\n",
    "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "\n",
    "    ax.set_ylabel('Length (amino acids)', fontsize=12)\n",
    "    ax.set_title('Linker Length Distribution by Category', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_dir:\n",
    "        plt.savefig(os.path.join(save_dir, 'length_category_boxplot.png'), dpi=300, bbox_inches='tight')\n",
    "        print(f\"✓ Saved: length_category_boxplot.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot 3: Histogram with category boundaries\n",
    "    _, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    all_lengths = []\n",
    "    all_categories = []\n",
    "    for cat in ['short', 'medium', 'long']:\n",
    "        all_lengths.extend(data_by_category[cat])\n",
    "        all_categories.extend([cat] * len(data_by_category[cat]))\n",
    "\n",
    "    # Create histogram\n",
    "    bins = 50\n",
    "    ax.hist([data_by_category['short'], data_by_category['medium'], data_by_category['long']],\n",
    "            bins=bins, label=['Short', 'Medium', 'Long'],\n",
    "            color=colors_list, alpha=0.7, edgecolor='black')\n",
    "\n",
    "    # Add vertical lines at category boundaries\n",
    "    if data_by_category['short'] and data_by_category['medium']:\n",
    "        boundary1 = max(data_by_category['short'])\n",
    "        ax.axvline(boundary1, color='red', linestyle='--', linewidth=2,\n",
    "                   label=f'Short/Medium boundary: {boundary1:.0f} aa')\n",
    "\n",
    "    if data_by_category['medium'] and data_by_category['long']:\n",
    "        boundary2 = max(data_by_category['medium'])\n",
    "        ax.axvline(boundary2, color='darkred', linestyle='--', linewidth=2,\n",
    "                   label=f'Medium/Long boundary: {boundary2:.0f} aa')\n",
    "\n",
    "    ax.set_xlabel('Length (amino acids)', fontsize=12)\n",
    "    ax.set_ylabel('Frequency', fontsize=12)\n",
    "    ax.set_title('Linker Length Distribution with Category Boundaries', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_dir:\n",
    "        plt.savefig(os.path.join(save_dir, 'length_category_histogram.png'), dpi=300, bbox_inches='tight')\n",
    "        print(f\"✓ Saved: length_category_histogram.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot 4: Stacked bar chart by linker type\n",
    "    _, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    linker_types = ['n-terminus', 'c-terminus', 'inner']\n",
    "    short_counts = [len(data_by_type_and_category[lt]['short']) for lt in linker_types]\n",
    "    medium_counts = [len(data_by_type_and_category[lt]['medium']) for lt in linker_types]\n",
    "    long_counts = [len(data_by_type_and_category[lt]['long']) for lt in linker_types]\n",
    "\n",
    "    x = np.arange(len(linker_types))\n",
    "    width = 0.6\n",
    "\n",
    "    _ = ax.bar(x, short_counts, width, label='Short', color='lightblue', edgecolor='black')\n",
    "    _ = ax.bar(x, medium_counts, width, bottom=short_counts, label='Medium',\n",
    "                color='lightgreen', edgecolor='black')\n",
    "    _ = ax.bar(x, long_counts, width,\n",
    "                bottom=np.array(short_counts) + np.array(medium_counts),\n",
    "                label='Long', color='lightcoral', edgecolor='black')\n",
    "\n",
    "    ax.set_ylabel('Count', fontsize=12)\n",
    "    ax.set_title('Length Category Distribution by Linker Type (Stacked)', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([lt.capitalize() for lt in linker_types])\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_dir:\n",
    "        plt.savefig(os.path.join(save_dir, 'length_category_stacked.png'), dpi=300, bbox_inches='tight')\n",
    "        print(f\"✓ Saved: length_category_stacked.png\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n✓ All visualizations complete!\")\n",
    "\n",
    "\n",
    "# Create and save visualizations\n",
    "visualize_length_categories(proteins_with_length, save_dir=workdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27032e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_motifs_by_group(proteins_dict, group_by='region', verbose=True):\n",
    "    \"\"\"\n",
    "    Count ELM motifs across different protein groups.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    proteins_dict : dict\n",
    "        Dictionary with protein accessions as keys and domains/linkers as values\n",
    "        Format: {acc: {'domains': [{'motifs': [...]}], 'linkers': [{'type': ..., 'length_category': ..., 'motifs': [...]}]}}\n",
    "    group_by : str\n",
    "        Grouping method:\n",
    "        - 'region': Group by region type (domains, n-terminus, c-terminus, inner)\n",
    "        - 'length': Group by length category (domains, short, medium, long)\n",
    "    verbose : bool\n",
    "        If True, print detailed statistics\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict with motif counters and statistics for each group\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "\n",
    "    if group_by == 'region':\n",
    "        # Group by region type\n",
    "        groups = {\n",
    "            'domains': {'counter': Counter(), 'with_motifs': 0, 'total': 0},\n",
    "            'n-terminus': {'counter': Counter(), 'with_motifs': 0, 'total': 0},\n",
    "            'c-terminus': {'counter': Counter(), 'with_motifs': 0, 'total': 0},\n",
    "            'inner': {'counter': Counter(), 'with_motifs': 0, 'total': 0}\n",
    "        }\n",
    "        group_labels = {\n",
    "            'domains': 'DOMAINS',\n",
    "            'n-terminus': 'N-TERMINUS LINKERS',\n",
    "            'c-terminus': 'C-TERMINUS LINKERS',\n",
    "            'inner': 'INNER LINKERS'\n",
    "        }\n",
    "    elif group_by == 'length':\n",
    "        # Group by length category\n",
    "        groups = {\n",
    "            'domains': {'counter': Counter(), 'with_motifs': 0, 'total': 0},\n",
    "            'short': {'counter': Counter(), 'with_motifs': 0, 'total': 0},\n",
    "            'medium': {'counter': Counter(), 'with_motifs': 0, 'total': 0},\n",
    "            'long': {'counter': Counter(), 'with_motifs': 0, 'total': 0}\n",
    "        }\n",
    "        group_labels = {\n",
    "            'domains': 'DOMAINS',\n",
    "            'short': 'SHORT LINKERS',\n",
    "            'medium': 'MEDIUM LINKERS',\n",
    "            'long': 'LONG LINKERS'\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown group_by: {group_by}. Use 'region' or 'length'\")\n",
    "\n",
    "    for _, data in proteins_dict.items():\n",
    "        # Count domain motifs (same for both grouping methods)\n",
    "        if 'domains' in data:\n",
    "            for domain in data['domains']:\n",
    "                groups['domains']['total'] += 1\n",
    "                if 'motifs' in domain and domain['motifs']:\n",
    "                    groups['domains']['with_motifs'] += 1\n",
    "                    for motif in domain['motifs']:\n",
    "                        if isinstance(motif, dict):\n",
    "                            motif_name = motif.get('motif', motif.get('name', motif.get('identifier', 'unknown')))\n",
    "                        else:\n",
    "                            motif_name = str(motif)\n",
    "                        groups['domains']['counter'][motif_name] += 1\n",
    "\n",
    "        # Count linker motifs\n",
    "        if 'linkers' in data:\n",
    "            for linker in data['linkers']:\n",
    "                if group_by == 'region':\n",
    "                    # Group by region type\n",
    "                    group_key = linker.get('type', 'unknown')\n",
    "                else:  # group_by == 'length'\n",
    "                    # Group by length category\n",
    "                    group_key = linker.get('length_category', 'unknown')\n",
    "\n",
    "                if group_key in groups:\n",
    "                    groups[group_key]['total'] += 1\n",
    "                    if 'motifs' in linker and linker['motifs']:\n",
    "                        groups[group_key]['with_motifs'] += 1\n",
    "                        for motif in linker['motifs']:\n",
    "                            if isinstance(motif, dict):\n",
    "                                motif_name = motif.get('motif', motif.get('name', motif.get('identifier', 'unknown')))\n",
    "                            else:\n",
    "                                motif_name = str(motif)\n",
    "                            groups[group_key]['counter'][motif_name] += 1\n",
    "\n",
    "    # Calculate statistics\n",
    "    statistics = {}\n",
    "    for group_key, group_data in groups.items():\n",
    "        statistics[group_key] = {\n",
    "            'total_regions': group_data['total'],\n",
    "            'regions_with_motifs': group_data['with_motifs'],\n",
    "            'total_motifs': sum(group_data['counter'].values()),\n",
    "            'unique_motifs': len(group_data['counter']),\n",
    "            'percentage_with_motifs': (group_data['with_motifs'] / group_data['total'] * 100) if group_data['total'] > 0 else 0\n",
    "        }\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"MOTIF DISTRIBUTION BY {group_by.upper()}\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        for group_key in groups.keys():\n",
    "            stats = statistics[group_key]\n",
    "            label = group_labels[group_key]\n",
    "            print(f\"\\n{label}:\")\n",
    "            print(f\"  Total regions: {stats['total_regions']}\")\n",
    "            print(f\"  Regions with motifs: {stats['regions_with_motifs']} ({stats['percentage_with_motifs']:.1f}%)\")\n",
    "            print(f\"  Total motif occurrences: {stats['total_motifs']}\")\n",
    "            print(f\"  Unique motifs: {stats['unique_motifs']}\")\n",
    "\n",
    "            counter = groups[group_key]['counter']\n",
    "            if counter:\n",
    "                print(f\"\\n  Top 10 motifs:\")\n",
    "                for i, (motif, count) in enumerate(counter.most_common(10), 1):\n",
    "                    print(f\"    {i:2d}. {motif}: {count}\")\n",
    "\n",
    "        # Overall comparison\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"OVERALL COMPARISON\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        print(f\"\\n{'Group':<20} {'Total':<10} {'With Motifs':<15} {'%':<8} {'Total Motifs':<15} {'Unique':<10}\")\n",
    "        print(\"-\" * 80)\n",
    "        for group_key in groups.keys():\n",
    "            stats = statistics[group_key]\n",
    "            label = group_labels[group_key].title()\n",
    "            print(f\"{label:<20} {stats['total_regions']:<10} {stats['regions_with_motifs']:<15} \"\n",
    "                  f\"{stats['percentage_with_motifs']:<7.1f}% {stats['total_motifs']:<15} {stats['unique_motifs']:<10}\")\n",
    "\n",
    "    # Prepare return dict\n",
    "    result = {'statistics': statistics}\n",
    "    for group_key, group_data in groups.items():\n",
    "        result[f'{group_key}_motifs'] = group_data['counter']\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Count motifs by region type (default behavior)\n",
    "print(\"=\"*80)\n",
    "print(\"GROUPING BY REGION TYPE\")\n",
    "print(\"=\"*80)\n",
    "motif_counts_by_region = count_motifs_by_group(proteins_with_length, group_by='region', verbose=True)\n",
    "\n",
    "# Count motifs by length category\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"GROUPING BY LENGTH CATEGORY\")\n",
    "print(\"=\"*80)\n",
    "motif_counts_by_length = count_motifs_by_group(proteins_with_length, group_by='length', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7b9d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save proteins with motifs and length categories to JSON\n",
    "save_path = os.path.join(os.path.expanduser(workdir), 'proteins_with_motifs_and_length.json')\n",
    "with open(save_path, 'w') as f:\n",
    "    json.dump(proteins_with_length, f, indent=2)\n",
    "print(f\"✓ Saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ba0830a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MOTIF SET ANALYSIS - LINKER TYPES\n",
      "================================================================================\n",
      "\n",
      "Unique motifs per linker type:\n",
      "  N-terminus: 44 unique motifs\n",
      "  C-terminus: 43 unique motifs\n",
      "  Inner:      146 unique motifs\n",
      "\n",
      "================================================================================\n",
      "COMMON MOTIFS - ALL COMBINATIONS\n",
      "================================================================================\n",
      "\n",
      "1. Common in ALL three types (N ∩ C ∩ I): 23 motifs\n",
      "   Motifs (sorted by total count):\n",
      "     MOD_GlcNHglycan                          | N:  36 C:  30 I: 150 | Total:  216\n",
      "     MOD_GSK3_1                               | N:  18 C:  12 I: 174 | Total:  204\n",
      "     CLV_PCSK_SKI1_1                          | N:  12 C:  12 I: 174 | Total:  198\n",
      "     MOD_CK2_1                                | N:  18 C:  12 I: 150 | Total:  180\n",
      "     DOC_WW_Pin1_4                            | N:  18 C:  12 I: 138 | Total:  168\n",
      "     MOD_ProDKin_1                            | N:  18 C:  12 I: 138 | Total:  168\n",
      "     LIG_SH3_3                                | N:  24 C:   6 I: 126 | Total:  156\n",
      "     DOC_USP7_MATH_1                          | N:  12 C:  18 I: 120 | Total:  150\n",
      "     LIG_LIR_Nem_3                            | N:   6 C:   6 I: 132 | Total:  144\n",
      "     LIG_FHA_2                                | N:   6 C:   6 I: 132 | Total:  144\n",
      "     DEG_Nend_UBRbox_2                        | N:  12 C:  42 I:  90 | Total:  144\n",
      "     DOC_USP7_UBL2_3                          | N:  12 C:  12 I: 114 | Total:  138\n",
      "     LIG_BIR_II_1                             | N:  48 C:  30 I:  54 | Total:  132\n",
      "     DEG_Nend_Nbox_1                          | N:   6 C:  24 I:  84 | Total:  114\n",
      "     DEG_Nend_UBRbox_1                        | N:  18 C:  12 I:  66 | Total:   96\n",
      "     MOD_PKA_2                                | N:  12 C:   6 I:  78 | Total:   96\n",
      "     MOD_N-GLC_1                              | N:   6 C:   6 I:  84 | Total:   96\n",
      "     MOD_PIKK_1                               | N:   6 C:  12 I:  60 | Total:   78\n",
      "     LIG_Arc_Nlobe_1                          | N:   6 C:   6 I:  66 | Total:   78\n",
      "     DEG_Nend_UBRbox_3                        | N:   6 C:  24 I:  12 | Total:   42\n",
      "     ... and 3 more\n",
      "\n",
      "2. Common in N-terminus AND C-terminus ONLY (N ∩ C - I): 0 motifs\n",
      "\n",
      "3. Common in N-terminus AND Inner ONLY (N ∩ I - C): 20 motifs\n",
      "   Top 10 motifs (sorted by total count):\n",
      "     LIG_FHA_1                                | N:   6 I: 216 | Total:  222\n",
      "     MOD_Plk_4                                | N:  12 I: 144 | Total:  156\n",
      "     LIG_14-3-3_CanoR_1                       | N:  18 I: 120 | Total:  138\n",
      "     LIG_SUMO_SIM_par_1                       | N:   6 I: 114 | Total:  120\n",
      "     CLV_NRD_NRD_1                            | N:   6 I:  96 | Total:  102\n",
      "     CLV_PCSK_KEX2_1                          | N:   6 I:  84 | Total:   90\n",
      "     MOD_CDK_SPxxK_3                          | N:   6 I:  66 | Total:   72\n",
      "     MOD_NEK2_2                               | N:   6 I:  48 | Total:   54\n",
      "     LIG_SH3_2                                | N:   6 I:  36 | Total:   42\n",
      "     TRG_ER_diArg_1                           | N:  12 I:  24 | Total:   36\n",
      "\n",
      "4. Common in C-terminus AND Inner ONLY (C ∩ I - N): 20 motifs\n",
      "   Top 10 motifs (sorted by total count):\n",
      "     LIG_WD40_WDR5_VDV_2                      | C:  24 I: 270 | Total:  294\n",
      "     MOD_CK1_1                                | C:  18 I: 138 | Total:  156\n",
      "     MOD_SUMO_for_1                           | C:  12 I:  78 | Total:   90\n",
      "     CLV_C14_Caspase3-7                       | C:  12 I:  66 | Total:   78\n",
      "     LIG_BRCT_BRCA1_1                         | C:   6 I:  54 | Total:   60\n",
      "     DOC_PP1_RVXF_1                           | C:   6 I:  54 | Total:   60\n",
      "     DOC_PP2A_B56_1                           | C:   6 I:  36 | Total:   42\n",
      "     LIG_PDZ_Class_3                          | C:   6 I:  24 | Total:   30\n",
      "     MOD_CDK_SPxK_1                           | C:   6 I:  24 | Total:   30\n",
      "     DEG_CRBN_cyclicCter_1                    | C:   6 I:  18 | Total:   24\n",
      "\n",
      "================================================================================\n",
      "UNIQUE MOTIFS (EXCLUSIVE TO ONE TYPE)\n",
      "================================================================================\n",
      "\n",
      "5. Unique to N-terminus ONLY: 1 motifs\n",
      "   Top 10 motifs (sorted by count):\n",
      "     MOD_PKB_1                                | Count:   6\n",
      "\n",
      "6. Unique to C-terminus ONLY: 0 motifs\n",
      "\n",
      "7. Unique to Inner ONLY: 83 motifs\n",
      "   Top 10 motifs (sorted by count):\n",
      "     MOD_Plk_1                                | Count: 162\n",
      "     LIG_SH2_STAT5                            | Count: 150\n",
      "     MOD_SUMO_rev_2                           | Count: 144\n",
      "     LIG_LIR_Gen_1                            | Count: 114\n",
      "     LIG_SH2_STAP1                            | Count: 114\n",
      "     DOC_MAPK_MEF2A_6                         | Count:  90\n",
      "     TRG_ENDOCYTIC_2                          | Count:  90\n",
      "     DOC_PP2B_PxIxIT_1                        | Count:  84\n",
      "     CLV_PCSK_PC1ET2_1                        | Count:  78\n",
      "     DOC_MAPK_gen_1                           | Count:  72\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Total unique motifs across all linker types: 147\n",
      "  Common to all 3 types:       23 ( 15.6%)\n",
      "  Common to exactly 2 types:   40 ( 27.2%)\n",
      "    - N & C only:               0\n",
      "    - N & I only:              20\n",
      "    - C & I only:              20\n",
      "  Unique to 1 type only:       84 ( 57.1%)\n",
      "    - N-terminus only:          1\n",
      "    - C-terminus only:          0\n",
      "    - Inner only:              83\n",
      "\n",
      "✓ Saved set analysis to: /home/pospim/Desktop/work/protein_linkers/input_1/type_based/motif_set_analysis.tsv\n"
     ]
    }
   ],
   "source": [
    "# Set operations on motifs across linker types\n",
    "# ==============================================\n",
    "\n",
    "# Get motif sets for each linker type (exclude domains)\n",
    "n_terminus_set = set(motif_counts_by_region['n-terminus_motifs'].keys())\n",
    "c_terminus_set = set(motif_counts_by_region['c-terminus_motifs'].keys())\n",
    "inner_set = set(motif_counts_by_region['inner_motifs'].keys())\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MOTIF SET ANALYSIS - LINKER TYPES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nUnique motifs per linker type:\")\n",
    "print(f\"  N-terminus: {len(n_terminus_set)} unique motifs\")\n",
    "print(f\"  C-terminus: {len(c_terminus_set)} unique motifs\")\n",
    "print(f\"  Inner:      {len(inner_set)} unique motifs\")\n",
    "\n",
    "# Common motifs across all combinations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMMON MOTIFS - ALL COMBINATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# All three (intersection of all)\n",
    "common_all = n_terminus_set & c_terminus_set & inner_set\n",
    "print(f\"\\n1. Common in ALL three types (N ∩ C ∩ I): {len(common_all)} motifs\")\n",
    "if common_all:\n",
    "    print(\"   Motifs (sorted by total count):\")\n",
    "    # Sort by total count across all types\n",
    "    common_all_sorted = sorted(\n",
    "        common_all,\n",
    "        key=lambda m: (motif_counts_by_region['n-terminus_motifs'][m] +\n",
    "                      motif_counts_by_region['c-terminus_motifs'][m] +\n",
    "                      motif_counts_by_region['inner_motifs'][m]),\n",
    "        reverse=True\n",
    "    )\n",
    "    for motif in common_all_sorted[:20]:  # Show top 20\n",
    "        n_count = motif_counts_by_region['n-terminus_motifs'][motif]\n",
    "        c_count = motif_counts_by_region['c-terminus_motifs'][motif]\n",
    "        i_count = motif_counts_by_region['inner_motifs'][motif]\n",
    "        total = n_count + c_count + i_count\n",
    "        print(f\"     {motif:<40} | N:{n_count:>4} C:{c_count:>4} I:{i_count:>4} | Total:{total:>5}\")\n",
    "    if len(common_all) > 20:\n",
    "        print(f\"     ... and {len(common_all) - 20} more\")\n",
    "\n",
    "# N-terminus and C-terminus only\n",
    "common_nc = (n_terminus_set & c_terminus_set) - inner_set\n",
    "common_nc_sorted = sorted(\n",
    "    common_nc,\n",
    "    key=lambda m: motif_counts_by_region['n-terminus_motifs'][m] + motif_counts_by_region['c-terminus_motifs'][m],\n",
    "    reverse=True\n",
    ") if common_nc else []\n",
    "print(f\"\\n2. Common in N-terminus AND C-terminus ONLY (N ∩ C - I): {len(common_nc)} motifs\")\n",
    "if common_nc:\n",
    "    print(\"   Top 10 motifs (sorted by total count):\")\n",
    "    for motif in common_nc_sorted[:10]:\n",
    "        n_count = motif_counts_by_region['n-terminus_motifs'][motif]\n",
    "        c_count = motif_counts_by_region['c-terminus_motifs'][motif]\n",
    "        total = n_count + c_count\n",
    "        print(f\"     {motif:<40} | N:{n_count:>4} C:{c_count:>4} | Total:{total:>5}\")\n",
    "\n",
    "# N-terminus and Inner only\n",
    "common_ni = (n_terminus_set & inner_set) - c_terminus_set\n",
    "common_ni_sorted = sorted(\n",
    "    common_ni,\n",
    "    key=lambda m: motif_counts_by_region['n-terminus_motifs'][m] + motif_counts_by_region['inner_motifs'][m],\n",
    "    reverse=True\n",
    ") if common_ni else []\n",
    "print(f\"\\n3. Common in N-terminus AND Inner ONLY (N ∩ I - C): {len(common_ni)} motifs\")\n",
    "if common_ni:\n",
    "    print(\"   Top 10 motifs (sorted by total count):\")\n",
    "    for motif in common_ni_sorted[:10]:\n",
    "        n_count = motif_counts_by_region['n-terminus_motifs'][motif]\n",
    "        i_count = motif_counts_by_region['inner_motifs'][motif]\n",
    "        total = n_count + i_count\n",
    "        print(f\"     {motif:<40} | N:{n_count:>4} I:{i_count:>4} | Total:{total:>5}\")\n",
    "\n",
    "# C-terminus and Inner only\n",
    "common_ci = (c_terminus_set & inner_set) - n_terminus_set\n",
    "common_ci_sorted = sorted(\n",
    "    common_ci,\n",
    "    key=lambda m: motif_counts_by_region['c-terminus_motifs'][m] + motif_counts_by_region['inner_motifs'][m],\n",
    "    reverse=True\n",
    ") if common_ci else []\n",
    "print(f\"\\n4. Common in C-terminus AND Inner ONLY (C ∩ I - N): {len(common_ci)} motifs\")\n",
    "if common_ci:\n",
    "    print(\"   Top 10 motifs (sorted by total count):\")\n",
    "    for motif in common_ci_sorted[:10]:\n",
    "        c_count = motif_counts_by_region['c-terminus_motifs'][motif]\n",
    "        i_count = motif_counts_by_region['inner_motifs'][motif]\n",
    "        total = c_count + i_count\n",
    "        print(f\"     {motif:<40} | C:{c_count:>4} I:{i_count:>4} | Total:{total:>5}\")\n",
    "\n",
    "# Unique to each type\n",
    "unique_n = n_terminus_set - c_terminus_set - inner_set\n",
    "unique_c = c_terminus_set - n_terminus_set - inner_set\n",
    "unique_i = inner_set - n_terminus_set - c_terminus_set\n",
    "\n",
    "unique_n_sorted = sorted(unique_n, key=lambda m: motif_counts_by_region['n-terminus_motifs'][m], reverse=True) if unique_n else []\n",
    "unique_c_sorted = sorted(unique_c, key=lambda m: motif_counts_by_region['c-terminus_motifs'][m], reverse=True) if unique_c else []\n",
    "unique_i_sorted = sorted(unique_i, key=lambda m: motif_counts_by_region['inner_motifs'][m], reverse=True) if unique_i else []\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"UNIQUE MOTIFS (EXCLUSIVE TO ONE TYPE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n5. Unique to N-terminus ONLY: {len(unique_n)} motifs\")\n",
    "if unique_n:\n",
    "    print(\"   Top 10 motifs (sorted by count):\")\n",
    "    for motif in unique_n_sorted[:10]:\n",
    "        count = motif_counts_by_region['n-terminus_motifs'][motif]\n",
    "        print(f\"     {motif:<40} | Count:{count:>4}\")\n",
    "\n",
    "print(f\"\\n6. Unique to C-terminus ONLY: {len(unique_c)} motifs\")\n",
    "if unique_c:\n",
    "    print(\"   Top 10 motifs (sorted by count):\")\n",
    "    for motif in unique_c_sorted[:10]:\n",
    "        count = motif_counts_by_region['c-terminus_motifs'][motif]\n",
    "        print(f\"     {motif:<40} | Count:{count:>4}\")\n",
    "\n",
    "print(f\"\\n7. Unique to Inner ONLY: {len(unique_i)} motifs\")\n",
    "if unique_i:\n",
    "    print(\"   Top 10 motifs (sorted by count):\")\n",
    "    for motif in unique_i_sorted[:10]:\n",
    "        count = motif_counts_by_region['inner_motifs'][motif]\n",
    "        print(f\"     {motif:<40} | Count:{count:>4}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "total_union = n_terminus_set | c_terminus_set | inner_set\n",
    "print(f\"\\nTotal unique motifs across all linker types: {len(total_union)}\")\n",
    "print(f\"  Common to all 3 types:     {len(common_all):>4} ({len(common_all)/len(total_union)*100:>5.1f}%)\")\n",
    "print(f\"  Common to exactly 2 types: {len(common_nc) + len(common_ni) + len(common_ci):>4} ({(len(common_nc) + len(common_ni) + len(common_ci))/len(total_union)*100:>5.1f}%)\")\n",
    "print(f\"    - N & C only:            {len(common_nc):>4}\")\n",
    "print(f\"    - N & I only:            {len(common_ni):>4}\")\n",
    "print(f\"    - C & I only:            {len(common_ci):>4}\")\n",
    "print(f\"  Unique to 1 type only:     {len(unique_n) + len(unique_c) + len(unique_i):>4} ({(len(unique_n) + len(unique_c) + len(unique_i))/len(total_union)*100:>5.1f}%)\")\n",
    "print(f\"    - N-terminus only:       {len(unique_n):>4}\")\n",
    "print(f\"    - C-terminus only:       {len(unique_c):>4}\")\n",
    "print(f\"    - Inner only:            {len(unique_i):>4}\")\n",
    "\n",
    "# Save to file in TSV format\n",
    "set_analysis_file = os.path.join(os.path.expanduser(workdir), 'type_based', 'motif_set_analysis.tsv')\n",
    "with open(set_analysis_file, 'w') as f:\n",
    "    # 1. COMMON IN ALL THREE TYPES\n",
    "    f.write(\"#1. COMMON IN ALL THREE TYPES\\n\")\n",
    "    f.write(\"Motif\\tN\\tC\\tI\\tTotal\\n\")\n",
    "    for motif in common_all_sorted:\n",
    "        n_count = motif_counts_by_region['n-terminus_motifs'][motif]\n",
    "        c_count = motif_counts_by_region['c-terminus_motifs'][motif]\n",
    "        i_count = motif_counts_by_region['inner_motifs'][motif]\n",
    "        total = n_count + c_count + i_count\n",
    "        f.write(f\"{motif}\\t{n_count}\\t{c_count}\\t{i_count}\\t{total}\\n\")\n",
    "\n",
    "    # 2. COMMON IN N & C ONLY\n",
    "    f.write(\"\\n#2. COMMON IN N & C ONLY\\n\")\n",
    "    f.write(\"Motif\\tN\\tC\\tTotal\\n\")\n",
    "    for motif in common_nc_sorted:\n",
    "        n_count = motif_counts_by_region['n-terminus_motifs'][motif]\n",
    "        c_count = motif_counts_by_region['c-terminus_motifs'][motif]\n",
    "        total = n_count + c_count\n",
    "        f.write(f\"{motif}\\t{n_count}\\t{c_count}\\t{total}\\n\")\n",
    "\n",
    "    # 3. COMMON IN N & I ONLY\n",
    "    f.write(\"\\n#3. COMMON IN N & I ONLY\\n\")\n",
    "    f.write(\"Motif\\tN\\tI\\tTotal\\n\")\n",
    "    for motif in common_ni_sorted:\n",
    "        n_count = motif_counts_by_region['n-terminus_motifs'][motif]\n",
    "        i_count = motif_counts_by_region['inner_motifs'][motif]\n",
    "        total = n_count + i_count\n",
    "        f.write(f\"{motif}\\t{n_count}\\t{i_count}\\t{total}\\n\")\n",
    "\n",
    "    # 4. COMMON IN C & I ONLY\n",
    "    f.write(\"\\n#4. COMMON IN C & I ONLY\\n\")\n",
    "    f.write(\"Motif\\tC\\tI\\tTotal\\n\")\n",
    "    for motif in common_ci_sorted:\n",
    "        c_count = motif_counts_by_region['c-terminus_motifs'][motif]\n",
    "        i_count = motif_counts_by_region['inner_motifs'][motif]\n",
    "        total = c_count + i_count\n",
    "        f.write(f\"{motif}\\t{c_count}\\t{i_count}\\t{total}\\n\")\n",
    "\n",
    "    # 5. UNIQUE TO N-TERMINUS\n",
    "    f.write(\"\\n#5. UNIQUE TO N-TERMINUS\\n\")\n",
    "    f.write(\"Motif\\tCount\\n\")\n",
    "    for motif in unique_n_sorted:\n",
    "        count = motif_counts_by_region['n-terminus_motifs'][motif]\n",
    "        f.write(f\"{motif}\\t{count}\\n\")\n",
    "\n",
    "    # 6. UNIQUE TO C-TERMINUS\n",
    "    f.write(\"\\n#6. UNIQUE TO C-TERMINUS\\n\")\n",
    "    f.write(\"Motif\\tCount\\n\")\n",
    "    for motif in unique_c_sorted:\n",
    "        count = motif_counts_by_region['c-terminus_motifs'][motif]\n",
    "        f.write(f\"{motif}\\t{count}\\n\")\n",
    "\n",
    "    # 7. UNIQUE TO INNER\n",
    "    f.write(\"\\n#7. UNIQUE TO INNER\\n\")\n",
    "    f.write(\"Motif\\tCount\\n\")\n",
    "    for motif in unique_i_sorted:\n",
    "        count = motif_counts_by_region['inner_motifs'][motif]\n",
    "        f.write(f\"{motif}\\t{count}\\n\")\n",
    "\n",
    "print(f\"\\n✓ Saved set analysis to: {set_analysis_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736d5353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize motif set operations\n",
    "# ================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create figure with multiple subplots\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "# 1. Pie chart: Overall distribution (common to all, common to 2, unique)\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "sizes = [\n",
    "    len(common_all),\n",
    "    len(common_nc) + len(common_ni) + len(common_ci),\n",
    "    len(unique_n) + len(unique_c) + len(unique_i)\n",
    "]\n",
    "labels = [\n",
    "    f'Common to all 3\\n({len(common_all)} motifs)',\n",
    "    f'Common to exactly 2\\n({len(common_nc) + len(common_ni) + len(common_ci)} motifs)',\n",
    "    f'Unique to 1 type\\n({len(unique_n) + len(unique_c) + len(unique_i)} motifs)'\n",
    "]\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
    "explode = (0.05, 0.05, 0.05)\n",
    "\n",
    "wedges, texts, autotexts = ax1.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%',\n",
    "                                     startangle=90, explode=explode, textprops={'fontsize': 10})\n",
    "ax1.set_title('Overall Motif Distribution Pattern', fontsize=12, fontweight='bold', pad=20)\n",
    "\n",
    "# 2. Pie chart: Breakdown of \"common to exactly 2\"\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "sizes_2 = [len(common_nc), len(common_ni), len(common_ci)]\n",
    "labels_2 = [\n",
    "    f'N & C only\\n({len(common_nc)} motifs)',\n",
    "    f'N & Inner only\\n({len(common_ni)} motifs)',\n",
    "    f'C & Inner only\\n({len(common_ci)} motifs)'\n",
    "]\n",
    "colors_2 = ['#9b59b6', '#f39c12', '#1abc9c']\n",
    "explode_2 = (0.05, 0.05, 0.05)\n",
    "\n",
    "wedges2, texts2, autotexts2 = ax2.pie(sizes_2, labels=labels_2, colors=colors_2, autopct='%1.1f%%',\n",
    "                                        startangle=90, explode=explode_2, textprops={'fontsize': 10})\n",
    "ax2.set_title('Motifs Common to Exactly 2 Types', fontsize=12, fontweight='bold', pad=20)\n",
    "\n",
    "# 3. Pie chart: Breakdown of \"unique to 1 type\"\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "sizes_3 = [len(unique_n), len(unique_c), len(unique_i)]\n",
    "labels_3 = [\n",
    "    f'N-terminus only\\n({len(unique_n)} motifs)',\n",
    "    f'C-terminus only\\n({len(unique_c)} motifs)',\n",
    "    f'Inner only\\n({len(unique_i)} motifs)'\n",
    "]\n",
    "colors_3 = ['#e67e22', '#34495e', '#16a085']\n",
    "explode_3 = (0.05, 0.05, 0.05)\n",
    "\n",
    "wedges3, texts3, autotexts3 = ax3.pie(sizes_3, labels=labels_3, colors=colors_3, autopct='%1.1f%%',\n",
    "                                        startangle=90, explode=explode_3, textprops={'fontsize': 10})\n",
    "ax3.set_title('Motifs Unique to Single Type', fontsize=12, fontweight='bold', pad=20)\n",
    "\n",
    "# 4. Venn diagram representation (stacked bar chart)\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "categories = ['Common\\nto all 3', 'N & C\\nonly', 'N & I\\nonly', 'C & I\\nonly',\n",
    "              'N-term\\nonly', 'C-term\\nonly', 'Inner\\nonly']\n",
    "values = [len(common_all), len(common_nc), len(common_ni), len(common_ci),\n",
    "          len(unique_n), len(unique_c), len(unique_i)]\n",
    "colors_bar = ['#2ecc71', '#9b59b6', '#f39c12', '#1abc9c', '#e67e22', '#34495e', '#16a085']\n",
    "\n",
    "bars = ax4.bar(categories, values, color=colors_bar, edgecolor='black', linewidth=1.5)\n",
    "ax4.set_ylabel('Number of Motifs', fontsize=11, fontweight='bold')\n",
    "ax4.set_title('Motif Set Operations - Detailed Breakdown', fontsize=12, fontweight='bold', pad=20)\n",
    "ax4.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax4.set_ylim(0, max(values) * 1.15)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height)}',\n",
    "             ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45, ha='right', fontsize=9)\n",
    "\n",
    "# 5. Comparison of motif set sizes\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "linker_types = ['N-terminus', 'C-terminus', 'Inner']\n",
    "set_sizes = [len(n_terminus_set), len(c_terminus_set), len(inner_set)]\n",
    "colors_comp = ['#e67e22', '#34495e', '#16a085']\n",
    "\n",
    "bars2 = ax5.bar(linker_types, set_sizes, color=colors_comp, edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "ax5.set_ylabel('Number of Unique Motifs', fontsize=11, fontweight='bold')\n",
    "ax5.set_title('Total Unique Motifs per Linker Type', fontsize=12, fontweight='bold', pad=20)\n",
    "ax5.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax5.set_ylim(0, max(set_sizes) * 1.15)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax5.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height)}',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# 6. Stacked bar showing composition of each linker type\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "\n",
    "# Calculate components for each type\n",
    "n_exclusive = len(unique_n)\n",
    "n_with_c = len(common_nc)\n",
    "n_with_i = len(common_ni)\n",
    "n_all_three = len(common_all)\n",
    "\n",
    "c_exclusive = len(unique_c)\n",
    "c_with_n = len(common_nc)\n",
    "c_with_i = len(common_ci)\n",
    "c_all_three = len(common_all)\n",
    "\n",
    "i_exclusive = len(unique_i)\n",
    "i_with_n = len(common_ni)\n",
    "i_with_c = len(common_ci)\n",
    "i_all_three = len(common_all)\n",
    "\n",
    "x_pos = np.arange(3)\n",
    "width = 0.6\n",
    "\n",
    "p1 = ax6.bar(x_pos, [n_exclusive, c_exclusive, i_exclusive], width,\n",
    "             label='Exclusive', color='#e74c3c', edgecolor='black')\n",
    "p2 = ax6.bar(x_pos, [n_with_c + n_with_i, c_with_n + c_with_i, i_with_n + i_with_c], width,\n",
    "             bottom=[n_exclusive, c_exclusive, i_exclusive],\n",
    "             label='Shared with 1 other', color='#3498db', edgecolor='black')\n",
    "p3 = ax6.bar(x_pos, [n_all_three, c_all_three, i_all_three], width,\n",
    "             bottom=[n_exclusive + n_with_c + n_with_i,\n",
    "                     c_exclusive + c_with_n + c_with_i,\n",
    "                     i_exclusive + i_with_n + i_with_c],\n",
    "             label='Shared with all', color='#2ecc71', edgecolor='black')\n",
    "\n",
    "ax6.set_ylabel('Number of Motifs', fontsize=11, fontweight='bold')\n",
    "ax6.set_title('Motif Composition by Linker Type', fontsize=12, fontweight='bold', pad=20)\n",
    "ax6.set_xticks(x_pos)\n",
    "ax6.set_xticklabels(linker_types)\n",
    "ax6.legend(loc='upper right', fontsize=9)\n",
    "ax6.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "viz_file = os.path.join(os.path.expanduser(workdir), 'type_based', 'motif_set_visualizations.png')\n",
    "plt.savefig(viz_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"✓ Saved visualizations to: {viz_file}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VISUALIZATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTotal unique motifs across all linker types: {len(total_union)}\")\n",
    "print(f\"\\nSharing patterns:\")\n",
    "print(f\"  • {len(common_all)} motifs ({len(common_all)/len(total_union)*100:.1f}%) are found in ALL three linker types\")\n",
    "print(f\"  • {len(common_nc) + len(common_ni) + len(common_ci)} motifs ({(len(common_nc) + len(common_ni) + len(common_ci))/len(total_union)*100:.1f}%) are shared by exactly 2 types\")\n",
    "print(f\"  • {len(unique_n) + len(unique_c) + len(unique_i)} motifs ({(len(unique_n) + len(unique_c) + len(unique_i))/len(total_union)*100:.1f}%) are exclusive to 1 type\")\n",
    "print(f\"\\nMotif diversity by linker type:\")\n",
    "print(f\"  • N-terminus: {len(n_terminus_set)} unique motifs\")\n",
    "print(f\"  • C-terminus: {len(c_terminus_set)} unique motifs\")\n",
    "print(f\"  • Inner: {len(inner_set)} unique motifs\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
