{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d6e0f7",
   "metadata": {},
   "source": [
    "# Protein Linker and Domain Analysis\n",
    "\n",
    "This notebook provides comprehensive analysis of protein linker regions and domains from InterPro database.\n",
    "\n",
    "## Overview\n",
    "\n",
    "**What are protein linkers?**  \n",
    "Linkers are unstructured regions between protein domains. They can be:\n",
    "- **N-terminus**: Region before the first domain\n",
    "- **C-terminus**: Region after the last domain  \n",
    "- **Inner linkers**: Regions between domains\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Fetches domain annotations from InterPro\n",
    "2. Calculates linker regions between domains\n",
    "3. Analyzes amino acid composition and length distributions\n",
    "4. Creates visualizations\n",
    "5. Clusters linkers by length for comparative analysis\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. **Data Collection**: Query InterPro API for domain annotations and merge overlaps\n",
    "2. **Linker Calculation**: Compute regions not covered by domains\n",
    "3. **Filtering**: Remove proteins with no domains\n",
    "4. **Statistical Analysis**: Calculate composition and length statistics\n",
    "5. **Visualization**: Generate plots and comparisons\n",
    "6. **Clustering**: Group linkers by length characteristics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0b596a",
   "metadata": {},
   "source": [
    "## 1. Core Functions: Domain Retrieval from InterPro\n",
    "\n",
    "These functions query the InterPro API to retrieve protein domain annotations.\n",
    "\n",
    "**Key functions:**\n",
    "- `_get_protein_seq()`: Fetches protein sequence from UniProt\n",
    "- `_extract_domains_from_source()`: Queries specific InterPro databases (Pfam, SMART, etc.)\n",
    "- `_get_interpro_domains()`: Aggregates domains from all databases\n",
    "- `summarize_protein_domains_dict()`: Returns complete domain information for a protein\n",
    "- `filter_proteins_without_domains()`: Removes proteins with no domain annotations\n",
    "\n",
    "**Features:**\n",
    "- Pagination support for large result sets\n",
    "- Representative-only filtering to reduce redundancy\n",
    "- Multi-database querying (Pfam, SMART, CDD, etc.)\n",
    "- Quality control filtering for meaningful analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2624ef57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, math, os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "INTERPRO_URL = \"https://www.ebi.ac.uk/interpro/api\"\n",
    "CATH_URL = f\"https://www.cathdb.info/version/v4_3_0/api/rest/uniprot_to_funfam\"\n",
    "\n",
    "\n",
    "KNOWN_DATABASES = [\"InterPro\", \"cathgene3d\", \"cdd\", \"HAMAP\", \"panther\", \"Pfam\", \"PIRSF\", \"PRINTS\", \"ssf\", \"antifam\",\n",
    "                        \"PROSITE\" \"Patterns\", \"PROSITE\", \"profile\", \"smart\", \"SFLD\", \"SUPERFAMILY\", \"ncbifam\"]\n",
    "# ================================\n",
    "# FINAL CONSOLIDATED FUNCTIONS\n",
    "# ================================\n",
    "\n",
    "def _get_protein_seq(uniprot_acc):\n",
    "    \"\"\"\n",
    "    Fetch protein sequence from UniProt API.\n",
    "    Returns tuple (sequence, length) or (None, -1) if not found.\n",
    "    \"\"\"\n",
    "    url = f\"https://rest.uniprot.org/uniprotkb/{uniprot_acc}.json\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        seq = data.get(\"sequence\", {}).get(\"value\")\n",
    "        if seq:\n",
    "            return seq, len(seq)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {uniprot_acc}: {e}\")\n",
    "    return None, -1\n",
    "\n",
    "# Helper function to extract domains from a specific source\n",
    "def _extract_domains_from_source(uniprot_acc, source, representative_only=False):\n",
    "    \"\"\"\n",
    "    Extract domains from a specific InterPro database source with pagination support.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    uniprot_acc : str\n",
    "        UniProt accession\n",
    "    source : str\n",
    "        Database source ('pfam', 'smart', etc.)\n",
    "    representative_only : bool\n",
    "        If True, only return representative domain hits\n",
    "    \"\"\"\n",
    "    url = f\"{INTERPRO_URL}/entry/{source}/protein/UniProt/{uniprot_acc}?page_size=200\"\n",
    "    domains = []\n",
    "\n",
    "    while url:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=30)\n",
    "            if response.status_code == 404:\n",
    "                break\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            results = data.get(\"results\", []) if isinstance(data, dict) else data\n",
    "\n",
    "            for entry in results:\n",
    "                # Get entry metadata\n",
    "                meta = entry.get(\"metadata\", {})\n",
    "                entry_acc = meta.get(\"accession\")\n",
    "                entry_name = meta.get(\"name\", \"\")\n",
    "                entry_type = meta.get(\"type\")\n",
    "                source_db = meta.get(\"source_database\")\n",
    "\n",
    "                # Find target protein and extract coordinates\n",
    "                for protein in entry.get(\"proteins\", []):\n",
    "                    protein_acc = protein.get(\"accession\")\n",
    "                    if protein_acc and protein_acc.upper() == uniprot_acc.upper():\n",
    "                        # Check for matches (entry_protein_locations)\n",
    "                        for location in protein.get(\"entry_protein_locations\", []):\n",
    "                            # KEY: representative flag is at the LOCATION level, not fragment level\n",
    "                            is_representative = bool(location.get(\"representative\"))\n",
    "\n",
    "                            # KEY FILTER: Skip non-representative locations if filtering\n",
    "                            if representative_only and not is_representative:\n",
    "                                continue\n",
    "\n",
    "                            for fragment in location.get(\"fragments\", []):\n",
    "                                # Skip domains with null/None/empty names\n",
    "                                if entry_name and entry_name != '':\n",
    "                                    domains.append({\n",
    "                                        \"accession\": entry_acc,\n",
    "                                        \"name\": entry_name,\n",
    "                                        \"type\": entry_type,\n",
    "                                        \"source\": source_db,\n",
    "                                        \"start\": fragment.get(\"start\"),\n",
    "                                        \"end\": fragment.get(\"end\"),\n",
    "                                        \"score\": location.get(\"score\"),\n",
    "                                        \"representative\": is_representative\n",
    "                                    })\n",
    "\n",
    "            # Follow pagination\n",
    "            url = data.get(\"next\") if isinstance(data, dict) else None\n",
    "\n",
    "        except requests.exceptions.RequestException:\n",
    "            break\n",
    "\n",
    "    return domains\n",
    "\n",
    "def _get_interpro_domains(uniprot_acc, source=None, representative_only=False):\n",
    "    \"\"\"\n",
    "    Get protein domain coordinates from InterPro API.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    uniprot_acc : str\n",
    "        UniProt accession (e.g., 'P28482')\n",
    "    source : str, list, or None\n",
    "        - str: Query a specific database ('pfam', 'smart', 'prosite', etc.)\n",
    "        - list: Query multiple specific databases (['pfam', 'smart'])\n",
    "        - None: Query all available databases (default: representative hits only)\n",
    "    representative_only : bool or None\n",
    "        - True: Only return representative domain hits (reduces redundancy)\n",
    "        - False: Return all hits\n",
    "        - None: Auto-decide (True if source=None, False if source is specified)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list of dict\n",
    "        Each domain contains: accession, name, type, source, start, end, score, representative\n",
    "        Sorted by start coordinate, then end coordinate\n",
    "    \"\"\"\n",
    "\n",
    "    if source:\n",
    "        # Handle list of databases\n",
    "        if isinstance(source, list):\n",
    "            all_domains = []\n",
    "            available_dbs = []\n",
    "\n",
    "            print(f\"Querying specified databases for {uniprot_acc}: {', '.join(source)}\")\n",
    "            if representative_only:\n",
    "                print(\"(filtering for representative hits only)\")\n",
    "\n",
    "            for db in source:\n",
    "                domains = _extract_domains_from_source(uniprot_acc, db, representative_only)\n",
    "                if domains:\n",
    "                    available_dbs.append(f\"{db}({len(domains)})\")\n",
    "                    all_domains.extend(domains)\n",
    "\n",
    "            if available_dbs:\n",
    "                print(f\"Found data in: {', '.join(available_dbs)}\")\n",
    "            else:\n",
    "                print(f\"No data found in specified databases\")\n",
    "\n",
    "            # Sort by start, then end coordinate\n",
    "            return sorted(all_domains, key=lambda x: (x[\"start\"] if x[\"start\"] is not None else float('inf'),\n",
    "                                                       x[\"end\"] if x[\"end\"] is not None else float('inf')))\n",
    "\n",
    "        # Handle single database (string)\n",
    "        else:\n",
    "            domains = _extract_domains_from_source(uniprot_acc, source, representative_only)\n",
    "            # Sort by start, then end coordinate\n",
    "            return sorted(domains, key=lambda x: (x[\"start\"] if x[\"start\"] is not None else float('inf'),\n",
    "                                                   x[\"end\"] if x[\"end\"] is not None else float('inf')))\n",
    "\n",
    "    else:\n",
    "        # Query all known databases (default: representative only)\n",
    "        all_domains = []\n",
    "        available_dbs = []\n",
    "\n",
    "        print(f\"Scanning all databases for {uniprot_acc}...\")\n",
    "        if representative_only:\n",
    "            print(\"(filtering for representative hits only)\")\n",
    "\n",
    "        for db in KNOWN_DATABASES:\n",
    "            domains = _extract_domains_from_source(uniprot_acc, db, representative_only)\n",
    "            if domains:\n",
    "                available_dbs.append(f\"{db}({len(domains)})\")\n",
    "                all_domains.extend(domains)\n",
    "\n",
    "        print(f\"Found data in: {', '.join(available_dbs)}\")\n",
    "        # Sort by start, then end coordinate\n",
    "        return sorted(all_domains, key=lambda x: (x[\"start\"] if x[\"start\"] is not None else float('inf'),\n",
    "                                                   x[\"end\"] if x[\"end\"] is not None else float('inf')))\n",
    "\n",
    "\n",
    "def filter_proteins_without_domains(proteins_dict, domains_list=None, save_path=None):\n",
    "    \"\"\"\n",
    "    Filter out proteins that have no domain annotations.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    proteins_dict : dict\n",
    "        Dictionary with protein accessions as keys and domain/linker info as values\n",
    "        Format: {acc: {'domains': [...], 'linkers': [...]}}\n",
    "    domains_list : list, optional\n",
    "        Analysis-ready list of domain data to filter in parallel\n",
    "    save_path : str, optional\n",
    "        Path to save filtered proteins_dict as JSON\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple of (filtered_proteins_dict, filtered_domains_list, stats)\n",
    "        - filtered_proteins_dict: Dict with only proteins having domains\n",
    "        - filtered_domains_list: Filtered domains list (None if not provided)\n",
    "        - stats: Dict with 'total_before', 'no_domains', 'total_after', 'removed'\n",
    "    \"\"\"\n",
    "    # Count before filtering\n",
    "    total_before = len(proteins_dict)\n",
    "    no_domains_count = sum(1 for data in proteins_dict.values() if len(data['domains']) == 0)\n",
    "\n",
    "    # Filter proteins_dict - keep only proteins with at least one domain\n",
    "    filtered_proteins = {\n",
    "        accession: data\n",
    "        for accession, data in proteins_dict.items()\n",
    "        if len(data['domains']) > 0\n",
    "    }\n",
    "\n",
    "    # Filter domains list if provided\n",
    "    filtered_domains = None\n",
    "    if domains_list is not None:\n",
    "        filtered_domains = [\n",
    "            protein_data\n",
    "            for protein_data in domains_list\n",
    "            if len(protein_data['domains']) > 0\n",
    "        ]\n",
    "\n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        'total_before': total_before,\n",
    "        'no_domains': no_domains_count,\n",
    "        'total_after': len(filtered_proteins),\n",
    "        'removed': total_before - len(filtered_proteins)\n",
    "    }\n",
    "\n",
    "    # Save if path provided\n",
    "    if save_path:\n",
    "        import json\n",
    "        with open(os.path.expanduser(save_path), 'w') as f:\n",
    "            json.dump(filtered_proteins, f, indent=2)\n",
    "\n",
    "    return filtered_proteins, filtered_domains, stats\n",
    "\n",
    "def summarize_protein_domains_dict(uniprot_acc, source=None, representative_only=True, skip_if_no_domains=False):\n",
    "    \"\"\"\n",
    "    Return a dict with protein name, domains (with coords and scores), and protein length.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    uniprot_acc : str\n",
    "        UniProt accession\n",
    "    source : str, list, or None\n",
    "        - str: Query a specific database\n",
    "        - list: Query multiple specific databases\n",
    "        - None: Query all available databases\n",
    "    representative_only : bool or None\n",
    "        - True: Only return representative domain hits\n",
    "        - False: Return all hits\n",
    "    skip_if_no_domains : bool\n",
    "        - True: Return None if protein has no domains (default: False)\n",
    "        - False: Return result even if no domains found\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict with keys: uniprot_acc, protein_length, domains (sorted by start coordinate), available_databases\n",
    "    Or None if skip_if_no_domains=True and no domains found\n",
    "    \"\"\"\n",
    "    # Get protein length\n",
    "    seq, length = _get_protein_seq(uniprot_acc)\n",
    "    if length <= 0:\n",
    "        return None\n",
    "\n",
    "    # Get domains (this will scan databases if source=None)\n",
    "    # Domains are already sorted by get_interpro_domains\n",
    "    domains = _get_interpro_domains(uniprot_acc, source=source, representative_only=representative_only)\n",
    "\n",
    "    # Skip if no domains found and filtering is enabled\n",
    "    if skip_if_no_domains and len(domains) == 0:\n",
    "        return None\n",
    "\n",
    "    # Build available databases dict from domains already retrieved\n",
    "    if source:\n",
    "        if isinstance(source, list):\n",
    "            databases = {}\n",
    "            for domain in domains:\n",
    "                db = domain.get('source')\n",
    "                if db:\n",
    "                    databases[db] = databases.get(db, 0) + 1\n",
    "        else:\n",
    "            databases = {source: len(domains)}\n",
    "    else:\n",
    "        # Extract database info from the domains we already have\n",
    "        databases = {}\n",
    "        for domain in domains:\n",
    "            db = domain.get('source')\n",
    "            if db:\n",
    "                databases[db] = databases.get(db, 0) + 1\n",
    "\n",
    "    # Build output dict\n",
    "    result = {\n",
    "        \"uniprot_acc\": uniprot_acc,\n",
    "        \"protein_length\": length,\n",
    "        \"domains\": domains,\n",
    "        \"available_databases\": databases,\n",
    "        \"sequence\": seq\n",
    "    }\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf0e09e",
   "metadata": {},
   "source": [
    "## 2. Linker Calculation Functions\n",
    "\n",
    "These functions compute linker regions by finding gaps between protein domains.\n",
    "\n",
    "**Key functions:**\n",
    "- `_merge_overlapping_domains()`: Merges overlapping/nested domains to avoid spurious linkers\n",
    "- `compute_linker_regions()`: Calculates regions not covered by any domain\n",
    "- `add_linkers_to_result()`: Adds linker information to domain data\n",
    "- `get_linker_aa_count()`: Counts amino acids in a linker region\n",
    "\n",
    "**Critical feature:**  \n",
    "Domain overlap handling ensures linkers are only calculated in true gaps between domains, not within overlapping domain annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d1e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _merge_overlapping_domains(domains):\n",
    "    \"\"\"\n",
    "    Merge overlapping or nested domains into non-overlapping regions.\n",
    "\n",
    "    This ensures linkers are calculated only in regions NOT covered by any domain.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    domains : list of dict\n",
    "        List of domain dictionaries with 'start' and 'end' keys\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list of dict\n",
    "        Merged non-overlapping domain regions, sorted by start coordinate\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    Input:  [{'start': 3, 'end': 330}, {'start': 60, 'end': 70}, {'start': 120, 'end': 150}]\n",
    "    Output: [{'start': 3, 'end': 330}]  # nested domains merged into parent\n",
    "\n",
    "    Input:  [{'start': 10, 'end': 50}, {'start': 45, 'end': 80}, {'start': 100, 'end': 120}]\n",
    "    Output: [{'start': 10, 'end': 80}, {'start': 100, 'end': 120}]  # overlapping merged\n",
    "    \"\"\"\n",
    "    if not domains:\n",
    "        return []\n",
    "\n",
    "    # Sort domains by start position, then by end position (descending for tie-breaking)\n",
    "    sorted_domains = sorted(domains, key=lambda x: (x[\"start\"] if x[\"start\"] is not None else float('inf'),\n",
    "                                                     -(x[\"end\"] if x[\"end\"] is not None else 0)))\n",
    "\n",
    "    merged = []\n",
    "    current_start = sorted_domains[0][\"start\"]\n",
    "    current_end = sorted_domains[0][\"end\"]\n",
    "\n",
    "    for domain in sorted_domains[1:]:\n",
    "        # Check if this domain overlaps or is nested within the current merged region\n",
    "        if domain[\"start\"] <= current_end + 1:  # Overlapping or adjacent (allow 1 gap to merge adjacent)\n",
    "            # Extend the current merged region to include this domain\n",
    "            current_end = max(current_end, domain[\"end\"])\n",
    "        else:\n",
    "            # No overlap, save current merged region and start a new one\n",
    "            merged.append({\"start\": current_start, \"end\": current_end})\n",
    "            current_start = domain[\"start\"]\n",
    "            current_end = domain[\"end\"]\n",
    "\n",
    "    # Add the last merged region\n",
    "    merged.append({\"start\": current_start, \"end\": current_end})\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "def compute_linker_regions(domains, protein_length=None):\n",
    "    \"\"\"\n",
    "    Compute linker regions between domains, after merging overlapping/nested domains.\n",
    "\n",
    "    CRITICAL: Overlapping and nested domains are merged first to ensure linkers\n",
    "    are only in regions NOT covered by any domain.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    domains : list of dict\n",
    "        List of domain dictionaries with 'start' and 'end' keys\n",
    "        (e.g., result['domains'] from summarize_protein_domains_dict)\n",
    "    protein_length : int, optional\n",
    "        Total protein length. If provided, includes N-terminal and C-terminal regions.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list of dict\n",
    "        Each linker region contains: start, end\n",
    "        Sorted by start coordinate\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    Domains: [{'start': 3, 'end': 330}, {'start': 60, 'end': 70}]\n",
    "    After merge: [{'start': 3, 'end': 330}]  # domain 60-70 is nested, merged\n",
    "    Linkers: [{'start': 1, 'end': 2}, {'start': 331, 'end': protein_length}]\n",
    "    \"\"\"\n",
    "    if not domains:\n",
    "        if protein_length:\n",
    "            return [{\"start\": 1, \"end\": protein_length}]\n",
    "        return []\n",
    "\n",
    "    # CRITICAL FIX: Merge overlapping/nested domains first\n",
    "    merged_domains = _merge_overlapping_domains(domains)\n",
    "\n",
    "    linkers = []\n",
    "\n",
    "    # Check for N-terminal linker (before first merged domain)\n",
    "    first_domain = merged_domains[0]\n",
    "    if first_domain[\"start\"] > 1:\n",
    "        linker_start = 1\n",
    "        linker_end = first_domain[\"start\"] - 1\n",
    "        # Only add if start <= end (length >= 1)\n",
    "        if linker_start <= linker_end:\n",
    "            linkers.append({\n",
    "                \"start\": linker_start,\n",
    "                \"end\": linker_end\n",
    "            })\n",
    "\n",
    "    # Find gaps between consecutive merged domains\n",
    "    for i in range(len(merged_domains) - 1):\n",
    "        current_domain = merged_domains[i]\n",
    "        next_domain = merged_domains[i + 1]\n",
    "\n",
    "        # Check if there's a gap between current domain end and next domain start\n",
    "        gap_start = current_domain[\"end\"] + 1\n",
    "        gap_end = next_domain[\"start\"] - 1\n",
    "\n",
    "        # Only add if start <= end (length >= 1)\n",
    "        if gap_start <= gap_end:\n",
    "            linkers.append({\n",
    "                \"start\": gap_start,\n",
    "                \"end\": gap_end\n",
    "            })\n",
    "\n",
    "    # Check for C-terminal linker (after last merged domain)\n",
    "    if protein_length:\n",
    "        last_domain = merged_domains[-1]\n",
    "        if last_domain[\"end\"] < protein_length:\n",
    "            linker_start = last_domain[\"end\"] + 1\n",
    "            linker_end = protein_length\n",
    "            # Only add if start <= end (length >= 1)\n",
    "            if linker_start <= linker_end:\n",
    "                linkers.append({\n",
    "                    \"start\": linker_start,\n",
    "                    \"end\": linker_end\n",
    "                })\n",
    "\n",
    "    return linkers\n",
    "\n",
    "def add_linkers_to_result(result):\n",
    "    \"\"\"\n",
    "    Add linker regions to a result dict from summarize_protein_domains_dict.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    result : dict\n",
    "        Result dictionary from summarize_protein_domains_dict with keys:\n",
    "        'domains', 'protein_length', etc.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Same result dict with added 'linkers' key containing linker regions\n",
    "    \"\"\"\n",
    "    linkers = compute_linker_regions(result['domains'], result.get('protein_length'))\n",
    "    result['linkers'] = linkers\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_linker_aa_count(linker, seq):\n",
    "    \"\"\"\n",
    "    Count amino acids in a linker region.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    linker : dict\n",
    "        Linker dictionary with 'start' and 'end' keys (1-indexed)\n",
    "    seq : str\n",
    "        Full protein sequence\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with amino acid single-letter codes as keys and counts as values\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "\n",
    "    # Extract linker sequence (convert to 0-indexed)\n",
    "    start = linker['start'] - 1\n",
    "    end = linker['end']\n",
    "    linker_seq = seq[start:end]\n",
    "\n",
    "    # Count amino acids\n",
    "    aa_count = Counter(linker_seq)\n",
    "\n",
    "    return dict(aa_count)\n",
    "\n",
    "\n",
    "def get_multiple_linkers_amino_acid_count(linker_seq_pairs):\n",
    "    \"\"\"\n",
    "    Count amino acids across multiple linker regions.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    linker_seq_pairs : list of tuples\n",
    "        List of (linker, seq) pairs where:\n",
    "        - linker is a dict with 'start' and 'end' keys (1-indexed)\n",
    "        - seq is the full protein sequence string\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with amino acid single-letter codes as keys and total counts as values\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "\n",
    "    total_count = Counter()\n",
    "\n",
    "    for linker, seq in linker_seq_pairs:\n",
    "        linker_aa_count = get_linker_aa_count(linker, seq)\n",
    "        total_count.update(linker_aa_count)\n",
    "\n",
    "    return dict(total_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3c8bd3",
   "metadata": {},
   "source": [
    "## 3. Data Formatting Functions\n",
    "\n",
    "Converts domain and linker data into simplified, analysis-ready formats.\n",
    "\n",
    "**Key function:**\n",
    "- `format_protein_structure()`: Simplifies protein data structure\n",
    "\n",
    "**Output format:**\n",
    "```python\n",
    "{\n",
    "    'domains': [('Domain_name', start, end), ...],\n",
    "    'linkers': [('linker_type', start, end), ...]\n",
    "}\n",
    "```\n",
    "\n",
    "Where linker_type is:\n",
    "- `'n-terminus'`: Before first domain\n",
    "- `'c-terminus'`: After last domain\n",
    "- `'inner'`: Between domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595f0321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_protein_structure(result):\n",
    "    \"\"\"\n",
    "    Format protein domain and linker data into simplified structure.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    result : dict\n",
    "        Result from summarize_protein_domains_dict with linkers added\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict with keys:\n",
    "        - domains: list of tuples (domain_name, start, end)\n",
    "        - linkers: list of tuples (type, start, end)\n",
    "            where type is 'n-terminus', 'c-terminus', or 'inner' for inter-domain\n",
    "    \"\"\"\n",
    "    if not result:\n",
    "        return None\n",
    "\n",
    "    protein_length = result.get('protein_length')\n",
    "    domains_list = result.get('domains', [])\n",
    "    linkers_list = result.get('linkers', [])\n",
    "\n",
    "    # Format domains as (name, start, end), excluding domains with null/None names\n",
    "    formatted_domains = [\n",
    "        (domain['name'], domain['start'], domain['end']) for domain in domains_list\n",
    "        if domain.get('name') is not None and domain.get('name') != ''\n",
    "    ]\n",
    "\n",
    "    # Format linkers as (type, start, end)\n",
    "    formatted_linkers = []\n",
    "    for linker in linkers_list:\n",
    "        start = linker['start']\n",
    "        end = linker['end']\n",
    "\n",
    "        # Determine if n-terminus, c-terminus, or inner (between domains)\n",
    "        if start == 1:\n",
    "            linker_type = 'n-terminus'\n",
    "        elif end == protein_length:\n",
    "            linker_type = 'c-terminus'\n",
    "        else:\n",
    "            linker_type = 'inner'\n",
    "\n",
    "        formatted_linkers.append((linker_type, start, end))\n",
    "\n",
    "    return {\n",
    "        'uniprot_acc': result.get('uniprot_acc'),\n",
    "        'domains': formatted_domains,\n",
    "        'linkers': formatted_linkers\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adae6f7",
   "metadata": {},
   "source": [
    "## 4. Analysis and Visualization Functions\n",
    "\n",
    "Comprehensive analysis and plotting functions for linkers and domains.\n",
    "\n",
    "**Analysis functions:**\n",
    "- `analyze_linkers()`: Amino acid composition and length statistics for N/C/inner linkers\n",
    "- `analyze_domains()`: Amino acid composition and length statistics for domains\n",
    "- `compare_domains_vs_linkers()`: Direct comparison between domains and linkers\n",
    "\n",
    "**Visualization functions:**\n",
    "- `visualize_linker_composition()`: Creates multiple plots for linker analysis\n",
    "  - Individual amino acid distributions by linker type\n",
    "  - Percentage composition comparisons\n",
    "  - Length distribution box plots\n",
    "- `visualize_domain_composition()`: Domain-specific visualizations\n",
    "- `visualize_linker_clusters()`: Length-based cluster visualizations\n",
    "\n",
    "**Output:**\n",
    "All plots saved as high-resolution PNG files (300 DPI) to specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a26d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# ================================\n",
    "# Helper Functions\n",
    "# ================================\n",
    "\n",
    "def _collect_domains(sequences_dict, domains):\n",
    "    \"\"\"Collect all domains with their sequences and metadata.\"\"\"\n",
    "    domain_list = []\n",
    "    domain_lengths = []\n",
    "    for protein_data in domains:\n",
    "        accession = protein_data['uniprot_acc']\n",
    "        seq = sequences_dict.get(accession)\n",
    "        if seq:\n",
    "            for domain in protein_data['domains']:\n",
    "                start, end = domain['start'], domain['end']\n",
    "                length = end - start + 1\n",
    "                domain_list.append((domain, seq))\n",
    "                domain_lengths.append(length)\n",
    "    return domain_list, domain_lengths\n",
    "\n",
    "\n",
    "def _collect_linkers_by_type(proteins_dict, sequences_dict, domains):\n",
    "    \"\"\"Collect and categorize linkers by type from protein data.\"\"\"\n",
    "    n_terminus_linkers, c_terminus_linkers, inner_linkers = [], [], []\n",
    "    n_terminus_lengths, c_terminus_lengths, inner_lengths = [], [], []\n",
    "\n",
    "    for accession, data in proteins_dict.items():\n",
    "        linkers_formatted = data['linkers']\n",
    "        protein_data = next((d for d in domains if d['uniprot_acc'] == accession), None)\n",
    "        if protein_data:\n",
    "            linkers_with_coords = protein_data['linkers']\n",
    "            seq = sequences_dict.get(accession)\n",
    "            if seq and linkers_with_coords:\n",
    "                for i, linker_coord in enumerate(linkers_with_coords):\n",
    "                    if i < len(linkers_formatted):\n",
    "                        linker_type, start, end = linkers_formatted[i]\n",
    "                        linker_length = end - start + 1\n",
    "                        if linker_type == 'n-terminus':\n",
    "                            n_terminus_linkers.append((linker_coord, seq))\n",
    "                            n_terminus_lengths.append(linker_length)\n",
    "                        elif linker_type == 'c-terminus':\n",
    "                            c_terminus_linkers.append((linker_coord, seq))\n",
    "                            c_terminus_lengths.append(linker_length)\n",
    "                        elif linker_type == 'inner':\n",
    "                            inner_linkers.append((linker_coord, seq))\n",
    "                            inner_lengths.append(linker_length)\n",
    "    return (n_terminus_linkers, c_terminus_linkers, inner_linkers,\n",
    "            n_terminus_lengths, c_terminus_lengths, inner_lengths)\n",
    "\n",
    "\n",
    "def _count_domain_amino_acids(domain_list):\n",
    "    \"\"\"Count amino acids in all domains.\"\"\"\n",
    "    domain_counter = Counter()\n",
    "    for domain, seq in domain_list:\n",
    "        start, end = domain['start'] - 1, domain['end']\n",
    "        domain_seq = seq[start:end]\n",
    "        domain_counter.update(domain_seq)\n",
    "    return domain_counter\n",
    "\n",
    "\n",
    "def _count_amino_acids_by_type(n_terminus_linkers, c_terminus_linkers, inner_linkers):\n",
    "    \"\"\"Count amino acids for each linker type.\"\"\"\n",
    "    n_terminus_counter = Counter()\n",
    "    c_terminus_counter = Counter()\n",
    "    inner_counter = Counter()\n",
    "    for linker, seq in n_terminus_linkers:\n",
    "        aa_count = get_linker_aa_count(linker, seq)\n",
    "        n_terminus_counter.update(aa_count)\n",
    "    for linker, seq in c_terminus_linkers:\n",
    "        aa_count = get_linker_aa_count(linker, seq)\n",
    "        c_terminus_counter.update(aa_count)\n",
    "    for linker, seq in inner_linkers:\n",
    "        aa_count = get_linker_aa_count(linker, seq)\n",
    "        inner_counter.update(aa_count)\n",
    "    return n_terminus_counter, c_terminus_counter, inner_counter\n",
    "\n",
    "\n",
    "# ================================\n",
    "# Main Analysis Functions\n",
    "# ================================\n",
    "\n",
    "def analyze_domains(sequences_dict, domains):\n",
    "    \"\"\"Analyze domain amino acid composition and lengths.\"\"\"\n",
    "    domain_list, domain_lengths = _collect_domains(sequences_dict, domains)\n",
    "    domain_counter = _count_domain_amino_acids(domain_list)\n",
    "    print(f\"\\nDomain amino acid counts ({len(domain_list)} domains):\")\n",
    "    print(dict(domain_counter))\n",
    "    return domain_counter, len(domain_list), domain_lengths\n",
    "\n",
    "\n",
    "def analyze_linkers(proteins_dict, sequences_dict, domains):\n",
    "    \"\"\"Analyze linker amino acid composition.\"\"\"\n",
    "    (n_terminus_linkers, c_terminus_linkers, inner_linkers,\n",
    "     n_terminus_lengths, c_terminus_lengths, inner_lengths) = _collect_linkers_by_type(\n",
    "        proteins_dict, sequences_dict, domains)\n",
    "    n_terminus_counter, c_terminus_counter, inner_counter = _count_amino_acids_by_type(\n",
    "        n_terminus_linkers, c_terminus_linkers, inner_linkers)\n",
    "    print(f\"\\nN-terminus amino acid counts ({len(n_terminus_linkers)} linkers):\")\n",
    "    print(dict(n_terminus_counter))\n",
    "    print(f\"\\nC-terminus amino acid counts ({len(c_terminus_linkers)} linkers):\")\n",
    "    print(dict(c_terminus_counter))\n",
    "    print(f\"\\nInner linker amino acid counts ({len(inner_linkers)} linkers):\")\n",
    "    print(dict(inner_counter))\n",
    "    return (n_terminus_counter, c_terminus_counter, inner_counter,\n",
    "            len(n_terminus_linkers), len(c_terminus_linkers), len(inner_linkers),\n",
    "            n_terminus_lengths, c_terminus_lengths, inner_lengths)\n",
    "\n",
    "\n",
    "# ================================\n",
    "# Visualization Functions\n",
    "# ================================\n",
    "\n",
    "def visualize_linker_composition(n_term_count, c_term_count, inner_count, n_count, c_count, i_count,\n",
    "                                n_lengths=None, c_lengths=None, i_lengths=None,\n",
    "                                domain_counter=None, domain_count=0, domain_lengths=None,\n",
    "                                save_dir=None):\n",
    "    \"\"\"Create visualizations for linker amino acid composition.\"\"\"\n",
    "    if save_dir:\n",
    "        save_dir = os.path.expanduser(save_dir)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare amino acid data\n",
    "    amino_acids = sorted(set(list(n_term_count.keys()) + list(c_term_count.keys()) + list(inner_count.keys())))\n",
    "    n_counts = [n_term_count.get(aa, 0) for aa in amino_acids]\n",
    "    c_counts = [c_term_count.get(aa, 0) for aa in amino_acids]\n",
    "    i_counts = [inner_count.get(aa, 0) for aa in amino_acids]\n",
    "\n",
    "    # Create combined comparison plot\n",
    "    _, ax = plt.subplots(figsize=(18, 6))\n",
    "    x = range(len(amino_acids))\n",
    "    width = 0.25\n",
    "    ax.bar([i - width for i in x], n_counts, width, label='N-terminus', color='steelblue', edgecolor='black')\n",
    "    ax.bar([i for i in x], c_counts, width, label='C-terminus', color='coral', edgecolor='black')\n",
    "    ax.bar([i + width for i in x], i_counts, width, label='Inner', color='mediumseagreen', edgecolor='black')\n",
    "    ax.set_xlabel('Amino Acid', fontsize=12)\n",
    "    ax.set_ylabel('Count', fontsize=12)\n",
    "    ax.set_title('Amino Acid Composition: N-terminus vs C-terminus vs Inner Linkers', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(amino_acids)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    if save_dir:\n",
    "        plt.savefig(os.path.join(save_dir, 'linker_aa_composition.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot length distribution if length data is provided\n",
    "    if n_lengths is not None and c_lengths is not None and i_lengths is not None:\n",
    "        data_to_plot = [n_lengths, c_lengths, i_lengths]\n",
    "        labels = ['N-terminus', 'C-terminus', 'Inner']\n",
    "        colors = ['steelblue', 'coral', 'mediumseagreen']\n",
    "\n",
    "        _, ax = plt.subplots(figsize=(12, 6))\n",
    "        bp = ax.boxplot(data_to_plot, labels=labels, patch_artist=True, showmeans=True,\n",
    "                        meanprops=dict(marker='D', markerfacecolor='red', markeredgecolor='darkred', markersize=10),\n",
    "                        medianprops=dict(color='black', linewidth=2))\n",
    "        for patch, color in zip(bp['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        ax.set_ylabel('Length (amino acids)', fontsize=12)\n",
    "        ax.set_title('Linker Length Distributions', fontsize=14, fontweight='bold')\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        if save_dir:\n",
    "            plt.savefig(os.path.join(save_dir, 'linker_length_distributions.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def visualize_domain_composition(domain_counter, domain_count, domain_lengths, save_dir=None):\n",
    "    \"\"\"Create visualizations for domain amino acid composition.\"\"\"\n",
    "    if save_dir:\n",
    "        save_dir = os.path.expanduser(save_dir)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    amino_acids = sorted(domain_counter.keys())\n",
    "    domain_counts = [domain_counter.get(aa, 0) for aa in amino_acids]\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(amino_acids, domain_counts, color='purple', edgecolor='black', alpha=0.7)\n",
    "    ax.set_xlabel('Amino Acid', fontsize=12)\n",
    "    ax.set_ylabel('Count', fontsize=12)\n",
    "    ax.set_title(f'Domain Amino Acid Distribution ({domain_count} domains)', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    if save_dir:\n",
    "        plt.savefig(os.path.join(save_dir, 'domain_aa_distribution.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compare_domains_vs_linkers(domain_counter, domain_count, n_term_count, c_term_count,\n",
    "                                inner_count, n_count, c_count, i_count, save_dir=None):\n",
    "    \"\"\"Create comparison visualizations between domains and linkers.\"\"\"\n",
    "    if save_dir:\n",
    "        save_dir = os.path.expanduser(save_dir)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    all_linker_counter = Counter()\n",
    "    all_linker_counter.update(n_term_count)\n",
    "    all_linker_counter.update(c_term_count)\n",
    "    all_linker_counter.update(inner_count)\n",
    "    total_linker_count = n_count + c_count + i_count\n",
    "\n",
    "    all_amino_acids = sorted(set(list(domain_counter.keys()) + list(all_linker_counter.keys())))\n",
    "    domain_total = sum(domain_counter.values())\n",
    "    linker_total = sum(all_linker_counter.values())\n",
    "    domain_pcts = [(domain_counter.get(aa, 0) / domain_total * 100) if domain_total > 0 else 0 for aa in all_amino_acids]\n",
    "    linker_pcts = [(all_linker_counter.get(aa, 0) / linker_total * 100) if linker_total > 0 else 0 for aa in all_amino_acids]\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(16, 6))\n",
    "    x = range(len(all_amino_acids))\n",
    "    width = 0.35\n",
    "    ax.bar([i - width/2 for i in x], domain_pcts, width, label=f'Domains (n={domain_count})', color='purple', edgecolor='black', alpha=0.7)\n",
    "    ax.bar([i + width/2 for i in x], linker_pcts, width, label=f'Linkers (n={total_linker_count})', color='orange', edgecolor='black', alpha=0.7)\n",
    "    ax.set_xlabel('Amino Acid', fontsize=12)\n",
    "    ax.set_ylabel('Percentage (%)', fontsize=12)\n",
    "    ax.set_title('Amino Acid Composition: Domains vs Linkers', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(all_amino_acids)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    if save_dir:\n",
    "        plt.savefig(os.path.join(save_dir, 'domains_vs_linkers_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "def cluster_linkers_by_length(proteins_dict, method='percentile', custom_thresholds=None):\n",
    "    \"\"\"\n",
    "    Cluster linkers into 3 length categories: short, medium, long.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    proteins_dict : dict\n",
    "        Dictionary with protein accessions as keys and domains/linkers as values\n",
    "        Format: {acc: {'domains': [(name, start, end)...], 'linkers': [(type, start, end)...]}}\n",
    "    method : str\n",
    "        Clustering method:\n",
    "        - 'percentile': Use 33rd and 67th percentiles (default)\n",
    "        - 'kmeans': Use K-means clustering (data-driven)\n",
    "        - 'std': Use mean ± 0.5*std for boundaries\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict with keys:\n",
    "        - 'clusters': dict with 'short', 'medium', 'long' keys, each containing list of\n",
    "          (accession, linker_type, start, end, length) tuples\n",
    "        - 'thresholds': dict with 'short_max', 'medium_max' values\n",
    "        - 'statistics': dict with stats for each cluster\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Collect all linker lengths\n",
    "    all_lengths = []\n",
    "    linker_data = []  # (accession, linker_type, start, end, length)\n",
    "\n",
    "    for accession, data in proteins_dict.items():\n",
    "        for linker in data['linkers']:\n",
    "            linker_type, start, end = linker\n",
    "            length = end - start + 1\n",
    "            all_lengths.append(length)\n",
    "            linker_data.append((accession, linker_type, start, end, length))\n",
    "\n",
    "    if not all_lengths:\n",
    "        return {'clusters': {'short': [], 'medium': [], 'long': []},\n",
    "                'thresholds': {}, 'statistics': {}}\n",
    "\n",
    "    all_lengths_array = np.array(all_lengths)\n",
    "\n",
    "    # Determine thresholds based on method\n",
    "    if method == 'percentile':\n",
    "        threshold1 = np.percentile(all_lengths_array, 33.33)\n",
    "        threshold2 = np.percentile(all_lengths_array, 66.67)\n",
    "        print(f\"Percentile method: 33rd percentile = {threshold1:.1f}, 67th percentile = {threshold2:.1f}\")\n",
    "\n",
    "    elif method == 'kmeans':\n",
    "        from sklearn.cluster import KMeans\n",
    "        # Reshape for sklearn\n",
    "        lengths_reshaped = all_lengths_array.reshape(-1, 1)\n",
    "        kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "        kmeans.fit(lengths_reshaped)\n",
    "\n",
    "        # Get cluster centers and sort them\n",
    "        centers = sorted(kmeans.cluster_centers_.flatten())\n",
    "        # Use midpoints between centers as thresholds\n",
    "        threshold1 = (centers[0] + centers[1]) / 2\n",
    "        threshold2 = (centers[1] + centers[2]) / 2\n",
    "        print(f\"K-means method: cluster centers = {centers}\")\n",
    "        print(f\"Thresholds: {threshold1:.1f}, {threshold2:.1f}\")\n",
    "\n",
    "    elif method == 'std':\n",
    "        mean_len = np.mean(all_lengths_array)\n",
    "        std_len = np.std(all_lengths_array)\n",
    "        # Use mean - std and mean + std for better separation\n",
    "        threshold1 = max(1, mean_len - std_len)  # Ensure threshold is at least 1\n",
    "        threshold2 = mean_len + std_len\n",
    "        print(f\"Std method: mean = {mean_len:.1f}, std = {std_len:.1f}\")\n",
    "        print(f\"Thresholds: short <= {threshold1:.1f}, medium {threshold1:.1f}-{threshold2:.1f}, long > {threshold2:.1f}\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}. Use 'percentile', 'kmeans', 'std', or 'custom'\")\n",
    "\n",
    "    # Cluster the linkers\n",
    "    clusters = {'short': [], 'medium': [], 'long': []}\n",
    "\n",
    "    for linker_info in linker_data:\n",
    "        accession, linker_type, start, end, length = linker_info\n",
    "\n",
    "        if length <= threshold1:\n",
    "            clusters['short'].append(linker_info)\n",
    "        elif length <= threshold2:\n",
    "            clusters['medium'].append(linker_info)\n",
    "        else:\n",
    "            clusters['long'].append(linker_info)\n",
    "\n",
    "    # Calculate statistics for each cluster\n",
    "    statistics = {}\n",
    "    for cluster_name, cluster_data in clusters.items():\n",
    "        if cluster_data:\n",
    "            cluster_lengths = [item[4] for item in cluster_data]  # length is index 4\n",
    "            statistics[cluster_name] = {\n",
    "                'count': len(cluster_lengths),\n",
    "                'min': np.min(cluster_lengths),\n",
    "                'max': np.max(cluster_lengths),\n",
    "                'mean': np.mean(cluster_lengths),\n",
    "                'median': np.median(cluster_lengths),\n",
    "                'std': np.std(cluster_lengths)\n",
    "            }\n",
    "        else:\n",
    "            statistics[cluster_name] = {\n",
    "                'count': 0, 'min': None, 'max': None,\n",
    "                'mean': None, 'median': None, 'std': None\n",
    "            }\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n=== Linker Clustering Summary ===\")\n",
    "    print(f\"Total linkers: {len(linker_data)}\")\n",
    "    for cluster_name in ['short', 'medium', 'long']:\n",
    "        stats = statistics[cluster_name]\n",
    "        if stats['count'] > 0:\n",
    "            print(f\"\\n{cluster_name.upper()}: {stats['count']} linkers\")\n",
    "            print(f\"  Range: {stats['min']:.0f} - {stats['max']:.0f} aa\")\n",
    "            print(f\"  Mean: {stats['mean']:.1f} ± {stats['std']:.1f} aa\")\n",
    "            print(f\"  Median: {stats['median']:.1f} aa\")\n",
    "\n",
    "    return {\n",
    "        'clusters': clusters,\n",
    "        'thresholds': {'short_max': threshold1, 'medium_max': threshold2},\n",
    "        'statistics': statistics\n",
    "    }\n",
    "\n",
    "\n",
    "def visualize_linker_clusters(cluster_results, save_dir=None):\n",
    "    \"\"\"\n",
    "    Visualize linker length clusters.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    cluster_results : dict\n",
    "        Results from cluster_linkers_by_length()\n",
    "    save_dir : str, optional\n",
    "        Directory to save plots\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    clusters = cluster_results['clusters']\n",
    "    stats = cluster_results['statistics']\n",
    "    thresholds = cluster_results['thresholds']\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    short_lengths = [item[4] for item in clusters['short']]\n",
    "    medium_lengths = [item[4] for item in clusters['medium']]\n",
    "    long_lengths = [item[4] for item in clusters['long']]\n",
    "\n",
    "    # Create figure with 2 subplots\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Plot 1: Box plot comparison\n",
    "    data_to_plot = [short_lengths, medium_lengths, long_lengths]\n",
    "    labels = ['Short', 'Medium', 'Long']\n",
    "    colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "\n",
    "    bp = ax1.boxplot(data_to_plot, labels=labels, patch_artist=True,\n",
    "                     showmeans=True,\n",
    "                     meanprops=dict(marker='D', markerfacecolor='red',\n",
    "                                   markeredgecolor='darkred', markersize=8),\n",
    "                     medianprops=dict(color='black', linewidth=2),\n",
    "                     flierprops=dict(marker='o', markerfacecolor='gray',\n",
    "                                    markersize=4, alpha=0.5))\n",
    "\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "\n",
    "    # Add statistics as text\n",
    "    for i, (cluster_name, color) in enumerate(zip(['short', 'medium', 'long'], colors), 1):\n",
    "        s = stats[cluster_name]\n",
    "        if s['count'] > 0:\n",
    "            ax1.text(i, s['mean'], f\"{s['mean']:.1f}±{s['std']:.1f}\",\n",
    "                    ha='left', va='center', fontsize=9, fontweight='bold',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "\n",
    "    ax1.set_ylabel('Length (amino acids)', fontsize=12)\n",
    "    ax1.set_title('Linker Length Clusters - Box Plot Comparison',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    ax1.legend(['Mean'], loc='upper right')\n",
    "\n",
    "    # Plot 2: Histogram with thresholds\n",
    "\n",
    "    ax2.hist([short_lengths, medium_lengths, long_lengths],\n",
    "             bins=30, label=labels, color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "    # Add threshold lines\n",
    "    ax2.axvline(thresholds['short_max'], color='red', linestyle='--',\n",
    "                linewidth=2, label=f\"Threshold 1: {thresholds['short_max']:.1f}\")\n",
    "    ax2.axvline(thresholds['medium_max'], color='darkred', linestyle='--',\n",
    "                linewidth=2, label=f\"Threshold 2: {thresholds['medium_max']:.1f}\")\n",
    "\n",
    "    ax2.set_xlabel('Length (amino acids)', fontsize=12)\n",
    "    ax2.set_ylabel('Frequency', fontsize=12)\n",
    "    ax2.set_title('Linker Length Distribution with Cluster Thresholds',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_dir:\n",
    "        save_dir = os.path.expanduser(save_dir)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(save_dir, 'linker_length_clusters.png'),\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Create separate plot for counts by linker type\n",
    "    _, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Count linkers by type and cluster\n",
    "    type_cluster_counts = {\n",
    "        'n-terminus': {'short': 0, 'medium': 0, 'long': 0},\n",
    "        'c-terminus': {'short': 0, 'medium': 0, 'long': 0},\n",
    "        'inner': {'short': 0, 'medium': 0, 'long': 0}\n",
    "    }\n",
    "\n",
    "    for cluster_name, cluster_data in clusters.items():\n",
    "        for item in cluster_data:\n",
    "            linker_type = item[1]  # linker_type is index 1\n",
    "            type_cluster_counts[linker_type][cluster_name] += 1\n",
    "\n",
    "    # Prepare data for grouped bar chart\n",
    "    linker_types = ['n-terminus', 'c-terminus', 'inner']\n",
    "    short_counts = [type_cluster_counts[t]['short'] for t in linker_types]\n",
    "    medium_counts = [type_cluster_counts[t]['medium'] for t in linker_types]\n",
    "    long_counts = [type_cluster_counts[t]['long'] for t in linker_types]\n",
    "\n",
    "    x = np.arange(len(linker_types))\n",
    "    width = 0.25\n",
    "\n",
    "    ax.bar(x - width, short_counts, width, label='Short', color='lightblue', edgecolor='black')\n",
    "    ax.bar(x, medium_counts, width, label='Medium', color='lightgreen', edgecolor='black')\n",
    "    ax.bar(x + width, long_counts, width, label='Long', color='lightcoral', edgecolor='black')\n",
    "\n",
    "    ax.set_xlabel('Linker Type', fontsize=12)\n",
    "    ax.set_ylabel('Count', fontsize=12)\n",
    "    ax.set_title('Linker Length Clusters by Type', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(linker_types)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_dir:\n",
    "        plt.savefig(os.path.join(save_dir, 'linker_clusters_by_type.png'),\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f646eb41",
   "metadata": {},
   "source": [
    "## 5. Linker Length Clustering Functions\n",
    "\n",
    "Cluster linkers into 3 categories (short, medium, long) using different methods.\n",
    "\n",
    "**Clustering methods available:**\n",
    "\n",
    "1. **`percentile`** (default): Uses 33rd and 67th percentiles\n",
    "   - Ensures equal-sized groups\n",
    "   - Good for exploratory analysis\n",
    "\n",
    "2. **`kmeans`**: Data-driven K-means clustering\n",
    "   - Finds natural groupings in the data\n",
    "   - Best when data has distinct length populations\n",
    "\n",
    "3. **`std`**: Statistical approach using mean ± 1 standard deviation\n",
    "   - Short: < mean - std\n",
    "   - Medium: between mean - std and mean + std\n",
    "   - Long: > mean + std\n",
    "\n",
    "4. **`custom`**: User-defined thresholds\n",
    "   - Specify exact cutoffs based on biological knowledge\n",
    "\n",
    "**Functions:**\n",
    "- `cluster_linkers_by_length()`: Performs clustering\n",
    "- `visualize_linker_clusters()`: Creates visualizations of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12d7fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_linkers_by_length(proteins_dict, method='percentile', custom_thresholds=None):\n",
    "    \"\"\"\n",
    "    Cluster linkers into 3 length categories: short, medium, long.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    proteins_dict : dict\n",
    "        Dictionary with protein accessions as keys and domains/linkers as values\n",
    "        Format: {acc: {'domains': [(name, start, end)...], 'linkers': [(type, start, end)...]}}\n",
    "    method : str\n",
    "        Clustering method:\n",
    "        - 'percentile': Use 33rd and 67th percentiles (default)\n",
    "        - 'kmeans': Use K-means clustering (data-driven)\n",
    "        - 'std': Use mean ± 0.5*std for boundaries\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict with keys:\n",
    "        - 'clusters': dict with 'short', 'medium', 'long' keys, each containing list of\n",
    "          (accession, linker_type, start, end, length) tuples\n",
    "        - 'thresholds': dict with 'short_max', 'medium_max' values\n",
    "        - 'statistics': dict with stats for each cluster\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Collect all linker lengths\n",
    "    all_lengths = []\n",
    "    linker_data = []  # (accession, linker_type, start, end, length)\n",
    "\n",
    "    for accession, data in proteins_dict.items():\n",
    "        for linker in data['linkers']:\n",
    "            linker_type, start, end = linker\n",
    "            length = end - start + 1\n",
    "            all_lengths.append(length)\n",
    "            linker_data.append((accession, linker_type, start, end, length))\n",
    "\n",
    "    if not all_lengths:\n",
    "        return {'clusters': {'short': [], 'medium': [], 'long': []},\n",
    "                'thresholds': {}, 'statistics': {}}\n",
    "\n",
    "    all_lengths_array = np.array(all_lengths)\n",
    "\n",
    "    # Determine thresholds based on method\n",
    "    if method == 'percentile':\n",
    "        threshold1 = np.percentile(all_lengths_array, 33.33)\n",
    "        threshold2 = np.percentile(all_lengths_array, 66.67)\n",
    "        print(f\"Percentile method: 33rd percentile = {threshold1:.1f}, 67th percentile = {threshold2:.1f}\")\n",
    "\n",
    "    elif method == 'kmeans':\n",
    "        from sklearn.cluster import KMeans\n",
    "        # Reshape for sklearn\n",
    "        lengths_reshaped = all_lengths_array.reshape(-1, 1)\n",
    "        kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "        kmeans.fit(lengths_reshaped)\n",
    "\n",
    "        # Get cluster centers and sort them\n",
    "        centers = sorted(kmeans.cluster_centers_.flatten())\n",
    "        # Use midpoints between centers as thresholds\n",
    "        threshold1 = (centers[0] + centers[1]) / 2\n",
    "        threshold2 = (centers[1] + centers[2]) / 2\n",
    "        print(f\"K-means method: cluster centers = {centers}\")\n",
    "        print(f\"Thresholds: {threshold1:.1f}, {threshold2:.1f}\")\n",
    "\n",
    "    elif method == 'std':\n",
    "        mean_len = np.mean(all_lengths_array)\n",
    "        std_len = np.std(all_lengths_array)\n",
    "        # Use mean - std and mean + std for better separation\n",
    "        threshold1 = max(1, mean_len - std_len)  # Ensure threshold is at least 1\n",
    "        threshold2 = mean_len + std_len\n",
    "        print(f\"Std method: mean = {mean_len:.1f}, std = {std_len:.1f}\")\n",
    "        print(f\"Thresholds: short <= {threshold1:.1f}, medium {threshold1:.1f}-{threshold2:.1f}, long > {threshold2:.1f}\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}. Use 'percentile', 'kmeans', 'std', or 'custom'\")\n",
    "\n",
    "    # Cluster the linkers\n",
    "    clusters = {'short': [], 'medium': [], 'long': []}\n",
    "\n",
    "    for linker_info in linker_data:\n",
    "        accession, linker_type, start, end, length = linker_info\n",
    "\n",
    "        if length <= threshold1:\n",
    "            clusters['short'].append(linker_info)\n",
    "        elif length <= threshold2:\n",
    "            clusters['medium'].append(linker_info)\n",
    "        else:\n",
    "            clusters['long'].append(linker_info)\n",
    "\n",
    "    # Calculate statistics for each cluster\n",
    "    statistics = {}\n",
    "    for cluster_name, cluster_data in clusters.items():\n",
    "        if cluster_data:\n",
    "            cluster_lengths = [item[4] for item in cluster_data]  # length is index 4\n",
    "            statistics[cluster_name] = {\n",
    "                'count': len(cluster_lengths),\n",
    "                'min': np.min(cluster_lengths),\n",
    "                'max': np.max(cluster_lengths),\n",
    "                'mean': np.mean(cluster_lengths),\n",
    "                'median': np.median(cluster_lengths),\n",
    "                'std': np.std(cluster_lengths)\n",
    "            }\n",
    "        else:\n",
    "            statistics[cluster_name] = {\n",
    "                'count': 0, 'min': None, 'max': None,\n",
    "                'mean': None, 'median': None, 'std': None\n",
    "            }\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n=== Linker Clustering Summary ===\")\n",
    "    print(f\"Total linkers: {len(linker_data)}\")\n",
    "    for cluster_name in ['short', 'medium', 'long']:\n",
    "        stats = statistics[cluster_name]\n",
    "        if stats['count'] > 0:\n",
    "            print(f\"\\n{cluster_name.upper()}: {stats['count']} linkers\")\n",
    "            print(f\"  Range: {stats['min']:.0f} - {stats['max']:.0f} aa\")\n",
    "            print(f\"  Mean: {stats['mean']:.1f} ± {stats['std']:.1f} aa\")\n",
    "            print(f\"  Median: {stats['median']:.1f} aa\")\n",
    "\n",
    "    return {\n",
    "        'clusters': clusters,\n",
    "        'thresholds': {'short_max': threshold1, 'medium_max': threshold2},\n",
    "        'statistics': statistics\n",
    "    }\n",
    "\n",
    "\n",
    "def visualize_linker_clusters(cluster_results, save_dir=None):\n",
    "    \"\"\"\n",
    "    Visualize linker length clusters.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    cluster_results : dict\n",
    "        Results from cluster_linkers_by_length()\n",
    "    save_dir : str, optional\n",
    "        Directory to save plots\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    clusters = cluster_results['clusters']\n",
    "    stats = cluster_results['statistics']\n",
    "    thresholds = cluster_results['thresholds']\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    short_lengths = [item[4] for item in clusters['short']]\n",
    "    medium_lengths = [item[4] for item in clusters['medium']]\n",
    "    long_lengths = [item[4] for item in clusters['long']]\n",
    "\n",
    "    # Create figure with 2 subplots\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Plot 1: Box plot comparison\n",
    "    data_to_plot = [short_lengths, medium_lengths, long_lengths]\n",
    "    labels = ['Short', 'Medium', 'Long']\n",
    "    colors = ['lightblue', 'lightgreen', 'lightcoral']\n",
    "\n",
    "    bp = ax1.boxplot(data_to_plot, labels=labels, patch_artist=True,\n",
    "                     showmeans=True,\n",
    "                     meanprops=dict(marker='D', markerfacecolor='red',\n",
    "                                   markeredgecolor='darkred', markersize=8),\n",
    "                     medianprops=dict(color='black', linewidth=2),\n",
    "                     flierprops=dict(marker='o', markerfacecolor='gray',\n",
    "                                    markersize=4, alpha=0.5))\n",
    "\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "\n",
    "    # Add statistics as text\n",
    "    for i, (cluster_name, color) in enumerate(zip(['short', 'medium', 'long'], colors), 1):\n",
    "        s = stats[cluster_name]\n",
    "        if s['count'] > 0:\n",
    "            ax1.text(i, s['mean'], f\"{s['mean']:.1f}±{s['std']:.1f}\",\n",
    "                    ha='left', va='center', fontsize=9, fontweight='bold',\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "\n",
    "    ax1.set_ylabel('Length (amino acids)', fontsize=12)\n",
    "    ax1.set_title('Linker Length Clusters - Box Plot Comparison',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    ax1.legend(['Mean'], loc='upper right')\n",
    "\n",
    "    # Plot 2: Histogram with thresholds\n",
    "\n",
    "    ax2.hist([short_lengths, medium_lengths, long_lengths],\n",
    "             bins=30, label=labels, color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "    # Add threshold lines\n",
    "    ax2.axvline(thresholds['short_max'], color='red', linestyle='--',\n",
    "                linewidth=2, label=f\"Threshold 1: {thresholds['short_max']:.1f}\")\n",
    "    ax2.axvline(thresholds['medium_max'], color='darkred', linestyle='--',\n",
    "                linewidth=2, label=f\"Threshold 2: {thresholds['medium_max']:.1f}\")\n",
    "\n",
    "    ax2.set_xlabel('Length (amino acids)', fontsize=12)\n",
    "    ax2.set_ylabel('Frequency', fontsize=12)\n",
    "    ax2.set_title('Linker Length Distribution with Cluster Thresholds',\n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_dir:\n",
    "        save_dir = os.path.expanduser(save_dir)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(save_dir, 'linker_length_clusters.png'),\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Create separate plot for counts by linker type\n",
    "    _, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Count linkers by type and cluster\n",
    "    type_cluster_counts = {\n",
    "        'n-terminus': {'short': 0, 'medium': 0, 'long': 0},\n",
    "        'c-terminus': {'short': 0, 'medium': 0, 'long': 0},\n",
    "        'inner': {'short': 0, 'medium': 0, 'long': 0}\n",
    "    }\n",
    "\n",
    "    for cluster_name, cluster_data in clusters.items():\n",
    "        for item in cluster_data:\n",
    "            linker_type = item[1]  # linker_type is index 1\n",
    "            type_cluster_counts[linker_type][cluster_name] += 1\n",
    "\n",
    "    # Prepare data for grouped bar chart\n",
    "    linker_types = ['n-terminus', 'c-terminus', 'inner']\n",
    "    short_counts = [type_cluster_counts[t]['short'] for t in linker_types]\n",
    "    medium_counts = [type_cluster_counts[t]['medium'] for t in linker_types]\n",
    "    long_counts = [type_cluster_counts[t]['long'] for t in linker_types]\n",
    "\n",
    "    x = np.arange(len(linker_types))\n",
    "    width = 0.25\n",
    "\n",
    "    ax.bar(x - width, short_counts, width, label='Short', color='lightblue', edgecolor='black')\n",
    "    ax.bar(x, medium_counts, width, label='Medium', color='lightgreen', edgecolor='black')\n",
    "    ax.bar(x + width, long_counts, width, label='Long', color='lightcoral', edgecolor='black')\n",
    "\n",
    "    ax.set_xlabel('Linker Type', fontsize=12)\n",
    "    ax.set_ylabel('Count', fontsize=12)\n",
    "    ax.set_title('Linker Length Clusters by Type', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(linker_types)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_dir:\n",
    "        plt.savefig(os.path.join(save_dir, 'linker_clusters_by_type.png'),\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5a1471",
   "metadata": {},
   "source": [
    "## 6. Data Loading Helper Functions\n",
    "\n",
    "Convenient functions to load protein sequences and pre-formatted domain/linker data.\n",
    "\n",
    "**Functions:**\n",
    "- `load_fasta()`: Loads protein sequences from FASTA file\n",
    "- `load_formatted_proteins()`: Loads JSON with domain/linker annotations\n",
    "\n",
    "**Purpose:**  \n",
    "These functions simplify data loading and convert between storage formats (compact tuples) and analysis formats (dictionaries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e05f497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasta(fasta_path):\n",
    "    \"\"\"\n",
    "    Load protein sequences from a FASTA file.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    fasta_path : str\n",
    "        Path to FASTA file\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with protein accessions as keys and sequences as values\n",
    "    \"\"\"\n",
    "    sequences_dict = {}\n",
    "\n",
    "    with open(os.path.expanduser(fasta_path), 'r') as f:\n",
    "        current_acc = None\n",
    "        current_seq = []\n",
    "\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                # Save previous sequence if exists\n",
    "                if current_acc:\n",
    "                    sequences_dict[current_acc] = ''.join(current_seq)\n",
    "\n",
    "                # Start new sequence\n",
    "                current_acc = line[1:]\n",
    "                current_seq = []\n",
    "            else:\n",
    "                current_seq.append(line)\n",
    "\n",
    "        # Save last sequence\n",
    "        if current_acc:\n",
    "            sequences_dict[current_acc] = ''.join(current_seq)\n",
    "\n",
    "    print(f\"✓ Loaded {len(sequences_dict)} sequences from {fasta_path}\")\n",
    "    return sequences_dict\n",
    "\n",
    "\n",
    "def load_formatted_proteins(json_path):\n",
    "    \"\"\"\n",
    "    Load formatted protein data and convert to analysis-ready structure.\n",
    "\n",
    "    This function loads the formatted proteins JSON file and converts the\n",
    "    compact tuple format into dictionaries suitable for analysis functions.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    json_path : str\n",
    "        Path to formatted_proteins.json file\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple of (proteins_dict, domains_list)\n",
    "        - proteins_dict: Original dict with accessions as keys, domains/linkers as values\n",
    "        - domains_list: Analysis-ready list with expanded domain/linker dictionaries\n",
    "\n",
    "    Example structure of domains_list:\n",
    "    [\n",
    "        {\n",
    "            'uniprot_acc': 'P12345',\n",
    "            'domains': [{'name': 'Domain1', 'start': 10, 'end': 100}, ...],\n",
    "            'linkers': [{'start': 1, 'end': 9}, ...]\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    # Load formatted proteins from JSON\n",
    "    with open(os.path.expanduser(json_path), 'r') as f:\n",
    "        proteins_dict = json.load(f)\n",
    "\n",
    "    print(f\"✓ Loaded {len(proteins_dict)} proteins from {json_path}\")\n",
    "\n",
    "    # Convert to analysis-ready structure\n",
    "    # formatted_proteins.json has domains as [name, start, end] and linkers as [type, start, end]\n",
    "    domains_list = []\n",
    "\n",
    "    for accession, data in proteins_dict.items():\n",
    "        # Convert domain tuples to dicts\n",
    "        domain_dicts = [\n",
    "            {'name': d[0], 'start': d[1], 'end': d[2]}\n",
    "            for d in data['domains']\n",
    "        ]\n",
    "\n",
    "        # Convert linker tuples (type, start, end) to dicts with coordinates\n",
    "        linker_coords = []\n",
    "        for linker in data['linkers']:\n",
    "            linker_type, start, end = linker\n",
    "            linker_coords.append({\n",
    "                'start': start,\n",
    "                'end': end\n",
    "            })\n",
    "\n",
    "        domains_list.append({\n",
    "            'uniprot_acc': accession,\n",
    "            'domains': domain_dicts,\n",
    "            'linkers': linker_coords\n",
    "        })\n",
    "\n",
    "    print(f\"✓ Converted to analysis-ready structure: {len(domains_list)} proteins\")\n",
    "\n",
    "    return proteins_dict, domains_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6404b1",
   "metadata": {},
   "source": [
    "--\n",
    "** WORKFLOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d9f1aa",
   "metadata": {},
   "source": [
    "## Step 1: Load Pre-computed Data\n",
    "\n",
    "Load protein sequences and domain/linker annotations from existing files.\n",
    "\n",
    "**What this does:**\n",
    "- Loads FASTA sequences for amino acid composition analysis\n",
    "- Loads pre-computed domain and linker coordinates from JSON\n",
    "- Converts data to analysis-ready format\n",
    "\n",
    "**Note:** The commented code shows how to generate the data from scratch by querying InterPro. This is slow (~14 hours for 850 proteins), so we use pre-computed results instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7098e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get domain info\n",
    "import requests, math, os, json\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "\n",
    "# ================================\n",
    "# Configuration\n",
    "# ================================\n",
    "workdir = \"~/Desktop/work/protein_linkers/input_2\"\n",
    "protein_file = os.path.expanduser(f\"{workdir}/proteins.tsv\")\n",
    "fasta_path = os.path.expanduser(f\"{workdir}/proteins.fa\")\n",
    "json_file = os.path.expanduser(f\"{workdir}/formatted_proteins.json\")\n",
    "\"\"\"\n",
    "\n",
    "# ================================\n",
    "# Linker + Domain info\n",
    "# ================================\n",
    "\n",
    "# Read protein accessions from TSV\n",
    "protein_accessions = []\n",
    "with open(protein_file, 'r') as f:\n",
    "    next(f)  # Skip header\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if parts and parts[0]:\n",
    "            protein_accessions.append(parts[0])\n",
    "\n",
    "print(f\"Found {len(protein_accessions)} proteins in {protein_file}\")\n",
    "\n",
    "# Generate new formatted proteins dict\n",
    "proteins_dict = {}\n",
    "for i, acc in enumerate(protein_accessions, 1):\n",
    "    print(f\"[{i}/{len(protein_accessions)}] Processing {acc}...\")\n",
    "\n",
    "    # Get protein data with domains\n",
    "    result = summarize_protein_domains_dict(acc, representative_only=True)\n",
    "    if result:\n",
    "        # Add linkers\n",
    "        result = add_linkers_to_result(result)\n",
    "\n",
    "        # Format to simplified structure with (type, start, end) for linkers\n",
    "        formatted = format_protein_structure(result)\n",
    "        if formatted:\n",
    "            proteins_dict[acc] = {\n",
    "                'domains': formatted['domains'],\n",
    "                'linkers': formatted['linkers']\n",
    "            }\n",
    "\n",
    "# Save to JSON\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(proteins_dict, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Saved {len(proteins_dict)} formatted proteins to {json_file}\")\n",
    "\"\"\"\n",
    "# ================================\n",
    "# Load the results\n",
    "# ================================\n",
    "\n",
    "# Load FASTA sequences\n",
    "sequences_dict = load_fasta(fasta_path)\n",
    "\n",
    "# Load formatted proteins and convert to analysis-ready structure\n",
    "proteins_dict, domains = load_formatted_proteins(json_file)\n",
    "\n",
    "# ================================\n",
    "# Run Analyses\n",
    "# ================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LINKER ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "n_terminus_counter, c_terminus_counter, inner_counter, n_count, c_count, i_count, n_lengths, c_lengths, i_lengths = analyze_linkers(\n",
    "    proteins_dict, sequences_dict, domains\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DOMAIN ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "domain_counter, domain_count, domain_lengths = analyze_domains(sequences_dict, domains)\n",
    "\n",
    "# ================================\n",
    "# Create Visualizations\n",
    "# ================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n[1/2] Creating combined linker and domain visualizations...\")\n",
    "visualize_linker_composition(n_terminus_counter, c_terminus_counter, inner_counter,\n",
    "                             n_count, c_count, i_count, n_lengths, c_lengths, i_lengths,\n",
    "                             domain_counter, domain_count, domain_lengths,\n",
    "                             save_dir=workdir)\n",
    "\n",
    "print(\"\\n[2/2] Creating domain vs linker comparison...\")\n",
    "compare_domains_vs_linkers(domain_counter, domain_count,\n",
    "                           n_terminus_counter, c_terminus_counter, inner_counter,\n",
    "                           n_count, c_count, i_count, save_dir=workdir)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ ANALYSIS COMPLETE!\")\n",
    "print(f\"All plots saved to: {os.path.expanduser(workdir)}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5452786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, math, os, json\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "\n",
    "# ================================\n",
    "# Configuration\n",
    "# ================================\n",
    "workdir = \"~/Desktop/work/protein_linkers/input_2\"\n",
    "protein_file = os.path.expanduser(f\"{workdir}/proteins.tsv\")\n",
    "fasta_path = os.path.expanduser(f\"{workdir}/proteins.fa\")\n",
    "json_file = os.path.expanduser(f\"{workdir}/formatted_proteins.json\")\n",
    "# ================================\n",
    "# Load the results\n",
    "# ================================\n",
    "\n",
    "# Load FASTA sequences\n",
    "sequences_dict = load_fasta(fasta_path)\n",
    "\n",
    "# Load formatted proteins and convert to analysis-ready structure\n",
    "proteins_dict, domains = load_formatted_proteins(json_file)\n",
    "\n",
    "# ================================\n",
    "# Filter out proteins with no domains\n",
    "# ================================\n",
    "\n",
    "# Use the filtering function\n",
    "proteins_dict, domains, filter_stats = filter_proteins_without_domains(\n",
    "    proteins_dict,\n",
    "    domains_list=domains,\n",
    "    save_path=f\"{workdir}/formatted_proteins.json\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"Before filtering: {filter_stats['total_before']} proteins\")\n",
    "print(f\"Proteins with no domains: {filter_stats['no_domains']}\")\n",
    "print(f\"After filtering: {filter_stats['total_after']} proteins\")\n",
    "print(f\"Removed: {filter_stats['removed']} proteins with no domains\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c2075a",
   "metadata": {},
   "source": [
    "## Step 2: Basic Linker Statistics\n",
    "\n",
    "Get quick overview statistics about all linkers in the dataset.\n",
    "\n",
    "**Statistics calculated:**\n",
    "- Total number of linkers\n",
    "- Min/Max lengths with protein accessions and coordinates\n",
    "- Mean, median, standard deviation\n",
    "- 25th and 75th percentiles\n",
    "- Breakdown by linker type (N-terminus, C-terminus, inner)\n",
    "\n",
    "**Purpose:**  \n",
    "These statistics help you understand the data distribution before running detailed analyses or clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9272b2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Get Basic Linker Statistics\n",
    "# ================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Collect all linker data with details\n",
    "all_linker_data = []  # (accession, linker_type, start, end, length)\n",
    "linker_type_data = {\n",
    "    'n-terminus': [],\n",
    "    'c-terminus': [],\n",
    "    'inner': []\n",
    "}\n",
    "\n",
    "for accession, data in proteins_dict.items():\n",
    "    for linker in data['linkers']:\n",
    "        linker_type, start, end = linker\n",
    "        length = end - start + 1\n",
    "        linker_info = (accession, linker_type, start, end, length)\n",
    "        all_linker_data.append(linker_info)\n",
    "        linker_type_data[linker_type].append(linker_info)\n",
    "\n",
    "# Extract all lengths for statistics\n",
    "all_linker_lengths = [item[4] for item in all_linker_data]\n",
    "\n",
    "# Find min and max linkers\n",
    "min_linker = min(all_linker_data, key=lambda x: x[4])\n",
    "max_linker = max(all_linker_data, key=lambda x: x[4])\n",
    "\n",
    "# Calculate overall statistics\n",
    "print(\"=\"*60)\n",
    "print(\"OVERALL LINKER STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total linkers: {len(all_linker_lengths)}\")\n",
    "print(f\"\\nMin length: {np.min(all_linker_lengths)} aa\")\n",
    "print(f\"  → {min_linker[0]} ({min_linker[1]}): positions {min_linker[2]}-{min_linker[3]}\")\n",
    "print(f\"\\nMax length: {np.max(all_linker_lengths)} aa\")\n",
    "print(f\"  → {max_linker[0]} ({max_linker[1]}): positions {max_linker[2]}-{max_linker[3]}\")\n",
    "print(f\"\\nMean length: {np.mean(all_linker_lengths):.2f} aa\")\n",
    "print(f\"Median length: {np.median(all_linker_lengths):.2f} aa\")\n",
    "print(f\"Std deviation: {np.std(all_linker_lengths):.2f} aa\")\n",
    "print(f\"25th percentile: {np.percentile(all_linker_lengths, 25):.2f} aa\")\n",
    "print(f\"75th percentile: {np.percentile(all_linker_lengths, 75):.2f} aa\")\n",
    "\n",
    "# Statistics by linker type\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTICS BY LINKER TYPE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for linker_type in ['n-terminus', 'c-terminus', 'inner']:\n",
    "    type_data = linker_type_data[linker_type]\n",
    "    if type_data:\n",
    "        lengths = [item[4] for item in type_data]\n",
    "        min_item = min(type_data, key=lambda x: x[4])\n",
    "        max_item = max(type_data, key=lambda x: x[4])\n",
    "\n",
    "        print(f\"\\n{linker_type.upper()}:\")\n",
    "        print(f\"  Count: {len(lengths)}\")\n",
    "        print(f\"  Min: {np.min(lengths)} aa\")\n",
    "        print(f\"    → {min_item[0]}: positions {min_item[2]}-{min_item[3]}\")\n",
    "        print(f\"  Max: {np.max(lengths)} aa\")\n",
    "        print(f\"    → {max_item[0]}: positions {max_item[2]}-{max_item[3]}\")\n",
    "        print(f\"  Mean: {np.mean(lengths):.2f} ± {np.std(lengths):.2f} aa\")\n",
    "        print(f\"  Median: {np.median(lengths):.2f} aa\")\n",
    "    else:\n",
    "        print(f\"\\n{linker_type.upper()}: No linkers found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb83db4",
   "metadata": {},
   "source": [
    "## Step 4: Linker Length Clustering (Optional)\n",
    "\n",
    "Compare different clustering methods to group linkers by length.\n",
    "\n",
    "**This cell runs all 3 methods:**\n",
    "1. Percentile-based (balanced groups)\n",
    "2. K-means (data-driven)\n",
    "3. Standard deviation-based (statistical)\n",
    "\n",
    "**Output for each method:**\n",
    "- Threshold values\n",
    "- Statistics for each cluster (count, min, max, mean, median, std)\n",
    "- Box plot comparison of clusters\n",
    "- Histogram with threshold lines\n",
    "- Breakdown by linker type (N/C/inner)\n",
    "\n",
    "**Recommendation:**  \n",
    "Start with percentile method for exploratory analysis. Use kmeans if the histogram shows distinct peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb7d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Example: Cluster Linkers by Length\n",
    "# ================================\n",
    "\n",
    "# Try different clustering methods:\n",
    "\n",
    "# Method 1: Percentile-based (divides into thirds)\n",
    "# USE THIS\n",
    "print(\"=\"*60)\n",
    "print(\"METHOD 1: PERCENTILE-BASED CLUSTERING\")\n",
    "print(\"=\"*60)\n",
    "cluster_results_percentile = cluster_linkers_by_length(proteins_dict, method='percentile')\n",
    "visualize_linker_clusters(cluster_results_percentile, save_dir=workdir)\n",
    "\n",
    "# Method 2: K-means clustering (data-driven)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"METHOD 2: K-MEANS CLUSTERING\")\n",
    "print(\"=\"*60)\n",
    "cluster_results_kmeans = cluster_linkers_by_length(proteins_dict, method='kmeans')\n",
    "visualize_linker_clusters(cluster_results_kmeans, save_dir=workdir)\n",
    "\n",
    "# Method 3: Standard deviation-based\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"METHOD 3: STANDARD DEVIATION-BASED CLUSTERING\")\n",
    "print(\"=\"*60)\n",
    "cluster_results_std = cluster_linkers_by_length(proteins_dict, method='std')\n",
    "visualize_linker_clusters(cluster_results_std, save_dir=workdir)\n",
    "\n",
    "\n",
    "# ================================\n",
    "# Access clustered data\n",
    "# ================================\n",
    "\n",
    "# Example: Get all short linkers\n",
    "short_linkers = cluster_results_percentile['clusters']['short']\n",
    "print(f\"\\nExample - First 5 short linkers:\")\n",
    "for i, (acc, ltype, start, end, length) in enumerate(short_linkers[:5], 1):\n",
    "    print(f\"  {i}. {acc}: {ltype} linker, positions {start}-{end}, length {length} aa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8556f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cath_gene3d(uniprot_acc):\n",
    "    \"\"\"\n",
    "    Pull CATH FunFams for a UniProt and infer domain spans if present.\n",
    "    (Some responses include mapped regions to the sequence.)\n",
    "    \"\"\"\n",
    "    url = f\"{CATH_URL}/{uniprot_acc}?content-type=application/json\"\n",
    "    r = requests.get(url, timeout=30)\n",
    "    if r.status_code == 404:\n",
    "        return []\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    hits = []\n",
    "    for ff in data.get(\"data\", []) if isinstance(data, dict) else data:\n",
    "        sfam = ff.get(\"superfamily_id\")\n",
    "        funfam = ff.get(\"funfam_number\")\n",
    "        for reg in ff.get(\"regions\", []):\n",
    "            hits.append({\n",
    "                \"db\": \"CATH-Gene3D\",\n",
    "                \"accession\": f\"{sfam}/FF{funfam}\",\n",
    "                \"start\": reg.fet(\"aln_start\") or reg.get(\"start\"),\n",
    "                \"end\": reg.get(\"aln_end\") or reg.get(\"end\"),\n",
    "                \"score\": reg.get(\"evalue\") or reg.get(\"bitscore\")\n",
    "            })\n",
    "    return sorted([h for h in hits if h[\"start\"] and h[\"end\"]], key=lambda h: (h[\"start\"], h[\"end\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
